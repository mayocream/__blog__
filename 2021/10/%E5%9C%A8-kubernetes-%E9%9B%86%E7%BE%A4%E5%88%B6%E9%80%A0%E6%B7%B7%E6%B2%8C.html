<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/css/c3b55921f92a131e.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-072f062dc024cc52.js"/><script src="/_next/static/chunks/f4f1b8d9-0107da81548bf985.js" async=""></script><script src="/_next/static/chunks/435-d305dd8b5fb158de.js" async=""></script><script src="/_next/static/chunks/main-app-3958e659bb0a464b.js" async=""></script><title>Mayo Rocks!</title><meta name="description" content="Mayo&#x27;s Blog"/><link rel="icon" href="/icon.png?14d5a92fbe70e82a" type="image/png" sizes="460x460"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="antialiased"><article><p><em>Chaos Mesh 工作原理与控制平面开发</em></p>
<p>Chaos Mesh 是由 TiDB 背后的 PingCAP 公司开发，运行在 Kubernetes 上的混沌工程（Chaos Engineering）系统。简而言之，Chaos Mesh 通过运行在 K8s 集群中的“特权”容器，依据 CRD 资源中的测试场景，在集群中制造浑沌（模拟故障）<sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref aria-describedby="footnote-label">1</a></sup>。</p>
<p>本文的首要目的是试探 Chaos Mesh 混沌工程系统能否整合进企业的云平台中、通过怎样的方式结合，打造一体化式的体验。如果你缺乏基础知识，要想对 Chaos Mesh 的架构有宏观上的认识，请参阅文末尾注中的链接。</p>
<p>本文试验代码位于 <a href="https://github.com/mayocream/chaos-mesh-controlpanel-demo">mayocream/chaos-mesh-controlpanel-demo</a> 仓库。</p>
<h2>怎么捣乱？</h2>
<h3>特权</h3>
<p>上面提到 Chaos Mesh 运行 Kubernetes 特权容器来制造故障。Daemon Set 方式运行的 Pod 授权了容器运行时的<a href="https://kubernetes.io/zh/docs/concepts/policy/pod-security-policy/#capabilities">权能字（Capabilities）</a>。</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: DaemonSet
spec:
  template:
    metadata: ...
    spec:
      containers:
        - name: chaos-daemon
          securityContext:
            {{- if .Values.chaosDaemon.privileged }}
            privileged: true
            capabilities:
              add:
                - SYS_PTRACE
            {{- else }}
            capabilities:
              add:
                - SYS_PTRACE
                - NET_ADMIN
                - MKNOD
                - SYS_CHROOT
                - SYS_ADMIN
                - KILL
                # CAP_IPC_LOCK is used to lock memory
                - IPC_LOCK
            {{- end }}
</code></pre>
<p>这些 Linux 权能字用于授予容器特权，以创建和访问 <code>/dev/fuse</code> FUSE 管道<sup><a href="#user-content-fn-2" id="user-content-fnref-2" data-footnote-ref aria-describedby="footnote-label">2</a></sup>（FUSE 是 Linux 用户空间文件系统接口，它使无特权的用户能够无需编辑内核代码而创建自己的文件系统）。</p>
<p>参阅 <a href="https://github.com/chaos-mesh/chaos-mesh/pull/1109">#1109</a> Pull Request，Daemon Set 程序使用 CGO 调用 Linux <code>makedev</code> 函数创建 FUSE 管道。</p>
<pre><code class="language-go">// #include &#x3C;sys/sysmacros.h>
// #include &#x3C;sys/types.h>
// // makedev is a macro, so a wrapper is needed
// dev_t Makedev(unsigned int maj, unsigned int min) {
//   return makedev(maj, min);
// }

// EnsureFuseDev ensures /dev/fuse exists. If not, it will create one
func EnsureFuseDev() {
	if _, err := os.Open("/dev/fuse"); os.IsNotExist(err) {
		// 10, 229 according to https://www.kernel.org/doc/Documentation/admin-guide/devices.txt
		fuse := C.Makedev(10, 229)
		syscall.Mknod("/dev/fuse", 0o666|syscall.S_IFCHR, int(fuse))
	}
}
</code></pre>
<p>同时在 <a href="https://github.com/chaos-mesh/chaos-mesh/pull/1453">#1453</a> PR 中，Chaos Daemon 默认启用特权模式，即容器的 <code>securityContext</code> 中设置 <code>privileged: true</code>。</p>
<h3>杀死 Pod</h3>
<p>PodKill、PodFailure、ContainerKill 都归属于 PodChaos 类别下，PodKill 是随机杀死 Pod。</p>
<p>PodKill 的具体实现其实是通过调用 API Server 发送 Kill 命令。</p>
<pre><code class="language-go">import (
	"context"

	v1 "k8s.io/api/core/v1"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

type Impl struct {
	client.Client
}

func (impl *Impl) Apply(ctx context.Context, index int, records []*v1alpha1.Record, obj v1alpha1.InnerObject) (v1alpha1.Phase, error) {
	...
	err = impl.Get(ctx, namespacedName, &#x26;pod)
	if err != nil {
		// TODO: handle this error
		return v1alpha1.NotInjected, err
	}

	err = impl.Delete(ctx, &#x26;pod, &#x26;client.DeleteOptions{
		GracePeriodSeconds: &#x26;podchaos.Spec.GracePeriod, // PeriodSeconds has to be set specifically
	})
	...
	return v1alpha1.Injected, nil
}
</code></pre>
<p><code>GracePeriodSeconds</code> 参数适用于 K8s <a href="https://kubernetes.io/zh/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination-forced">强制终止 Pod</a>。例如在需要快速删除 Pod 时，我们使用 <code>kubectl delete pod --grace-period=0 --force</code> 命令。</p>
<p>PodFailure 是通过 Patch Pod 对象资源，用错误的镜像替换 Pod 中的镜像。Chaos 只修改了 <code>containers</code> 和 <code>initContainers</code> 的 <code>image</code> 字段，这也是因为 Pod 大部分字段是无法更改的，详情可以参阅 <a href="https://kubernetes.io/zh/docs/concepts/workloads/pods/#pod-update-and-replacement">Pod 更新与替换</a>。</p>
<pre><code class="language-go">func (impl *Impl) Apply(ctx context.Context, index int, records []*v1alpha1.Record, obj v1alpha1.InnerObject) (v1alpha1.Phase, error) {
	...
	pod := origin.DeepCopy()
	for index := range pod.Spec.Containers {
		originImage := pod.Spec.Containers[index].Image
		name := pod.Spec.Containers[index].Name

		key := annotation.GenKeyForImage(podchaos, name, false)
		if pod.Annotations == nil {
			pod.Annotations = make(map[string]string)
		}

		// If the annotation is already existed, we could skip the reconcile for this container
		if _, ok := pod.Annotations[key]; ok {
			continue
		}
		pod.Annotations[key] = originImage
		pod.Spec.Containers[index].Image = config.ControllerCfg.PodFailurePauseImage
	}

	for index := range pod.Spec.InitContainers {
		originImage := pod.Spec.InitContainers[index].Image
		name := pod.Spec.InitContainers[index].Name

		key := annotation.GenKeyForImage(podchaos, name, true)
		if pod.Annotations == nil {
			pod.Annotations = make(map[string]string)
		}

		// If the annotation is already existed, we could skip the reconcile for this container
		if _, ok := pod.Annotations[key]; ok {
			continue
		}
		pod.Annotations[key] = originImage
		pod.Spec.InitContainers[index].Image = config.ControllerCfg.PodFailurePauseImage
	}

	err = impl.Patch(ctx, pod, client.MergeFrom(&#x26;origin))
	if err != nil {
		// TODO: handle this error
		return v1alpha1.NotInjected, err
	}

	return v1alpha1.Injected, nil
}
</code></pre>
<p>默认用于引发故障的容器镜像是 <code>gcr.io/google-containers/pause:latest</code>， 如果在国内环境使用，大概率会水土不服，可以将 <code>gcr.io</code> 替换为 <code>registry.aliyuncs.com</code>。</p>
<p>ContainerKill 不同于 PodKill 和 PodFailure，后两个都是通过 K8s API Server 控制 Pod 生命周期，而 ContainerKill 是通过运行在集群 Node 上的 Chaos Daemon 程序操作完成。具体来说，ContainerKill 通过 Chaos Controller Manager 运行客户端向 Chaos Daemon 发起 grpc 调用。</p>
<pre><code class="language-go">func (b *ChaosDaemonClientBuilder) Build(ctx context.Context, pod *v1.Pod) (chaosdaemonclient.ChaosDaemonClientInterface, error) {
	...
	daemonIP, err := b.FindDaemonIP(ctx, pod)
	if err != nil {
		return nil, err
	}
	builder := grpcUtils.Builder(daemonIP, config.ControllerCfg.ChaosDaemonPort).WithDefaultTimeout()
	if config.ControllerCfg.TLSConfig.ChaosMeshCACert != "" {
		builder.TLSFromFile(config.ControllerCfg.TLSConfig.ChaosMeshCACert, config.ControllerCfg.TLSConfig.ChaosDaemonClientCert, config.ControllerCfg.TLSConfig.ChaosDaemonClientKey)
	} else {
		builder.Insecure()
	}
	cc, err := builder.Build()
	if err != nil {
		return nil, err
	}
	return chaosdaemonclient.New(cc), nil
}

</code></pre>
<p>向 Chaos Daemon 发送命令时会依据 Pod 信息创建对应的客户端，例如要控制某个 Node 上的 Pod，会获取该 Pod 所在 Node 的 ClusterIP，以创建客户端。如果 TLS 证书配置存在，Controller Manager 会为客户端添加 TLS 证书。</p>
<p>Chaos Daemon 在启动时如果有 TLS 证书，会附加证书以启用 grpcs。TLS 校验配置 <code>RequireAndVerifyClientCert</code> 表示启用双向 TLS 认证（mTLS）。</p>
<pre><code class="language-go">func newGRPCServer(containerRuntime string, reg prometheus.Registerer, tlsConf tlsConfig) (*grpc.Server, error) {
	...
	if tlsConf != (tlsConfig{}) {
		caCert, err := ioutil.ReadFile(tlsConf.CaCert)
		if err != nil {
			return nil, err
		}
		caCertPool := x509.NewCertPool()
		caCertPool.AppendCertsFromPEM(caCert)

		serverCert, err := tls.LoadX509KeyPair(tlsConf.Cert, tlsConf.Key)
		if err != nil {
			return nil, err
		}

		creds := credentials.NewTLS(&#x26;tls.Config{
			Certificates: []tls.Certificate{serverCert},
			ClientCAs:    caCertPool,
			ClientAuth:   tls.RequireAndVerifyClientCert,
		})

		grpcOpts = append(grpcOpts, grpc.Creds(creds))
	}

	s := grpc.NewServer(grpcOpts...)
	grpcMetrics.InitializeMetrics(s)

	pb.RegisterChaosDaemonServer(s, ds)
	reflection.Register(s)

	return s, nil
}
</code></pre>
<p>Chaos Daemon 提供了以下 grpc 调用接口：</p>
<pre><code class="language-go">// ChaosDaemonClient is the client API for ChaosDaemon service.
//
// For semantics around ctx use and closing/ending streaming RPCs, please refer to https://godoc.org/google.golang.org/grpc#ClientConn.NewStream.
type ChaosDaemonClient interface {
	SetTcs(ctx context.Context, in *TcsRequest, opts ...grpc.CallOption) (*empty.Empty, error)
	FlushIPSets(ctx context.Context, in *IPSetsRequest, opts ...grpc.CallOption) (*empty.Empty, error)
	SetIptablesChains(ctx context.Context, in *IptablesChainsRequest, opts ...grpc.CallOption) (*empty.Empty, error)
	SetTimeOffset(ctx context.Context, in *TimeRequest, opts ...grpc.CallOption) (*empty.Empty, error)
	RecoverTimeOffset(ctx context.Context, in *TimeRequest, opts ...grpc.CallOption) (*empty.Empty, error)
	ContainerKill(ctx context.Context, in *ContainerRequest, opts ...grpc.CallOption) (*empty.Empty, error)
	ContainerGetPid(ctx context.Context, in *ContainerRequest, opts ...grpc.CallOption) (*ContainerResponse, error)
	ExecStressors(ctx context.Context, in *ExecStressRequest, opts ...grpc.CallOption) (*ExecStressResponse, error)
	CancelStressors(ctx context.Context, in *CancelStressRequest, opts ...grpc.CallOption) (*empty.Empty, error)
	ApplyIOChaos(ctx context.Context, in *ApplyIOChaosRequest, opts ...grpc.CallOption) (*ApplyIOChaosResponse, error)
	ApplyHttpChaos(ctx context.Context, in *ApplyHttpChaosRequest, opts ...grpc.CallOption) (*ApplyHttpChaosResponse, error)
	SetDNSServer(ctx context.Context, in *SetDNSServerRequest, opts ...grpc.CallOption) (*empty.Empty, error)
}
</code></pre>
<h3>网络故障</h3>
<p>从最初的 <a href="https://github.com/chaos-mesh/chaos-mesh/pull/41">#41</a> PR 中，可以清晰地了解到，Chaos Mesh 的网络错误注入是通过调用 <code>pbClient.SetNetem</code> 方法，将参数封装成请求，交给 Node 上的 Chaos Daemon 处理的。</p>
<p>（注：这是 2019 年初期的代码，随着项目发展，代码中的函数已分散到不同的文件中）</p>
<pre><code class="language-go">func (r *Reconciler) applyPod(ctx context.Context, pod *v1.Pod, networkchaos *v1alpha1.NetworkChaos) error {
	...
	pbClient := pb.NewChaosDaemonClient(c)

	containerId := pod.Status.ContainerStatuses[0].ContainerID

	netem, err := spec.ToNetem()
	if err != nil {
		return err
	}

	_, err = pbClient.SetNetem(ctx, &#x26;pb.NetemRequest{
		ContainerId: containerId,
		Netem:       netem,
	})

	return err
}
</code></pre>
<p>同时在 <code>pkg/chaosdaemon</code> 包中，我们能看到 Chaos Daemon 处理请求的方法。</p>
<pre><code class="language-go">func (s *Server) SetNetem(ctx context.Context, in *pb.NetemRequest) (*empty.Empty, error) {
	log.Info("Set netem", "Request", in)

	pid, err := s.crClient.GetPidFromContainerID(ctx, in.ContainerId)

	if err != nil {
		return nil, status.Errorf(codes.Internal, "get pid from containerID error: %v", err)
	}

	if err := Apply(in.Netem, pid); err != nil {
		return nil, status.Errorf(codes.Internal, "netem apply error: %v", err)
	}

	return &#x26;empty.Empty{}, nil
}

// Apply applies a netem on eth0 in pid related namespace
func Apply(netem *pb.Netem, pid uint32) error {
	log.Info("Apply netem on PID", "pid", pid)

	ns, err := netns.GetFromPath(GenNetnsPath(pid))
	if err != nil {
		log.Error(err, "failed to find network namespace", "pid", pid)
		return errors.Trace(err)
	}
	defer ns.Close()

	handle, err := netlink.NewHandleAt(ns)
	if err != nil {
		log.Error(err, "failed to get handle at network namespace", "network namespace", ns)
		return err
	}

	link, err := handle.LinkByName("eth0") // TODO: check whether interface name is eth0
	if err != nil {
		log.Error(err, "failed to find eth0 interface")
		return errors.Trace(err)
	}

	netemQdisc := netlink.NewNetem(netlink.QdiscAttrs{
		LinkIndex: link.Attrs().Index,
		Handle:    netlink.MakeHandle(1, 0),
		Parent:    netlink.HANDLE_ROOT,
	}, ToNetlinkNetemAttrs(netem))

	if err = handle.QdiscAdd(netemQdisc); err != nil {
		if !strings.Contains(err.Error(), "file exists") {
			log.Error(err, "failed to add Qdisc")
			return errors.Trace(err)
		}
	}

	return nil
}
</code></pre>
<p>最终使用 <a href="https://github.com/vishvananda/netlink">vishvananda/netlink</a> 库操作 Linux 网络接口来完成工作。</p>
<p>至于为什么没有详细说明使用了什么命令，进行了哪些操作，是因为我也不会。待我恶补完网络知识后再来回顾吧。不过这里能够知道，NetworkChaos 混沌类型，操作了Linux 宿主机网络来制造混沌，包含 iptables、ipset 等工具。</p>
<p>在 Chaos Daemon 的 Dockerfile 中，可以看到其依赖的 Linux 工具链：</p>
<pre><code class="language-dockerfile">RUN apt-get update &#x26;&#x26; \
    apt-get install -y tzdata iptables ipset stress-ng iproute2 fuse util-linux procps curl &#x26;&#x26; \
    rm -rf /var/lib/apt/lists/*
</code></pre>
<h3>压力测试</h3>
<p>StressChaos 类型的混沌也是由 Chaos Daemon 实施的，Controller Manager 计算好规则后就将任务下发到具体的 Daemon 上。拼装的参数如下，这些参数会组合成命令执行的参数，附加到 <code>stress-ng</code> 命令后执行<sup><a href="#user-content-fn-3" id="user-content-fnref-3" data-footnote-ref aria-describedby="footnote-label">3</a></sup>。</p>
<pre><code class="language-go">// Normalize the stressors to comply with stress-ng
func (in *Stressors) Normalize() (string, error) {
	stressors := ""
	if in.MemoryStressor != nil &#x26;&#x26; in.MemoryStressor.Workers != 0 {
		stressors += fmt.Sprintf(" --vm %d --vm-keep", in.MemoryStressor.Workers)
		if len(in.MemoryStressor.Size) != 0 {
			if in.MemoryStressor.Size[len(in.MemoryStressor.Size)-1] != '%' {
				size, err := units.FromHumanSize(string(in.MemoryStressor.Size))
				if err != nil {
					return "", err
				}
				stressors += fmt.Sprintf(" --vm-bytes %d", size)
			} else {
				stressors += fmt.Sprintf(" --vm-bytes %s",
					in.MemoryStressor.Size)
			}
		}

		if in.MemoryStressor.Options != nil {
			for _, v := range in.MemoryStressor.Options {
				stressors += fmt.Sprintf(" %v ", v)
			}
		}
	}
	if in.CPUStressor != nil &#x26;&#x26; in.CPUStressor.Workers != 0 {
		stressors += fmt.Sprintf(" --cpu %d", in.CPUStressor.Workers)
		if in.CPUStressor.Load != nil {
			stressors += fmt.Sprintf(" --cpu-load %d",
				*in.CPUStressor.Load)
		}

		if in.CPUStressor.Options != nil {
			for _, v := range in.CPUStressor.Options {
				stressors += fmt.Sprintf(" %v ", v)
			}
		}
	}
	return stressors, nil
}
</code></pre>
<p>Chaos Daemon 服务端处理函数中调用 Go 官方包 <code>os/exec</code> 执行命令，再大段贴代码就没有意思了，具体可以阅读 <a href="https://github.com/chaos-mesh/chaos-mesh/blob/98af3a0e7832a4971d6b133a32069539d982ef0a/pkg/chaosdaemon/stress_server_linux.go#L33">pkg/chaosdaemon/stress_server_linux.go</a> 文件。同名文件还有以 darwin 结尾的，推测是为了在 macOS 上开发调试方便。</p>
<p>既然都做了兼容，那为什么没有 windows 版本的，气冷抖（不过我也不用 Windows 进行开发调试）。</p>
<p>代码中使用 <a href="https://github.com/shirou/gopsutil">shirou/gopsutil</a> 包获取 PID 进程状态，并读取了 stdout、stderr 等标准输出，这种处理模式我在 <a href="https://github.com/hashicorp/go-plugin">hashicorp/go-plugin</a> 见过，go-plugin 在这方面做得更加优秀。我的另一篇文章 <a href="https://shoujo.ink/2021/09/dkron-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/">Dkron 源码分析</a> 中提到了它。</p>
<h3>IO 注入</h3>
<p>开篇就提到了 Chaos Mesh 使用了特权容器，用于挂载宿主机上的 FUSE 设备 <code>/dev/fuse</code>。</p>
<p>看到这里，我铁定以为 Chaos Mesh 是使用 <a href="https://kubernetes.io/zh/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook">Mutating 准入控制器</a>进行 Sidecar 容器的注入，挂载 FUSE 设备，或修改 Pod Volumes Mount 等配置，然后它的实现方式与直观上不同。</p>
<p>仔细看了 <a href="https://github.com/chaos-mesh/chaos-mesh/pull/826">#826</a> PR，这个 PR 引入了新的 IOChaos 的实现，避免使用 Sidecar 注入的方式，而采用 Chaos Daemon 直接通过 runc 容器底层命令操作 Linux 命名空间，运行用 Rust 开发的 <a href="https://github.com/chaos-mesh/toda">chaos-mesh/toda</a> FUSE 程序（使用 <a href="https://pkg.go.dev/github.com/ethereum/go-ethereum/rpc">JSON-RPC 2.0</a> 协议通信）进行容器 IO 混沌注入。</p>
<p>关注新的 IOChaos 实现，它不会修改 Pod 资源，IOChaos 混沌实验定义被创建时，针对选择器（selector 字段）筛选出的每一个 Pod，对应的一个 PodIoChaos 资源会被创建，PodIoChaos 的<a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/owners-dependents/">属主引用（Owner Reference）</a>为该 Pod。PodIoChaos 同时会被添加上一组 <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/finalizers/">Finalizers</a>，用于在被删除前释放 PodIoChaos 资源。</p>
<pre><code class="language-go">// Apply implements the reconciler.InnerReconciler.Apply
func (r *Reconciler) Apply(ctx context.Context, req ctrl.Request, chaos v1alpha1.InnerObject) error {
	iochaos, ok := chaos.(*v1alpha1.IoChaos)
	if !ok {
		err := errors.New("chaos is not IoChaos")
		r.Log.Error(err, "chaos is not IoChaos", "chaos", chaos)
		return err
	}

	source := iochaos.Namespace + "/" + iochaos.Name
	m := podiochaosmanager.New(source, r.Log, r.Client)

	pods, err := utils.SelectAndFilterPods(ctx, r.Client, r.Reader, &#x26;iochaos.Spec)
	if err != nil {
		r.Log.Error(err, "failed to select and filter pods")
		return err
	}

	r.Log.Info("applying iochaos", "iochaos", iochaos)

	for _, pod := range pods {
		t := m.WithInit(types.NamespacedName{
			Name:      pod.Name,
			Namespace: pod.Namespace,
		})

		// TODO: support chaos on multiple volume
		t.SetVolumePath(iochaos.Spec.VolumePath)
		t.Append(v1alpha1.IoChaosAction{
			Type: iochaos.Spec.Action,
			Filter: v1alpha1.Filter{
				Path:    iochaos.Spec.Path,
				Percent: iochaos.Spec.Percent,
				Methods: iochaos.Spec.Methods,
			},
			Faults: []v1alpha1.IoFault{
				{
					Errno:  iochaos.Spec.Errno,
					Weight: 1,
				},
			},
			Latency:          iochaos.Spec.Delay,
			AttrOverrideSpec: iochaos.Spec.Attr,
			Source:           m.Source,
		})

		key, err := cache.MetaNamespaceKeyFunc(&#x26;pod)
		if err != nil {
			return err
		}
		iochaos.Finalizers = utils.InsertFinalizer(iochaos.Finalizers, key)
	}
	r.Log.Info("commiting updates of podiochaos")
	err = m.Commit(ctx)
	if err != nil {
		r.Log.Error(err, "fail to commit")
		return err
	}
	r.Event(iochaos, v1.EventTypeNormal, utils.EventChaosInjected, "")

	return nil
}
</code></pre>
<p>在 PodIoChaos 资源的控制器中，Controller Manager 会将资源封装成参数，调用 Chaos Daemon 接口进行实际处理。</p>
<pre><code class="language-go">// Apply flushes io configuration on pod
func (h *Handler) Apply(ctx context.Context, chaos *v1alpha1.PodIoChaos) error {
	h.Log.Info("updating io chaos", "pod", chaos.Namespace+"/"+chaos.Name, "spec", chaos.Spec)
    ...
	res, err := pbClient.ApplyIoChaos(ctx, &#x26;pb.ApplyIoChaosRequest{
		Actions:     input,
		Volume:      chaos.Spec.VolumeMountPath,
		ContainerId: containerID,

		Instance:  chaos.Spec.Pid,
		StartTime: chaos.Spec.StartTime,
	})
	if err != nil {
		return err
	}

	chaos.Spec.Pid = res.Instance
	chaos.Spec.StartTime = res.StartTime
	chaos.OwnerReferences = []metav1.OwnerReference{
		{
			APIVersion: pod.APIVersion,
			Kind:       pod.Kind,
			Name:       pod.Name,
			UID:        pod.UID,
		},
	}

	return nil
}
</code></pre>
<p>在 Chaos Daemon 中处理 IOChaos 的代码文件 <code>pkg/chaosdaemon/iochaos_server.go</code> 中，容器需要被注入一个 FUSE 程序，通过 <a href="https://github.com/chaos-mesh/chaos-mesh/issues/2305">#2305</a> Issue，可以了解到执行了 <code>/usr/local/bin/nsexec -l -p /proc/119186/ns/pid -m /proc/119186/ns/mnt -- /usr/local/bin/toda --path /tmp --verbose info</code> 命令，以在特定的 Linux 命名空间（Namespace）下运行 toda 程序，即与 Pod 在同一个命名空间下。</p>
<pre><code class="language-go">func (s *DaemonServer) ApplyIOChaos(ctx context.Context, in *pb.ApplyIOChaosRequest) (*pb.ApplyIOChaosResponse, error) {
	...
	pid, err := s.crClient.GetPidFromContainerID(ctx, in.ContainerId)
	if err != nil {
		log.Error(err, "error while getting PID")
		return nil, err
	}

	args := fmt.Sprintf("--path %s --verbose info", in.Volume)
	log.Info("executing", "cmd", todaBin+" "+args)

	processBuilder := bpm.DefaultProcessBuilder(todaBin, strings.Split(args, " ")...).
		EnableLocalMnt().
		SetIdentifier(in.ContainerId)

	if in.EnterNS {
		processBuilder = processBuilder.SetNS(pid, bpm.MountNS).SetNS(pid, bpm.PidNS)
	}

    ...
    // JSON RPC 调用
	client, err := jrpc.DialIO(ctx, receiver, caller)
	if err != nil {
		return nil, err
	}

	cmd := processBuilder.Build()
    procState, err := s.backgroundProcessManager.StartProcess(cmd)
	if err != nil {
		return nil, err
	}
	...
}
</code></pre>
<p>下面这段代码最终构建了运行的命令，而这些命令正是 <a href="https://github.com/opencontainers/runc">runc</a> 底层的 Namespace 隔离实现<sup><a href="#user-content-fn-4" id="user-content-fnref-4" data-footnote-ref aria-describedby="footnote-label">4</a></sup>：</p>
<pre><code class="language-go">// GetNsPath returns corresponding namespace path
func GetNsPath(pid uint32, typ NsType) string {
	return fmt.Sprintf("%s/%d/ns/%s", DefaultProcPrefix, pid, string(typ))
}

// SetNS sets the namespace of the process
func (b *ProcessBuilder) SetNS(pid uint32, typ NsType) *ProcessBuilder {
	return b.SetNSOpt([]nsOption{{
		Typ:  typ,
		Path: GetNsPath(pid, typ),
	}})
}

// Build builds the process
func (b *ProcessBuilder) Build() *ManagedProcess {
	args := b.args
	cmd := b.cmd

	if len(b.nsOptions) > 0 {
		args = append([]string{"--", cmd}, args...)
		for _, option := range b.nsOptions {
			args = append([]string{"-" + nsArgMap[option.Typ], option.Path}, args...)
		}

		if b.localMnt {
			args = append([]string{"-l"}, args...)
		}
		cmd = nsexecPath
	}
    ...
}
</code></pre>
<h2>控制平面</h2>
<p>Chaos Mesh 是一个开源的混沌工程系统，以 Apache 2.0 协议开源，经过以上分析知道它的能力丰富，并且它的生态良好，维护团队围绕混沌系统研发了用户态文件系统（FUSE）<a href="https://github.com/chaos-mesh/toda">chaos-mesh/toda</a> 、CoreDNS 混沌插件 <a href="https://github.com/chaos-mesh/k8s_dns_chaos">chaos-mesh/k8s_dns_chaos</a>、基于 BPF 的内核错误注入 <a href="https://github.com/chaos-mesh/bpfki">chaos-mesh/bpfki</a> 等。如果能够将这一套混沌工程系统引入企业内部，想必那是非常好吧。</p>
<p>下述如果我想要建立一个面向终端用户的混沌工程平台，在服务端实现的代码应该是怎样的。示例仅为一种实践，并不代表最佳实践，如果想看 Real World 平台的开发实践的话，可以参考 Chaos Mesh 官方的 <a href="https://github.com/chaos-mesh/chaos-mesh/tree/master/pkg/dashboard">Dashboard</a>，内部使用了 <a href="https://github.com/uber-go/fx">uber-go/fx</a> 依赖注入框架和 controller runtime 的 manager 模式。</p>
<h3>要做什么？</h3>
<p><img src="/images/2021-10-01-07.png" alt="Chaos Mesh 的基本工作流原理图"></p>
<p>这里标题虽是控制平面，但查看上述 Chaos Mesh 工作流程图，其实我们需要做的只是实现一个将 YAML 下发到 Kubernetes API 的服务器，复杂的规则校验、规则下发到 Chaos Daemon 的行为是由 Chaos Controller Manager 完成的。想要结合自己的平台使用，只需要对接 CRD 资源创建的过程就足够了。</p>
<p>我们来看一下 PingCAP 官方给出的示例：</p>
<pre><code class="language-go">import (
    "context"

    "github.com/pingcap/chaos-mesh/api/v1alpha1"
    "sigs.k8s.io/controller-runtime/pkg/client"
)

func main() {
    ...
    delay := &#x26;chaosv1alpha1.NetworkChaos{
        Spec: chaosv1alpha1.NetworkChaosSpec{...},
    }
    k8sClient := client.New(conf, client.Options{ Scheme: scheme.Scheme })
    k8sClient.Create(context.TODO(), delay)
    k8sClient.Delete(context.TODO(), delay)
}
</code></pre>
<p>Chaos Mesh 已经提供了所有的 CRD 资源定义对应的 API，我们使用 Kubernetes <a href="https://github.com/kubernetes/community/tree/master/sig-api-machinery">API Machinery SIG</a> 开发的 <a href="https://github.com/kubernetes-sigs/controller-runtime">controller-runtime</a> 来简化与 Kubernetes API 的交互。</p>
<h3>实施混沌</h3>
<p>例如我们想通过程序调用的方式，创建一个 PodKill 资源，该资源被发送到 Kubernetes API Server 后，会经由  Chaos Controller Manager 的 <a href="https://kubernetes.io/zh/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook/">Validating 准入控制器</a>，进行数据校验，若数据格式验证失败，会在创建时返回错误。具体参数可以查阅官方文档<a href="https://chaos-mesh.org/zh/docs/simulate-pod-chaos-on-kubernetes/#%E4%BD%BF%E7%94%A8-yaml-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%88%9B%E5%BB%BA%E5%AE%9E%E9%AA%8C">使用 YAML 配置文件创建实验</a>。</p>
<p><code>NewClient</code> 创建了一个 K8s API Client，可以参考<a href="https://pkg.go.dev/sigs.k8s.io/controller-runtime@v0.10.2/pkg/client#example-New">客户端创建示例</a>。</p>
<pre><code class="language-go">package main

import (
	"context"
	"controlpanel"
	"log"

	"github.com/chaos-mesh/chaos-mesh/api/v1alpha1"
	"github.com/pkg/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

func applyPodKill(name, namespace string, labels map[string]string) error {
	cli, err := controlpanel.NewClient()
	if err != nil {
		return errors.Wrap(err, "create client")
	}

	cr := &#x26;v1alpha1.PodChaos{
		ObjectMeta: metav1.ObjectMeta{
			GenerateName: name,
			Namespace:    namespace,
		},
		Spec: v1alpha1.PodChaosSpec{
			Action: v1alpha1.PodKillAction,
			ContainerSelector: v1alpha1.ContainerSelector{
				PodSelector: v1alpha1.PodSelector{
					Mode: v1alpha1.OnePodMode,
					Selector: v1alpha1.PodSelectorSpec{
						Namespaces:     []string{namespace},
						LabelSelectors: labels,
					},
				},
			},
		},
	}

	if err := cli.Create(context.Background(), cr); err != nil {
		return errors.Wrap(err, "create podkill")
	}

	return nil
}
</code></pre>
<p>运行程序的日志输出为：</p>
<pre><code class="language-bash">I1021 00:51:55.225502   23781 request.go:665] Waited for 1.033116256s due to client-side throttling, not priority and fairness, request: GET:https://***
2021/10/21 00:51:56 apply podkill
</code></pre>
<p>通过 kubectl 查看 PodKill 资源的状态：</p>
<pre><code class="language-bash">$ k describe podchaos.chaos-mesh.org -n dev podkillvjn77
Name:         podkillvjn77
Namespace:    dev
Labels:       &#x3C;none>
Annotations:  &#x3C;none>
API Version:  chaos-mesh.org/v1alpha1
Kind:         PodChaos
Metadata:
  Creation Timestamp:  2021-10-20T16:51:56Z
  Finalizers:
    chaos-mesh/records
  Generate Name:     podkill
  Generation:        7
  Resource Version:  938921488
  Self Link:         /apis/chaos-mesh.org/v1alpha1/namespaces/dev/podchaos/podkillvjn77
  UID:               afbb40b3-ade8-48ba-89db-04918d89fd0b
Spec:
  Action:        pod-kill
  Grace Period:  0
  Mode:          one
  Selector:
    Label Selectors:
      app:  nginx
    Namespaces:
      dev
Status:
  Conditions:
    Reason:
    Status:  False
    Type:    Paused
    Reason:
    Status:  True
    Type:    Selected
    Reason:
    Status:  True
    Type:    AllInjected
    Reason:
    Status:  False
    Type:    AllRecovered
  Experiment:
    Container Records:
      Id:            dev/nginx
      Phase:         Injected
      Selector Key:  .
    Desired Phase:   Run
Events:
  Type    Reason           Age    From          Message
  ----    ------           ----   ----          -------
  Normal  FinalizerInited  6m35s  finalizer     Finalizer has been inited
  Normal  Updated          6m35s  finalizer     Successfully update finalizer of resource
  Normal  Updated          6m35s  records       Successfully update records of resource
  Normal  Updated          6m35s  desiredphase  Successfully update desiredPhase of resource
  Normal  Applied          6m35s  records       Successfully apply chaos for dev/nginx
  Normal  Updated          6m35s  records       Successfully update records of resource
</code></pre>
<p>控制面还自然而然要有查询和获取 Chaos 资源的功能，方便平台用户查看到所有的混沌试验的实施状态，对其进行管理。当然，这里无非是调用 REST API 发送 Get/List 请求，但在实践上细节需要留意，敝司就发生过 Controller 每次请求全量的资源数据，造成 K8s API Server 的负载增高。</p>
<p>这里十分推荐阅读 <a href="https://zoetrope.github.io/kubebuilder-training/controller-runtime/client.html">クライアントの使い方</a>，这篇 controller runtime 使用教程，提到了很细节的地方。例如 controller runtime 默认会从多个位置读取 kubeconfig，flag、环境变量、再是自动挂载在 Pod 中的 Service Account，<a href="https://github.com/armosec/kubescape">armosec/kubescape</a> <a href="https://github.com/armosec/kubescape/pull/21">#21</a> PR 也是利用了该特性。这篇教程还包括了如何分页、更新、覆盖对象等常用的操作，我目前还没有看到有哪篇中文、英文教程有这么详细。</p>
<p>Get/List 请求示例：</p>
<pre><code class="language-go">package controlpanel

import (
	"context"

	"github.com/chaos-mesh/chaos-mesh/api/v1alpha1"
	"github.com/pkg/errors"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

func GetPodChaos(name, namespace string) (*v1alpha1.PodChaos, error) {
	cli := mgr.GetClient()

	item := new(v1alpha1.PodChaos)
	if err := cli.Get(context.Background(), client.ObjectKey{Name: name, Namespace: namespace}, item); err != nil {
		return nil, errors.Wrap(err, "get cr")
	}

	return item, nil
}

func ListPodChaos(namespace string, labels map[string]string) ([]v1alpha1.PodChaos, error) {
	cli := mgr.GetClient()

	list := new(v1alpha1.PodChaosList)
	if err := cli.List(context.Background(), list, client.InNamespace(namespace), client.MatchingLabels(labels)); err != nil {
		return nil, err
	}

	return list.Items, nil
}
</code></pre>
<p>示例中使用了 manager，该模式下会启用 cache 机制，避免重复获取大量数据。</p>
<p><img src="/images/2021-10-01-08.png" alt=""></p>
<ol>
<li>获取 Pod</li>
<li>初次获取全量数据（List）</li>
<li>Watch 数据变化时更新缓存</li>
</ol>
<h3>混沌编排</h3>
<p>就如同 CRI 容器运行时提供了强大的底层隔离能力，能够支撑容器的稳定运行，而想要更大规模、更复杂的场景就需要容器编排一样，Chaos Mesh 提供了 Schedule 和 Workflow 功能。<a href="https://chaos-mesh.org/zh/docs/define-scheduling-rules/">Schedule</a> 能够根据设定的 Cron 时间定时、间隔地触发故障，<a href="https://chaos-mesh.org/zh/docs/create-chaos-mesh-workflow/">Workflow</a> 能像 Argo Workflow 一样编排多个故障试验。</p>
<p>当然，Chaos Controller Manager 替我们做了大部分工作，控制面所需要的还是管理这些 YAML 资源，唯一需要考虑的是要给用户提供怎样的功能。</p>
<h3>平台功能</h3>
<p>参考 Chaos Mesh Dashboard，我们需要考虑平台该提供哪些功能给终端用户。</p>
<p><img src="/images/2021-10-01-09.png" alt=""></p>
<p>可能的平台功能点：</p>
<ul>
<li>混沌注入
<ul>
<li>Pod 崩溃</li>
<li>网络故障</li>
<li>负载测试</li>
<li>IO 故障</li>
</ul>
</li>
<li>事件跟踪</li>
<li>关联告警</li>
<li>时序遥测</li>
</ul>
<h2>参阅资料</h2>
<p>本文既是为公司引入新技术进行试探，同时也是自我学习的记录，此前接触的学习材料及撰写本文时查阅的资料，较为优秀的部分列在这里，以便快速查阅。</p>
<ul>
<li><a href="https://qiankunli.github.io/2020/08/10/controller_runtime.html">controller-runtime源码分析</a></li>
<li><a href="https://github.com/zoetrope/kubebuilder-training">つくって学ぶKubebuilder</a>（日文教程）</li>
<li><a href="https://book.kubebuilder.io/">Kubebuilder Book</a> / <a href="https://cloudnative.to/kubebuilder/">中文版</a></li>
<li><a href="https://www.huweihuang.com/kubernetes-notes/code-analysis/kube-controller-manager/sharedIndexInformer.html">kube-controller-manager源码分析（三）之 Informer机制</a></li>
<li><a href="https://segmentfault.com/a/1190000020359577">kubebuilder2.0学习笔记——进阶使用</a></li>
<li><a href="https://www.cnblogs.com/charlieroro/p/11112526.html">client-go和golang源码中的技巧</a></li>
</ul>
<section data-footnotes class="footnotes"><h2 class="sr-only" id="footnote-label">Footnotes</h2>
<ol>
<li id="user-content-fn-1">
<p><a href="https://pingcap.com/zh/blog/chaos-mesh">Chaos Mesh - 让应用跟混沌在 Kubernetes 上共舞</a> <a href="#user-content-fnref-1" data-footnote-backref="" aria-label="Back to reference 1" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-2">
<p><a href="https://xie.infoq.cn/article/655c0893ed150ff65f2b7a16f">自制文件系统 —— 02 开发者的福音，FUSE 文件系统</a> <a href="#user-content-fnref-2" data-footnote-backref="" aria-label="Back to reference 2" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-3">
<p><a href="http://www.freeoa.net/product/apptool/stress-ng_3456.html">系统压力测试工具-stress-ng</a> <a href="#user-content-fnref-3" data-footnote-backref="" aria-label="Back to reference 3" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-4">
<p><a href="https://www.jianshu.com/p/a73f984f53b5">RunC 源码通读指南之 NameSpace</a> <a href="#user-content-fnref-4" data-footnote-backref="" aria-label="Back to reference 4" class="data-footnote-backref">↩</a></p>
</li>
</ol>
</section></article><script src="/_next/static/chunks/webpack-072f062dc024cc52.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[5803,[],\"\"]\n3:I[695,[],\"\"]\n5:I[2576,[],\"OutletBoundary\"]\n7:I[2576,[],\"MetadataBoundary\"]\n9:I[2576,[],\"ViewportBoundary\"]\nb:I[7614,[],\"\"]\n:HL[\"/_next/static/css/c3b55921f92a131e.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"m6A-Tc8bnMTyQXrjIe8w0\",\"p\":\"\",\"c\":[\"\",\"2021\",\"10\",\"%25E5%259C%25A8-kubernetes-%25E9%259B%2586%25E7%25BE%25A4%25E5%2588%25B6%25E9%2580%25A0%25E6%25B7%25B7%25E6%25B2%258C\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"(posts)\",{\"children\":[[\"year\",\"2021\",\"d\"],{\"children\":[[\"month\",\"10\",\"d\"],{\"children\":[[\"slug\",\"%25E5%259C%25A8-kubernetes-%25E9%259B%2586%25E7%25BE%25A4%25E5%2588%25B6%25E9%2580%25A0%25E6%25B7%25B7%25E6%25B2%258C\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/c3b55921f92a131e.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"children\":[\"$\",\"body\",null,{\"className\":\"antialiased\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[],[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[\"(posts)\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(posts)\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[],[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":\"$0:f:0:1:1:props:children:1:props:children:props:children:props:notFound:1:1:props:style\",\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":\"$0:f:0:1:1:props:children:1:props:children:props:children:props:notFound:1:1:props:children:props:children:1:props:style\",\"children\":404}],[\"$\",\"div\",null,{\"style\":\"$0:f:0:1:1:props:children:1:props:children:props:children:props:notFound:1:1:props:children:props:children:2:props:style\",\"children\":[\"$\",\"h2\",null,{\"style\":\"$0:f:0:1:1:props:children:1:props:children:props:children:props:notFound:1:1:props:children:props:children:2:props:children:props:style\",\"children\":\"This page could not be found.\"}]}]]}]}]]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"year\",\"2021\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(posts)\",\"children\",\"$0:f:0:1:2:children:2:children:0\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"month\",\"10\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(posts)\",\"children\",\"$0:f:0:1:2:children:2:children:0\",\"children\",\"$0:f:0:1:2:children:2:children:2:children:0\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"slug\",\"%25E5%259C%25A8-kubernetes-%25E9%259B%2586%25E7%25BE%25A4%25E5%2588%25B6%25E9%2580%25A0%25E6%25B7%25B7%25E6%25B2%258C\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(posts)\",\"children\",\"$0:f:0:1:2:children:2:children:0\",\"children\",\"$0:f:0:1:2:children:2:children:2:children:0\",\"children\",\"$0:f:0:1:2:children:2:children:2:children:2:children:0\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L4\",null,[\"$\",\"$L5\",null,{\"children\":\"$L6\"}]]}],{},null,false]},null,false]},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"0iU07LuwJUF1A9FmUNoDH\",{\"children\":[[\"$\",\"$L7\",null,{\"children\":\"$L8\"}],[\"$\",\"$L9\",null,{\"children\":\"$La\"}],null]}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$b\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"a:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n8:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"1\",{\"children\":\"Mayo Rocks!\"}],[\"$\",\"meta\",\"2\",{\"name\":\"description\",\"content\":\"Mayo's Blog\"}],[\"$\",\"link\",\"3\",{\"rel\":\"icon\",\"href\":\"/icon.png?14d5a92fbe70e82a\",\"type\":\"image/png\",\"sizes\":\"460x460\"}]]\n"])</script><script>self.__next_f.push([1,"6:null\n"])</script><script>self.__next_f.push([1,"c:T8a63,"])</script><script>self.__next_f.push([1,"\u003cp\u003e\u003cem\u003eChaos Mesh 工作原理与控制平面开发\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eChaos Mesh 是由 TiDB 背后的 PingCAP 公司开发，运行在 Kubernetes 上的混沌工程（Chaos Engineering）系统。简而言之，Chaos Mesh 通过运行在 K8s 集群中的“特权”容器，依据 CRD 资源中的测试场景，在集群中制造浑沌（模拟故障）\u003csup\u003e\u003ca href=\"#user-content-fn-1\" id=\"user-content-fnref-1\" data-footnote-ref aria-describedby=\"footnote-label\"\u003e1\u003c/a\u003e\u003c/sup\u003e。\u003c/p\u003e\n\u003cp\u003e本文的首要目的是试探 Chaos Mesh 混沌工程系统能否整合进企业的云平台中、通过怎样的方式结合，打造一体化式的体验。如果你缺乏基础知识，要想对 Chaos Mesh 的架构有宏观上的认识，请参阅文末尾注中的链接。\u003c/p\u003e\n\u003cp\u003e本文试验代码位于 \u003ca href=\"https://github.com/mayocream/chaos-mesh-controlpanel-demo\"\u003emayocream/chaos-mesh-controlpanel-demo\u003c/a\u003e 仓库。\u003c/p\u003e\n\u003ch2\u003e怎么捣乱？\u003c/h2\u003e\n\u003ch3\u003e特权\u003c/h3\u003e\n\u003cp\u003e上面提到 Chaos Mesh 运行 Kubernetes 特权容器来制造故障。Daemon Set 方式运行的 Pod 授权了容器运行时的\u003ca href=\"https://kubernetes.io/zh/docs/concepts/policy/pod-security-policy/#capabilities\"\u003e权能字（Capabilities）\u003c/a\u003e。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-yaml\"\u003eapiVersion: apps/v1\nkind: DaemonSet\nspec:\n  template:\n    metadata: ...\n    spec:\n      containers:\n        - name: chaos-daemon\n          securityContext:\n            {{- if .Values.chaosDaemon.privileged }}\n            privileged: true\n            capabilities:\n              add:\n                - SYS_PTRACE\n            {{- else }}\n            capabilities:\n              add:\n                - SYS_PTRACE\n                - NET_ADMIN\n                - MKNOD\n                - SYS_CHROOT\n                - SYS_ADMIN\n                - KILL\n                # CAP_IPC_LOCK is used to lock memory\n                - IPC_LOCK\n            {{- end }}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e这些 Linux 权能字用于授予容器特权，以创建和访问 \u003ccode\u003e/dev/fuse\u003c/code\u003e FUSE 管道\u003csup\u003e\u003ca href=\"#user-content-fn-2\" id=\"user-content-fnref-2\" data-footnote-ref aria-describedby=\"footnote-label\"\u003e2\u003c/a\u003e\u003c/sup\u003e（FUSE 是 Linux 用户空间文件系统接口，它使无特权的用户能够无需编辑内核代码而创建自己的文件系统）。\u003c/p\u003e\n\u003cp\u003e参阅 \u003ca href=\"https://github.com/chaos-mesh/chaos-mesh/pull/1109\"\u003e#1109\u003c/a\u003e Pull Request，Daemon Set 程序使用 CGO 调用 Linux \u003ccode\u003emakedev\u003c/code\u003e 函数创建 FUSE 管道。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// #include \u0026#x3C;sys/sysmacros.h\u003e\n// #include \u0026#x3C;sys/types.h\u003e\n// // makedev is a macro, so a wrapper is needed\n// dev_t Makedev(unsigned int maj, unsigned int min) {\n//   return makedev(maj, min);\n// }\n\n// EnsureFuseDev ensures /dev/fuse exists. If not, it will create one\nfunc EnsureFuseDev() {\n\tif _, err := os.Open(\"/dev/fuse\"); os.IsNotExist(err) {\n\t\t// 10, 229 according to https://www.kernel.org/doc/Documentation/admin-guide/devices.txt\n\t\tfuse := C.Makedev(10, 229)\n\t\tsyscall.Mknod(\"/dev/fuse\", 0o666|syscall.S_IFCHR, int(fuse))\n\t}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e同时在 \u003ca href=\"https://github.com/chaos-mesh/chaos-mesh/pull/1453\"\u003e#1453\u003c/a\u003e PR 中，Chaos Daemon 默认启用特权模式，即容器的 \u003ccode\u003esecurityContext\u003c/code\u003e 中设置 \u003ccode\u003eprivileged: true\u003c/code\u003e。\u003c/p\u003e\n\u003ch3\u003e杀死 Pod\u003c/h3\u003e\n\u003cp\u003ePodKill、PodFailure、ContainerKill 都归属于 PodChaos 类别下，PodKill 是随机杀死 Pod。\u003c/p\u003e\n\u003cp\u003ePodKill 的具体实现其实是通过调用 API Server 发送 Kill 命令。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003eimport (\n\t\"context\"\n\n\tv1 \"k8s.io/api/core/v1\"\n\t\"sigs.k8s.io/controller-runtime/pkg/client\"\n)\n\ntype Impl struct {\n\tclient.Client\n}\n\nfunc (impl *Impl) Apply(ctx context.Context, index int, records []*v1alpha1.Record, obj v1alpha1.InnerObject) (v1alpha1.Phase, error) {\n\t...\n\terr = impl.Get(ctx, namespacedName, \u0026#x26;pod)\n\tif err != nil {\n\t\t// TODO: handle this error\n\t\treturn v1alpha1.NotInjected, err\n\t}\n\n\terr = impl.Delete(ctx, \u0026#x26;pod, \u0026#x26;client.DeleteOptions{\n\t\tGracePeriodSeconds: \u0026#x26;podchaos.Spec.GracePeriod, // PeriodSeconds has to be set specifically\n\t})\n\t...\n\treturn v1alpha1.Injected, nil\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ccode\u003eGracePeriodSeconds\u003c/code\u003e 参数适用于 K8s \u003ca href=\"https://kubernetes.io/zh/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination-forced\"\u003e强制终止 Pod\u003c/a\u003e。例如在需要快速删除 Pod 时，我们使用 \u003ccode\u003ekubectl delete pod --grace-period=0 --force\u003c/code\u003e 命令。\u003c/p\u003e\n\u003cp\u003ePodFailure 是通过 Patch Pod 对象资源，用错误的镜像替换 Pod 中的镜像。Chaos 只修改了 \u003ccode\u003econtainers\u003c/code\u003e 和 \u003ccode\u003einitContainers\u003c/code\u003e 的 \u003ccode\u003eimage\u003c/code\u003e 字段，这也是因为 Pod 大部分字段是无法更改的，详情可以参阅 \u003ca href=\"https://kubernetes.io/zh/docs/concepts/workloads/pods/#pod-update-and-replacement\"\u003ePod 更新与替换\u003c/a\u003e。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003efunc (impl *Impl) Apply(ctx context.Context, index int, records []*v1alpha1.Record, obj v1alpha1.InnerObject) (v1alpha1.Phase, error) {\n\t...\n\tpod := origin.DeepCopy()\n\tfor index := range pod.Spec.Containers {\n\t\toriginImage := pod.Spec.Containers[index].Image\n\t\tname := pod.Spec.Containers[index].Name\n\n\t\tkey := annotation.GenKeyForImage(podchaos, name, false)\n\t\tif pod.Annotations == nil {\n\t\t\tpod.Annotations = make(map[string]string)\n\t\t}\n\n\t\t// If the annotation is already existed, we could skip the reconcile for this container\n\t\tif _, ok := pod.Annotations[key]; ok {\n\t\t\tcontinue\n\t\t}\n\t\tpod.Annotations[key] = originImage\n\t\tpod.Spec.Containers[index].Image = config.ControllerCfg.PodFailurePauseImage\n\t}\n\n\tfor index := range pod.Spec.InitContainers {\n\t\toriginImage := pod.Spec.InitContainers[index].Image\n\t\tname := pod.Spec.InitContainers[index].Name\n\n\t\tkey := annotation.GenKeyForImage(podchaos, name, true)\n\t\tif pod.Annotations == nil {\n\t\t\tpod.Annotations = make(map[string]string)\n\t\t}\n\n\t\t// If the annotation is already existed, we could skip the reconcile for this container\n\t\tif _, ok := pod.Annotations[key]; ok {\n\t\t\tcontinue\n\t\t}\n\t\tpod.Annotations[key] = originImage\n\t\tpod.Spec.InitContainers[index].Image = config.ControllerCfg.PodFailurePauseImage\n\t}\n\n\terr = impl.Patch(ctx, pod, client.MergeFrom(\u0026#x26;origin))\n\tif err != nil {\n\t\t// TODO: handle this error\n\t\treturn v1alpha1.NotInjected, err\n\t}\n\n\treturn v1alpha1.Injected, nil\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e默认用于引发故障的容器镜像是 \u003ccode\u003egcr.io/google-containers/pause:latest\u003c/code\u003e， 如果在国内环境使用，大概率会水土不服，可以将 \u003ccode\u003egcr.io\u003c/code\u003e 替换为 \u003ccode\u003eregistry.aliyuncs.com\u003c/code\u003e。\u003c/p\u003e\n\u003cp\u003eContainerKill 不同于 PodKill 和 PodFailure，后两个都是通过 K8s API Server 控制 Pod 生命周期，而 ContainerKill 是通过运行在集群 Node 上的 Chaos Daemon 程序操作完成。具体来说，ContainerKill 通过 Chaos Controller Manager 运行客户端向 Chaos Daemon 发起 grpc 调用。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003efunc (b *ChaosDaemonClientBuilder) Build(ctx context.Context, pod *v1.Pod) (chaosdaemonclient.ChaosDaemonClientInterface, error) {\n\t...\n\tdaemonIP, err := b.FindDaemonIP(ctx, pod)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbuilder := grpcUtils.Builder(daemonIP, config.ControllerCfg.ChaosDaemonPort).WithDefaultTimeout()\n\tif config.ControllerCfg.TLSConfig.ChaosMeshCACert != \"\" {\n\t\tbuilder.TLSFromFile(config.ControllerCfg.TLSConfig.ChaosMeshCACert, config.ControllerCfg.TLSConfig.ChaosDaemonClientCert, config.ControllerCfg.TLSConfig.ChaosDaemonClientKey)\n\t} else {\n\t\tbuilder.Insecure()\n\t}\n\tcc, err := builder.Build()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn chaosdaemonclient.New(cc), nil\n}\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e向 Chaos Daemon 发送命令时会依据 Pod 信息创建对应的客户端，例如要控制某个 Node 上的 Pod，会获取该 Pod 所在 Node 的 ClusterIP，以创建客户端。如果 TLS 证书配置存在，Controller Manager 会为客户端添加 TLS 证书。\u003c/p\u003e\n\u003cp\u003eChaos Daemon 在启动时如果有 TLS 证书，会附加证书以启用 grpcs。TLS 校验配置 \u003ccode\u003eRequireAndVerifyClientCert\u003c/code\u003e 表示启用双向 TLS 认证（mTLS）。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003efunc newGRPCServer(containerRuntime string, reg prometheus.Registerer, tlsConf tlsConfig) (*grpc.Server, error) {\n\t...\n\tif tlsConf != (tlsConfig{}) {\n\t\tcaCert, err := ioutil.ReadFile(tlsConf.CaCert)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tcaCertPool := x509.NewCertPool()\n\t\tcaCertPool.AppendCertsFromPEM(caCert)\n\n\t\tserverCert, err := tls.LoadX509KeyPair(tlsConf.Cert, tlsConf.Key)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tcreds := credentials.NewTLS(\u0026#x26;tls.Config{\n\t\t\tCertificates: []tls.Certificate{serverCert},\n\t\t\tClientCAs:    caCertPool,\n\t\t\tClientAuth:   tls.RequireAndVerifyClientCert,\n\t\t})\n\n\t\tgrpcOpts = append(grpcOpts, grpc.Creds(creds))\n\t}\n\n\ts := grpc.NewServer(grpcOpts...)\n\tgrpcMetrics.InitializeMetrics(s)\n\n\tpb.RegisterChaosDaemonServer(s, ds)\n\treflection.Register(s)\n\n\treturn s, nil\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eChaos Daemon 提供了以下 grpc 调用接口：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// ChaosDaemonClient is the client API for ChaosDaemon service.\n//\n// For semantics around ctx use and closing/ending streaming RPCs, please refer to https://godoc.org/google.golang.org/grpc#ClientConn.NewStream.\ntype ChaosDaemonClient interface {\n\tSetTcs(ctx context.Context, in *TcsRequest, opts ...grpc.CallOption) (*empty.Empty, error)\n\tFlushIPSets(ctx context.Context, in *IPSetsRequest, opts ...grpc.CallOption) (*empty.Empty, error)\n\tSetIptablesChains(ctx context.Context, in *IptablesChainsRequest, opts ...grpc.CallOption) (*empty.Empty, error)\n\tSetTimeOffset(ctx context.Context, in *TimeRequest, opts ...grpc.CallOption) (*empty.Empty, error)\n\tRecoverTimeOffset(ctx context.Context, in *TimeRequest, opts ...grpc.CallOption) (*empty.Empty, error)\n\tContainerKill(ctx context.Context, in *ContainerRequest, opts ...grpc.CallOption) (*empty.Empty, error)\n\tContainerGetPid(ctx context.Context, in *ContainerRequest, opts ...grpc.CallOption) (*ContainerResponse, error)\n\tExecStressors(ctx context.Context, in *ExecStressRequest, opts ...grpc.CallOption) (*ExecStressResponse, error)\n\tCancelStressors(ctx context.Context, in *CancelStressRequest, opts ...grpc.CallOption) (*empty.Empty, error)\n\tApplyIOChaos(ctx context.Context, in *ApplyIOChaosRequest, opts ...grpc.CallOption) (*ApplyIOChaosResponse, error)\n\tApplyHttpChaos(ctx context.Context, in *ApplyHttpChaosRequest, opts ...grpc.CallOption) (*ApplyHttpChaosResponse, error)\n\tSetDNSServer(ctx context.Context, in *SetDNSServerRequest, opts ...grpc.CallOption) (*empty.Empty, error)\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e网络故障\u003c/h3\u003e\n\u003cp\u003e从最初的 \u003ca href=\"https://github.com/chaos-mesh/chaos-mesh/pull/41\"\u003e#41\u003c/a\u003e PR 中，可以清晰地了解到，Chaos Mesh 的网络错误注入是通过调用 \u003ccode\u003epbClient.SetNetem\u003c/code\u003e 方法，将参数封装成请求，交给 Node 上的 Chaos Daemon 处理的。\u003c/p\u003e\n\u003cp\u003e（注：这是 2019 年初期的代码，随着项目发展，代码中的函数已分散到不同的文件中）\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003efunc (r *Reconciler) applyPod(ctx context.Context, pod *v1.Pod, networkchaos *v1alpha1.NetworkChaos) error {\n\t...\n\tpbClient := pb.NewChaosDaemonClient(c)\n\n\tcontainerId := pod.Status.ContainerStatuses[0].ContainerID\n\n\tnetem, err := spec.ToNetem()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t_, err = pbClient.SetNetem(ctx, \u0026#x26;pb.NetemRequest{\n\t\tContainerId: containerId,\n\t\tNetem:       netem,\n\t})\n\n\treturn err\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e同时在 \u003ccode\u003epkg/chaosdaemon\u003c/code\u003e 包中，我们能看到 Chaos Daemon 处理请求的方法。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003efunc (s *Server) SetNetem(ctx context.Context, in *pb.NetemRequest) (*empty.Empty, error) {\n\tlog.Info(\"Set netem\", \"Request\", in)\n\n\tpid, err := s.crClient.GetPidFromContainerID(ctx, in.ContainerId)\n\n\tif err != nil {\n\t\treturn nil, status.Errorf(codes.Internal, \"get pid from containerID error: %v\", err)\n\t}\n\n\tif err := Apply(in.Netem, pid); err != nil {\n\t\treturn nil, status.Errorf(codes.Internal, \"netem apply error: %v\", err)\n\t}\n\n\treturn \u0026#x26;empty.Empty{}, nil\n}\n\n// Apply applies a netem on eth0 in pid related namespace\nfunc Apply(netem *pb.Netem, pid uint32) error {\n\tlog.Info(\"Apply netem on PID\", \"pid\", pid)\n\n\tns, err := netns.GetFromPath(GenNetnsPath(pid))\n\tif err != nil {\n\t\tlog.Error(err, \"failed to find network namespace\", \"pid\", pid)\n\t\treturn errors.Trace(err)\n\t}\n\tdefer ns.Close()\n\n\thandle, err := netlink.NewHandleAt(ns)\n\tif err != nil {\n\t\tlog.Error(err, \"failed to get handle at network namespace\", \"network namespace\", ns)\n\t\treturn err\n\t}\n\n\tlink, err := handle.LinkByName(\"eth0\") // TODO: check whether interface name is eth0\n\tif err != nil {\n\t\tlog.Error(err, \"failed to find eth0 interface\")\n\t\treturn errors.Trace(err)\n\t}\n\n\tnetemQdisc := netlink.NewNetem(netlink.QdiscAttrs{\n\t\tLinkIndex: link.Attrs().Index,\n\t\tHandle:    netlink.MakeHandle(1, 0),\n\t\tParent:    netlink.HANDLE_ROOT,\n\t}, ToNetlinkNetemAttrs(netem))\n\n\tif err = handle.QdiscAdd(netemQdisc); err != nil {\n\t\tif !strings.Contains(err.Error(), \"file exists\") {\n\t\t\tlog.Error(err, \"failed to add Qdisc\")\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e最终使用 \u003ca href=\"https://github.com/vishvananda/netlink\"\u003evishvananda/netlink\u003c/a\u003e 库操作 Linux 网络接口来完成工作。\u003c/p\u003e\n\u003cp\u003e至于为什么没有详细说明使用了什么命令，进行了哪些操作，是因为我也不会。待我恶补完网络知识后再来回顾吧。不过这里能够知道，NetworkChaos 混沌类型，操作了Linux 宿主机网络来制造混沌，包含 iptables、ipset 等工具。\u003c/p\u003e\n\u003cp\u003e在 Chaos Daemon 的 Dockerfile 中，可以看到其依赖的 Linux 工具链：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-dockerfile\"\u003eRUN apt-get update \u0026#x26;\u0026#x26; \\\n    apt-get install -y tzdata iptables ipset stress-ng iproute2 fuse util-linux procps curl \u0026#x26;\u0026#x26; \\\n    rm -rf /var/lib/apt/lists/*\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e压力测试\u003c/h3\u003e\n\u003cp\u003eStressChaos 类型的混沌也是由 Chaos Daemon 实施的，Controller Manager 计算好规则后就将任务下发到具体的 Daemon 上。拼装的参数如下，这些参数会组合成命令执行的参数，附加到 \u003ccode\u003estress-ng\u003c/code\u003e 命令后执行\u003csup\u003e\u003ca href=\"#user-content-fn-3\" id=\"user-content-fnref-3\" data-footnote-ref aria-describedby=\"footnote-label\"\u003e3\u003c/a\u003e\u003c/sup\u003e。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// Normalize the stressors to comply with stress-ng\nfunc (in *Stressors) Normalize() (string, error) {\n\tstressors := \"\"\n\tif in.MemoryStressor != nil \u0026#x26;\u0026#x26; in.MemoryStressor.Workers != 0 {\n\t\tstressors += fmt.Sprintf(\" --vm %d --vm-keep\", in.MemoryStressor.Workers)\n\t\tif len(in.MemoryStressor.Size) != 0 {\n\t\t\tif in.MemoryStressor.Size[len(in.MemoryStressor.Size)-1] != '%' {\n\t\t\t\tsize, err := units.FromHumanSize(string(in.MemoryStressor.Size))\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn \"\", err\n\t\t\t\t}\n\t\t\t\tstressors += fmt.Sprintf(\" --vm-bytes %d\", size)\n\t\t\t} else {\n\t\t\t\tstressors += fmt.Sprintf(\" --vm-bytes %s\",\n\t\t\t\t\tin.MemoryStressor.Size)\n\t\t\t}\n\t\t}\n\n\t\tif in.MemoryStressor.Options != nil {\n\t\t\tfor _, v := range in.MemoryStressor.Options {\n\t\t\t\tstressors += fmt.Sprintf(\" %v \", v)\n\t\t\t}\n\t\t}\n\t}\n\tif in.CPUStressor != nil \u0026#x26;\u0026#x26; in.CPUStressor.Workers != 0 {\n\t\tstressors += fmt.Sprintf(\" --cpu %d\", in.CPUStressor.Workers)\n\t\tif in.CPUStressor.Load != nil {\n\t\t\tstressors += fmt.Sprintf(\" --cpu-load %d\",\n\t\t\t\t*in.CPUStressor.Load)\n\t\t}\n\n\t\tif in.CPUStressor.Options != nil {\n\t\t\tfor _, v := range in.CPUStressor.Options {\n\t\t\t\tstressors += fmt.Sprintf(\" %v \", v)\n\t\t\t}\n\t\t}\n\t}\n\treturn stressors, nil\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eChaos Daemon 服务端处理函数中调用 Go 官方包 \u003ccode\u003eos/exec\u003c/code\u003e 执行命令，再大段贴代码就没有意思了，具体可以阅读 \u003ca href=\"https://github.com/chaos-mesh/chaos-mesh/blob/98af3a0e7832a4971d6b133a32069539d982ef0a/pkg/chaosdaemon/stress_server_linux.go#L33\"\u003epkg/chaosdaemon/stress_server_linux.go\u003c/a\u003e 文件。同名文件还有以 darwin 结尾的，推测是为了在 macOS 上开发调试方便。\u003c/p\u003e\n\u003cp\u003e既然都做了兼容，那为什么没有 windows 版本的，气冷抖（不过我也不用 Windows 进行开发调试）。\u003c/p\u003e\n\u003cp\u003e代码中使用 \u003ca href=\"https://github.com/shirou/gopsutil\"\u003eshirou/gopsutil\u003c/a\u003e 包获取 PID 进程状态，并读取了 stdout、stderr 等标准输出，这种处理模式我在 \u003ca href=\"https://github.com/hashicorp/go-plugin\"\u003ehashicorp/go-plugin\u003c/a\u003e 见过，go-plugin 在这方面做得更加优秀。我的另一篇文章 \u003ca href=\"https://shoujo.ink/2021/09/dkron-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/\"\u003eDkron 源码分析\u003c/a\u003e 中提到了它。\u003c/p\u003e\n\u003ch3\u003eIO 注入\u003c/h3\u003e\n\u003cp\u003e开篇就提到了 Chaos Mesh 使用了特权容器，用于挂载宿主机上的 FUSE 设备 \u003ccode\u003e/dev/fuse\u003c/code\u003e。\u003c/p\u003e\n\u003cp\u003e看到这里，我铁定以为 Chaos Mesh 是使用 \u003ca href=\"https://kubernetes.io/zh/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook\"\u003eMutating 准入控制器\u003c/a\u003e进行 Sidecar 容器的注入，挂载 FUSE 设备，或修改 Pod Volumes Mount 等配置，然后它的实现方式与直观上不同。\u003c/p\u003e\n\u003cp\u003e仔细看了 \u003ca href=\"https://github.com/chaos-mesh/chaos-mesh/pull/826\"\u003e#826\u003c/a\u003e PR，这个 PR 引入了新的 IOChaos 的实现，避免使用 Sidecar 注入的方式，而采用 Chaos Daemon 直接通过 runc 容器底层命令操作 Linux 命名空间，运行用 Rust 开发的 \u003ca href=\"https://github.com/chaos-mesh/toda\"\u003echaos-mesh/toda\u003c/a\u003e FUSE 程序（使用 \u003ca href=\"https://pkg.go.dev/github.com/ethereum/go-ethereum/rpc\"\u003eJSON-RPC 2.0\u003c/a\u003e 协议通信）进行容器 IO 混沌注入。\u003c/p\u003e\n\u003cp\u003e关注新的 IOChaos 实现，它不会修改 Pod 资源，IOChaos 混沌实验定义被创建时，针对选择器（selector 字段）筛选出的每一个 Pod，对应的一个 PodIoChaos 资源会被创建，PodIoChaos 的\u003ca href=\"https://kubernetes.io/docs/concepts/overview/working-with-objects/owners-dependents/\"\u003e属主引用（Owner Reference）\u003c/a\u003e为该 Pod。PodIoChaos 同时会被添加上一组 \u003ca href=\"https://kubernetes.io/docs/concepts/overview/working-with-objects/finalizers/\"\u003eFinalizers\u003c/a\u003e，用于在被删除前释放 PodIoChaos 资源。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// Apply implements the reconciler.InnerReconciler.Apply\nfunc (r *Reconciler) Apply(ctx context.Context, req ctrl.Request, chaos v1alpha1.InnerObject) error {\n\tiochaos, ok := chaos.(*v1alpha1.IoChaos)\n\tif !ok {\n\t\terr := errors.New(\"chaos is not IoChaos\")\n\t\tr.Log.Error(err, \"chaos is not IoChaos\", \"chaos\", chaos)\n\t\treturn err\n\t}\n\n\tsource := iochaos.Namespace + \"/\" + iochaos.Name\n\tm := podiochaosmanager.New(source, r.Log, r.Client)\n\n\tpods, err := utils.SelectAndFilterPods(ctx, r.Client, r.Reader, \u0026#x26;iochaos.Spec)\n\tif err != nil {\n\t\tr.Log.Error(err, \"failed to select and filter pods\")\n\t\treturn err\n\t}\n\n\tr.Log.Info(\"applying iochaos\", \"iochaos\", iochaos)\n\n\tfor _, pod := range pods {\n\t\tt := m.WithInit(types.NamespacedName{\n\t\t\tName:      pod.Name,\n\t\t\tNamespace: pod.Namespace,\n\t\t})\n\n\t\t// TODO: support chaos on multiple volume\n\t\tt.SetVolumePath(iochaos.Spec.VolumePath)\n\t\tt.Append(v1alpha1.IoChaosAction{\n\t\t\tType: iochaos.Spec.Action,\n\t\t\tFilter: v1alpha1.Filter{\n\t\t\t\tPath:    iochaos.Spec.Path,\n\t\t\t\tPercent: iochaos.Spec.Percent,\n\t\t\t\tMethods: iochaos.Spec.Methods,\n\t\t\t},\n\t\t\tFaults: []v1alpha1.IoFault{\n\t\t\t\t{\n\t\t\t\t\tErrno:  iochaos.Spec.Errno,\n\t\t\t\t\tWeight: 1,\n\t\t\t\t},\n\t\t\t},\n\t\t\tLatency:          iochaos.Spec.Delay,\n\t\t\tAttrOverrideSpec: iochaos.Spec.Attr,\n\t\t\tSource:           m.Source,\n\t\t})\n\n\t\tkey, err := cache.MetaNamespaceKeyFunc(\u0026#x26;pod)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tiochaos.Finalizers = utils.InsertFinalizer(iochaos.Finalizers, key)\n\t}\n\tr.Log.Info(\"commiting updates of podiochaos\")\n\terr = m.Commit(ctx)\n\tif err != nil {\n\t\tr.Log.Error(err, \"fail to commit\")\n\t\treturn err\n\t}\n\tr.Event(iochaos, v1.EventTypeNormal, utils.EventChaosInjected, \"\")\n\n\treturn nil\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e在 PodIoChaos 资源的控制器中，Controller Manager 会将资源封装成参数，调用 Chaos Daemon 接口进行实际处理。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// Apply flushes io configuration on pod\nfunc (h *Handler) Apply(ctx context.Context, chaos *v1alpha1.PodIoChaos) error {\n\th.Log.Info(\"updating io chaos\", \"pod\", chaos.Namespace+\"/\"+chaos.Name, \"spec\", chaos.Spec)\n    ...\n\tres, err := pbClient.ApplyIoChaos(ctx, \u0026#x26;pb.ApplyIoChaosRequest{\n\t\tActions:     input,\n\t\tVolume:      chaos.Spec.VolumeMountPath,\n\t\tContainerId: containerID,\n\n\t\tInstance:  chaos.Spec.Pid,\n\t\tStartTime: chaos.Spec.StartTime,\n\t})\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tchaos.Spec.Pid = res.Instance\n\tchaos.Spec.StartTime = res.StartTime\n\tchaos.OwnerReferences = []metav1.OwnerReference{\n\t\t{\n\t\t\tAPIVersion: pod.APIVersion,\n\t\t\tKind:       pod.Kind,\n\t\t\tName:       pod.Name,\n\t\t\tUID:        pod.UID,\n\t\t},\n\t}\n\n\treturn nil\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e在 Chaos Daemon 中处理 IOChaos 的代码文件 \u003ccode\u003epkg/chaosdaemon/iochaos_server.go\u003c/code\u003e 中，容器需要被注入一个 FUSE 程序，通过 \u003ca href=\"https://github.com/chaos-mesh/chaos-mesh/issues/2305\"\u003e#2305\u003c/a\u003e Issue，可以了解到执行了 \u003ccode\u003e/usr/local/bin/nsexec -l -p /proc/119186/ns/pid -m /proc/119186/ns/mnt -- /usr/local/bin/toda --path /tmp --verbose info\u003c/code\u003e 命令，以在特定的 Linux 命名空间（Namespace）下运行 toda 程序，即与 Pod 在同一个命名空间下。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003efunc (s *DaemonServer) ApplyIOChaos(ctx context.Context, in *pb.ApplyIOChaosRequest) (*pb.ApplyIOChaosResponse, error) {\n\t...\n\tpid, err := s.crClient.GetPidFromContainerID(ctx, in.ContainerId)\n\tif err != nil {\n\t\tlog.Error(err, \"error while getting PID\")\n\t\treturn nil, err\n\t}\n\n\targs := fmt.Sprintf(\"--path %s --verbose info\", in.Volume)\n\tlog.Info(\"executing\", \"cmd\", todaBin+\" \"+args)\n\n\tprocessBuilder := bpm.DefaultProcessBuilder(todaBin, strings.Split(args, \" \")...).\n\t\tEnableLocalMnt().\n\t\tSetIdentifier(in.ContainerId)\n\n\tif in.EnterNS {\n\t\tprocessBuilder = processBuilder.SetNS(pid, bpm.MountNS).SetNS(pid, bpm.PidNS)\n\t}\n\n    ...\n    // JSON RPC 调用\n\tclient, err := jrpc.DialIO(ctx, receiver, caller)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcmd := processBuilder.Build()\n    procState, err := s.backgroundProcessManager.StartProcess(cmd)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t...\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e下面这段代码最终构建了运行的命令，而这些命令正是 \u003ca href=\"https://github.com/opencontainers/runc\"\u003erunc\u003c/a\u003e 底层的 Namespace 隔离实现\u003csup\u003e\u003ca href=\"#user-content-fn-4\" id=\"user-content-fnref-4\" data-footnote-ref aria-describedby=\"footnote-label\"\u003e4\u003c/a\u003e\u003c/sup\u003e：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// GetNsPath returns corresponding namespace path\nfunc GetNsPath(pid uint32, typ NsType) string {\n\treturn fmt.Sprintf(\"%s/%d/ns/%s\", DefaultProcPrefix, pid, string(typ))\n}\n\n// SetNS sets the namespace of the process\nfunc (b *ProcessBuilder) SetNS(pid uint32, typ NsType) *ProcessBuilder {\n\treturn b.SetNSOpt([]nsOption{{\n\t\tTyp:  typ,\n\t\tPath: GetNsPath(pid, typ),\n\t}})\n}\n\n// Build builds the process\nfunc (b *ProcessBuilder) Build() *ManagedProcess {\n\targs := b.args\n\tcmd := b.cmd\n\n\tif len(b.nsOptions) \u003e 0 {\n\t\targs = append([]string{\"--\", cmd}, args...)\n\t\tfor _, option := range b.nsOptions {\n\t\t\targs = append([]string{\"-\" + nsArgMap[option.Typ], option.Path}, args...)\n\t\t}\n\n\t\tif b.localMnt {\n\t\t\targs = append([]string{\"-l\"}, args...)\n\t\t}\n\t\tcmd = nsexecPath\n\t}\n    ...\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e控制平面\u003c/h2\u003e\n\u003cp\u003eChaos Mesh 是一个开源的混沌工程系统，以 Apache 2.0 协议开源，经过以上分析知道它的能力丰富，并且它的生态良好，维护团队围绕混沌系统研发了用户态文件系统（FUSE）\u003ca href=\"https://github.com/chaos-mesh/toda\"\u003echaos-mesh/toda\u003c/a\u003e 、CoreDNS 混沌插件 \u003ca href=\"https://github.com/chaos-mesh/k8s_dns_chaos\"\u003echaos-mesh/k8s_dns_chaos\u003c/a\u003e、基于 BPF 的内核错误注入 \u003ca href=\"https://github.com/chaos-mesh/bpfki\"\u003echaos-mesh/bpfki\u003c/a\u003e 等。如果能够将这一套混沌工程系统引入企业内部，想必那是非常好吧。\u003c/p\u003e\n\u003cp\u003e下述如果我想要建立一个面向终端用户的混沌工程平台，在服务端实现的代码应该是怎样的。示例仅为一种实践，并不代表最佳实践，如果想看 Real World 平台的开发实践的话，可以参考 Chaos Mesh 官方的 \u003ca href=\"https://github.com/chaos-mesh/chaos-mesh/tree/master/pkg/dashboard\"\u003eDashboard\u003c/a\u003e，内部使用了 \u003ca href=\"https://github.com/uber-go/fx\"\u003euber-go/fx\u003c/a\u003e 依赖注入框架和 controller runtime 的 manager 模式。\u003c/p\u003e\n\u003ch3\u003e要做什么？\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"/images/2021-10-01-07.png\" alt=\"Chaos Mesh 的基本工作流原理图\"\u003e\u003c/p\u003e\n\u003cp\u003e这里标题虽是控制平面，但查看上述 Chaos Mesh 工作流程图，其实我们需要做的只是实现一个将 YAML 下发到 Kubernetes API 的服务器，复杂的规则校验、规则下发到 Chaos Daemon 的行为是由 Chaos Controller Manager 完成的。想要结合自己的平台使用，只需要对接 CRD 资源创建的过程就足够了。\u003c/p\u003e\n\u003cp\u003e我们来看一下 PingCAP 官方给出的示例：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003eimport (\n    \"context\"\n\n    \"github.com/pingcap/chaos-mesh/api/v1alpha1\"\n    \"sigs.k8s.io/controller-runtime/pkg/client\"\n)\n\nfunc main() {\n    ...\n    delay := \u0026#x26;chaosv1alpha1.NetworkChaos{\n        Spec: chaosv1alpha1.NetworkChaosSpec{...},\n    }\n    k8sClient := client.New(conf, client.Options{ Scheme: scheme.Scheme })\n    k8sClient.Create(context.TODO(), delay)\n    k8sClient.Delete(context.TODO(), delay)\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eChaos Mesh 已经提供了所有的 CRD 资源定义对应的 API，我们使用 Kubernetes \u003ca href=\"https://github.com/kubernetes/community/tree/master/sig-api-machinery\"\u003eAPI Machinery SIG\u003c/a\u003e 开发的 \u003ca href=\"https://github.com/kubernetes-sigs/controller-runtime\"\u003econtroller-runtime\u003c/a\u003e 来简化与 Kubernetes API 的交互。\u003c/p\u003e\n\u003ch3\u003e实施混沌\u003c/h3\u003e\n\u003cp\u003e例如我们想通过程序调用的方式，创建一个 PodKill 资源，该资源被发送到 Kubernetes API Server 后，会经由  Chaos Controller Manager 的 \u003ca href=\"https://kubernetes.io/zh/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook/\"\u003eValidating 准入控制器\u003c/a\u003e，进行数据校验，若数据格式验证失败，会在创建时返回错误。具体参数可以查阅官方文档\u003ca href=\"https://chaos-mesh.org/zh/docs/simulate-pod-chaos-on-kubernetes/#%E4%BD%BF%E7%94%A8-yaml-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%88%9B%E5%BB%BA%E5%AE%9E%E9%AA%8C\"\u003e使用 YAML 配置文件创建实验\u003c/a\u003e。\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eNewClient\u003c/code\u003e 创建了一个 K8s API Client，可以参考\u003ca href=\"https://pkg.go.dev/sigs.k8s.io/controller-runtime@v0.10.2/pkg/client#example-New\"\u003e客户端创建示例\u003c/a\u003e。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003epackage main\n\nimport (\n\t\"context\"\n\t\"controlpanel\"\n\t\"log\"\n\n\t\"github.com/chaos-mesh/chaos-mesh/api/v1alpha1\"\n\t\"github.com/pkg/errors\"\n\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n)\n\nfunc applyPodKill(name, namespace string, labels map[string]string) error {\n\tcli, err := controlpanel.NewClient()\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"create client\")\n\t}\n\n\tcr := \u0026#x26;v1alpha1.PodChaos{\n\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\tGenerateName: name,\n\t\t\tNamespace:    namespace,\n\t\t},\n\t\tSpec: v1alpha1.PodChaosSpec{\n\t\t\tAction: v1alpha1.PodKillAction,\n\t\t\tContainerSelector: v1alpha1.ContainerSelector{\n\t\t\t\tPodSelector: v1alpha1.PodSelector{\n\t\t\t\t\tMode: v1alpha1.OnePodMode,\n\t\t\t\t\tSelector: v1alpha1.PodSelectorSpec{\n\t\t\t\t\t\tNamespaces:     []string{namespace},\n\t\t\t\t\t\tLabelSelectors: labels,\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\n\tif err := cli.Create(context.Background(), cr); err != nil {\n\t\treturn errors.Wrap(err, \"create podkill\")\n\t}\n\n\treturn nil\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e运行程序的日志输出为：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003eI1021 00:51:55.225502   23781 request.go:665] Waited for 1.033116256s due to client-side throttling, not priority and fairness, request: GET:https://***\n2021/10/21 00:51:56 apply podkill\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e通过 kubectl 查看 PodKill 资源的状态：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003e$ k describe podchaos.chaos-mesh.org -n dev podkillvjn77\nName:         podkillvjn77\nNamespace:    dev\nLabels:       \u0026#x3C;none\u003e\nAnnotations:  \u0026#x3C;none\u003e\nAPI Version:  chaos-mesh.org/v1alpha1\nKind:         PodChaos\nMetadata:\n  Creation Timestamp:  2021-10-20T16:51:56Z\n  Finalizers:\n    chaos-mesh/records\n  Generate Name:     podkill\n  Generation:        7\n  Resource Version:  938921488\n  Self Link:         /apis/chaos-mesh.org/v1alpha1/namespaces/dev/podchaos/podkillvjn77\n  UID:               afbb40b3-ade8-48ba-89db-04918d89fd0b\nSpec:\n  Action:        pod-kill\n  Grace Period:  0\n  Mode:          one\n  Selector:\n    Label Selectors:\n      app:  nginx\n    Namespaces:\n      dev\nStatus:\n  Conditions:\n    Reason:\n    Status:  False\n    Type:    Paused\n    Reason:\n    Status:  True\n    Type:    Selected\n    Reason:\n    Status:  True\n    Type:    AllInjected\n    Reason:\n    Status:  False\n    Type:    AllRecovered\n  Experiment:\n    Container Records:\n      Id:            dev/nginx\n      Phase:         Injected\n      Selector Key:  .\n    Desired Phase:   Run\nEvents:\n  Type    Reason           Age    From          Message\n  ----    ------           ----   ----          -------\n  Normal  FinalizerInited  6m35s  finalizer     Finalizer has been inited\n  Normal  Updated          6m35s  finalizer     Successfully update finalizer of resource\n  Normal  Updated          6m35s  records       Successfully update records of resource\n  Normal  Updated          6m35s  desiredphase  Successfully update desiredPhase of resource\n  Normal  Applied          6m35s  records       Successfully apply chaos for dev/nginx\n  Normal  Updated          6m35s  records       Successfully update records of resource\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e控制面还自然而然要有查询和获取 Chaos 资源的功能，方便平台用户查看到所有的混沌试验的实施状态，对其进行管理。当然，这里无非是调用 REST API 发送 Get/List 请求，但在实践上细节需要留意，敝司就发生过 Controller 每次请求全量的资源数据，造成 K8s API Server 的负载增高。\u003c/p\u003e\n\u003cp\u003e这里十分推荐阅读 \u003ca href=\"https://zoetrope.github.io/kubebuilder-training/controller-runtime/client.html\"\u003eクライアントの使い方\u003c/a\u003e，这篇 controller runtime 使用教程，提到了很细节的地方。例如 controller runtime 默认会从多个位置读取 kubeconfig，flag、环境变量、再是自动挂载在 Pod 中的 Service Account，\u003ca href=\"https://github.com/armosec/kubescape\"\u003earmosec/kubescape\u003c/a\u003e \u003ca href=\"https://github.com/armosec/kubescape/pull/21\"\u003e#21\u003c/a\u003e PR 也是利用了该特性。这篇教程还包括了如何分页、更新、覆盖对象等常用的操作，我目前还没有看到有哪篇中文、英文教程有这么详细。\u003c/p\u003e\n\u003cp\u003eGet/List 请求示例：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003epackage controlpanel\n\nimport (\n\t\"context\"\n\n\t\"github.com/chaos-mesh/chaos-mesh/api/v1alpha1\"\n\t\"github.com/pkg/errors\"\n\t\"sigs.k8s.io/controller-runtime/pkg/client\"\n)\n\nfunc GetPodChaos(name, namespace string) (*v1alpha1.PodChaos, error) {\n\tcli := mgr.GetClient()\n\n\titem := new(v1alpha1.PodChaos)\n\tif err := cli.Get(context.Background(), client.ObjectKey{Name: name, Namespace: namespace}, item); err != nil {\n\t\treturn nil, errors.Wrap(err, \"get cr\")\n\t}\n\n\treturn item, nil\n}\n\nfunc ListPodChaos(namespace string, labels map[string]string) ([]v1alpha1.PodChaos, error) {\n\tcli := mgr.GetClient()\n\n\tlist := new(v1alpha1.PodChaosList)\n\tif err := cli.List(context.Background(), list, client.InNamespace(namespace), client.MatchingLabels(labels)); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn list.Items, nil\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e示例中使用了 manager，该模式下会启用 cache 机制，避免重复获取大量数据。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/2021-10-01-08.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e获取 Pod\u003c/li\u003e\n\u003cli\u003e初次获取全量数据（List）\u003c/li\u003e\n\u003cli\u003eWatch 数据变化时更新缓存\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e混沌编排\u003c/h3\u003e\n\u003cp\u003e就如同 CRI 容器运行时提供了强大的底层隔离能力，能够支撑容器的稳定运行，而想要更大规模、更复杂的场景就需要容器编排一样，Chaos Mesh 提供了 Schedule 和 Workflow 功能。\u003ca href=\"https://chaos-mesh.org/zh/docs/define-scheduling-rules/\"\u003eSchedule\u003c/a\u003e 能够根据设定的 Cron 时间定时、间隔地触发故障，\u003ca href=\"https://chaos-mesh.org/zh/docs/create-chaos-mesh-workflow/\"\u003eWorkflow\u003c/a\u003e 能像 Argo Workflow 一样编排多个故障试验。\u003c/p\u003e\n\u003cp\u003e当然，Chaos Controller Manager 替我们做了大部分工作，控制面所需要的还是管理这些 YAML 资源，唯一需要考虑的是要给用户提供怎样的功能。\u003c/p\u003e\n\u003ch3\u003e平台功能\u003c/h3\u003e\n\u003cp\u003e参考 Chaos Mesh Dashboard，我们需要考虑平台该提供哪些功能给终端用户。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/2021-10-01-09.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e可能的平台功能点：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e混沌注入\n\u003cul\u003e\n\u003cli\u003ePod 崩溃\u003c/li\u003e\n\u003cli\u003e网络故障\u003c/li\u003e\n\u003cli\u003e负载测试\u003c/li\u003e\n\u003cli\u003eIO 故障\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e事件跟踪\u003c/li\u003e\n\u003cli\u003e关联告警\u003c/li\u003e\n\u003cli\u003e时序遥测\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e参阅资料\u003c/h2\u003e\n\u003cp\u003e本文既是为公司引入新技术进行试探，同时也是自我学习的记录，此前接触的学习材料及撰写本文时查阅的资料，较为优秀的部分列在这里，以便快速查阅。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://qiankunli.github.io/2020/08/10/controller_runtime.html\"\u003econtroller-runtime源码分析\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/zoetrope/kubebuilder-training\"\u003eつくって学ぶKubebuilder\u003c/a\u003e（日文教程）\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://book.kubebuilder.io/\"\u003eKubebuilder Book\u003c/a\u003e / \u003ca href=\"https://cloudnative.to/kubebuilder/\"\u003e中文版\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.huweihuang.com/kubernetes-notes/code-analysis/kube-controller-manager/sharedIndexInformer.html\"\u003ekube-controller-manager源码分析（三）之 Informer机制\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://segmentfault.com/a/1190000020359577\"\u003ekubebuilder2.0学习笔记——进阶使用\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.cnblogs.com/charlieroro/p/11112526.html\"\u003eclient-go和golang源码中的技巧\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003csection data-footnotes class=\"footnotes\"\u003e\u003ch2 class=\"sr-only\" id=\"footnote-label\"\u003eFootnotes\u003c/h2\u003e\n\u003col\u003e\n\u003cli id=\"user-content-fn-1\"\u003e\n\u003cp\u003e\u003ca href=\"https://pingcap.com/zh/blog/chaos-mesh\"\u003eChaos Mesh - 让应用跟混沌在 Kubernetes 上共舞\u003c/a\u003e \u003ca href=\"#user-content-fnref-1\" data-footnote-backref=\"\" aria-label=\"Back to reference 1\" class=\"data-footnote-backref\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"user-content-fn-2\"\u003e\n\u003cp\u003e\u003ca href=\"https://xie.infoq.cn/article/655c0893ed150ff65f2b7a16f\"\u003e自制文件系统 —— 02 开发者的福音，FUSE 文件系统\u003c/a\u003e \u003ca href=\"#user-content-fnref-2\" data-footnote-backref=\"\" aria-label=\"Back to reference 2\" class=\"data-footnote-backref\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"user-content-fn-3\"\u003e\n\u003cp\u003e\u003ca href=\"http://www.freeoa.net/product/apptool/stress-ng_3456.html\"\u003e系统压力测试工具-stress-ng\u003c/a\u003e \u003ca href=\"#user-content-fnref-3\" data-footnote-backref=\"\" aria-label=\"Back to reference 3\" class=\"data-footnote-backref\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"user-content-fn-4\"\u003e\n\u003cp\u003e\u003ca href=\"https://www.jianshu.com/p/a73f984f53b5\"\u003eRunC 源码通读指南之 NameSpace\u003c/a\u003e \u003ca href=\"#user-content-fnref-4\" data-footnote-backref=\"\" aria-label=\"Back to reference 4\" class=\"data-footnote-backref\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/section\u003e"])</script><script>self.__next_f.push([1,"4:[\"$\",\"article\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"$c\"}}]\n"])</script></body></html>