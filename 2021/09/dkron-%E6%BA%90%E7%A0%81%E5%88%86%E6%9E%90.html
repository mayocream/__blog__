<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/css/c3b55921f92a131e.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-072f062dc024cc52.js"/><script src="/_next/static/chunks/f4f1b8d9-0107da81548bf985.js" async=""></script><script src="/_next/static/chunks/435-d305dd8b5fb158de.js" async=""></script><script src="/_next/static/chunks/main-app-3958e659bb0a464b.js" async=""></script><title>Mayo Rocks!</title><meta name="description" content="Mayo&#x27;s Blog"/><link rel="icon" href="/icon.png?14d5a92fbe70e82a" type="image/png" sizes="460x460"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="antialiased"><article><blockquote>
<p>本文于 2021.05.21 发表于敝司内部，现做部分修订与批注公开发表。</p>
</blockquote>
<p><a href="https://github.com/distribworks/dkron">Dkron</a> 是基于 Google 白皮书理论编写的分布式任务调度系统，内部技术利用到了 <a href="https://www.serf.io/">Serf</a>（Gossip）进行病毒式消息扩散，<a href="https://github.com/hashicorp/raft">Raft</a> 进行分布式数据强一致性同步，设计上采用 Server / Agent 模式，只有一个中心调度主节点，其他节点作为工作节点，通过 <a href="https://github.com/hashicorp/go-plugin">go-plugin</a> 插件机制定义多种类型、可自由扩展的任务处理器，使用 <a href="https://grpc.io/">GRPC</a> 的双向流传输进行任务分发与状态上报，同时使用 <a href="https://pkg.go.dev/net/rpc">net/rpc</a> 进行跨进程调用，还使用高性能 KV 储存 <a href="https://github.com/tidwall/buntdb">buntdb</a> 的 in-memory 模式缓存与结构化储存任务元数据。</p>
<p>Dkron 可谓是站在了巨人的肩膀上，利用了诸多优秀开源组件，汲取了优秀设计思想实现的产品（官方还运营了商业版本）。</p>
<p>我在将其引入公司落地途中，对其进行了代码审计，本文便是落地过程中的产物之一（学习笔记）。由于 Dkron 涉及到的开源组件较多，光是 Goosip 与 Raft 协议就够复杂了，我抱着学习的心态进行工作，逐渐对这些开源组件有了认知，阅读代码也知道具体的用法是什么样子，有一定的收获。</p>
<p>本文基于 2021.05.19 <a href="https://github.com/distribworks/dkron">dkron</a> 最新 master 分支（对应 v3.1.6 版本）代码阅读， 详细源码笔记于 <a href="https://github.com/mayocream/dkron">mayocream/dkron</a> review 分支。</p>
<h2>1. 架构设计</h2>
<h3>1.1. 概述</h3>
<p>引用 Dkron 官网的表述：</p>
<blockquote>
<p>Dkron 是分布式的 Cron 服务，以 Golang 编写，并利用 Raft 协议和 Serf 提供可容错性，可靠性和可伸缩性，同时易于使用与安装。</p>
</blockquote>
<h3>1.2. 设计思路</h3>
<p>Dkron 参考 Google 的<a href="https://queue.acm.org/detail.cfm?id=2745840">分布式 Cron 系统白皮书</a>，实现了与 Google 内部任务系统“相同”的功能。</p>
<h4>1.2.1. 可靠性</h4>
<blockquote>
<p>Running these jobs is facilitated by keeping a file containing timestamps of the last launch for all registered Cron jobs.</p>
</blockquote>
<p>Dkron 的 Job 数据结构包含了任务最后一次执行的时间戳：</p>
<pre><code class="language-protobuf">message Job {
  ...
  // 最后一次执行成功/失败时间戳
  NullableTime last_success = 25;
  NullableTime last_error = 26;
  // 下一次执行时间戳
  google.protobuf.Timestamp next = 23;
  ...
}
</code></pre>
<blockquote>
<p>To make matters more complicated, failure to launch is acceptable for  some Cron jobs but not for others. A garbage collection Cron job  scheduled to run every five minutes may be able to skip one launch, but a payroll job scheduled to run once a month probably should not.</p>
</blockquote>
<p>一些任务设计上不是幂等的，简单地错误重试会导致严重的问题，任务所有者应有任务执行、重试等操作的控制权。</p>
<p>Dkron 能够让用户控制 Job 的执行、重试次数：</p>
<pre><code class="language-protobuf">message Job {
  ...
  // 启用/禁用
  bool disabled = 11;
  // 错误尝试次数
  uint32 retries = 13;
  // 是否允许同时调度
  string concurrency = 16;
  ...
}
</code></pre>
<h4>1.2.2. 可伸缩性</h4>
<blockquote>
<p>If you want to run a service, simply specify which data center it should run in and what it requires — the data center scheduling system (which  itself should be reliable) takes care of figuring out which machine or  machines to deploy it on, as well as handling machine deaths. Launching a job in a data center then effectively turns into sending one or more  RPCs to the data center scheduler.</p>
</blockquote>
<p>定时任务分布式地运行，设计上要能够避免集群中一台机器宕机影响。</p>
<p>Dkron 在任务调度和集群通信上定义了“数据中心”，所有的 Dkron 实例可以通过 Gossip 组建集群。<em>但 Dkron 目前只支持同一数据中心下进行任务调度，不支持跨数据中心调度任务。</em></p>
<p>Dkron 任务调度程序和执行节点通过 RPC 通信。</p>
<h4>1.2.3. 状态储存</h4>
<blockquote>
<p>We have two options: store data externally in a generally available  distributed storage, or store a small amount of state as part of the  Cron service itself. When designing the distributed Cron, we opted for  the second option.</p>
</blockquote>
<p>任务执行状态在 Dkron 被视为是与 Server 一体的，Dkron 通过 Raft FSM 在调度集群 Server 之间同步 Job 和 Job  执行历史的数据。<em>社区版本数据储存在内存 KV (Buntdb) 中，Pro 版本支持 etcd 储存。</em></p>
<p>持久化数据通过 Raft Snapshot 实现。</p>
<h4>1.2.4. 任务调度</h4>
<h5>集群</h5>
<p><img src="/images/2021-09-01-11.png" alt=""></p>
<p>Dkron 中服务器节点分为 Server / Agent，Server 集群负责 Job 的数据存储，Server 中选举出的 Leader 负责任务调度。<em>Server 同时也是 Agent，可以执行 Job。</em></p>
<p>Dkron 通过 Gossip 组建集群，通过 Raft 选举 Leader，Leader 担当 Scheduler，通过 GRPC 下发 Job 到 Gossip 集群的 Peers 节点执行。当 Leader 节点宕机，会在 Server 集群中选举出新的 Leader。</p>
<h5>任务执行</h5>
<p><img src="/images/2021-09-01-12.png" alt=""></p>
<p>Dkron Job 执行中 Agent 与 Server GRPC 双向流通信，Agent 持续发送任务执行情况，Server 记录任务开始和任务结束时的执行状态，通过 Raft Apply 同步到 Server 集群。</p>
<h2>2. 源码分析</h2>
<h3>2.1. 基本概述</h3>
<h4>2.1.1. 主要机制</h4>
<p><img src="/images/2021-09-01-13.png" alt=""></p>
<p>Dkron 主要基于三大机制实现分布式任务调度：</p>
<ol>
<li>通过 Gossip 库 Serf 实现节点发现、集群组建，监听 Gossip Member 变化事件，发现 Server/Agent 的 Peer 通信地址及活跃状态；</li>
<li>通过 Raft 实现 Server 集群 Leader 选举，实现 FSM 接口，通过 Raft Log 实现 Server 集群数据同步、数据快照、数据最终一致性；</li>
<li>利用 go-plugin 实现插件机制，基于 GRPC 通信 Agent 实时返回任务执行状态。</li>
</ol>
<h4>2.1.2. 目录结构</h4>
<pre><code class="language-bash">$ tree -A -L 1
.
# 自带的插件 /Processor 的 main 包
├── builtin
# CLI
├── cmd
# 主要功能实现
├── dkron
├── Dockerfile
# 封装 cron
├── extcron
├── go.mod
├── main.go
# 生成的 pb.go 以及通信逻辑
├── plugin
# 定义 Job 以及通信协议
├── proto
</code></pre>
<h3>2.2. 组建集群</h3>
<h4>2.2.1. 节点发现</h4>
<p>Dkron 使用 <a href="hashicorp/go-discover">hashicorp/go-discover</a> 进行节点 IP 的自动发现，方便通过云厂商接口、K8s 进行初始化服务发现。<em>用于发现 Peer 节点，进行 Bootstrap 或者 Join 集群。</em></p>
<pre><code class="language-go">// dkron/retry_join.go
func (r *retryJoiner) retryJoin(logger *logrus.Entry) error {
    ...
	// 使用 go-discovery 发通过不同的基础设施自动发现 IP
	// Copy the default providers, and then add the non-default
	providers := make(map[string]discover.Provider)
	for k, v := range discover.Providers {
		providers[k] = v
	}

	// 尝试 In-Cluster 方式从 "/var/run/secrets/kubernetes.io/serviceaccount"
	//	获取 kubeconfig
	//	ref: https://github.com/kubernetes/client-go/tree/master/examples/in-cluster-client-configuration
	//	获取 PodIP 列表
	providers["k8s"] = &#x26;discoverk8s.Provider{}

	disco, err := discover.New(
		discover.WithUserAgent(UserAgent()),
		discover.WithProviders(providers),
	)
	if err != nil {
		return err
	}

	attempt := 0
	for {
		var addrs []string
		var err error

		for _, addr := range r.addrs {
			switch {
				// 使用 go-discovery 发现 IP
			case strings.Contains(addr, "provider="):
				servers, err := disco.Addrs(addr, log.New(logger.Logger.Writer(), "", log.LstdFlags|log.Lshortfile))
				if err != nil {
                    ...
				} else {
					addrs = append(addrs, servers...)
				}

			default:
				ipAddr, err := ParseSingleIPTemplate(addr)
				if err != nil {
					logger.WithField("addr", addr).WithError(err).Error("agent: Error parsing retry-join ip template")
					continue
				}
				addrs = append(addrs, ipAddr)
			}
		}

		if len(addrs) > 0 {
			// Serf 加入集群
			n, err := r.join(addrs)
			if err == nil {
				logger.Infof("agent: Join %s completed. Synced with %d initial agents", r.cluster, n)
				return nil
			}
		}

		attempt++
	}
}
</code></pre>
<p>在 K8s 部署方式中程序通过创建 go-client，请求 K8s API Server 筛选获取一批 Dkron 的 Pod IP。</p>
<h4>2.2.2. Gossip 集群</h4>
<p>Dkron 使用 <a href="https://github.com/hashicorp/serf">hashicorp/serf</a> 库（Gossip 封装）的 Member 事件 Handler，监听集群成员变化事件，进行集群 Peers 的管理。</p>
<p>默认使用 <code>8946</code> 端口进行 Gossip 集群的 Peers 通信。</p>
<pre><code class="language-go">// dkron/agent.go
// setupSerf is used to create the agent we use
func (a *Agent) setupSerf() (*serf.Serf, error) {
    ...
	serfConfig := serf.DefaultConfig()
	serfConfig.Init()

	// metadata
	serfConfig.Tags = a.config.Tags
	serfConfig.Tags["role"] = "dkron"
	// 默认 "dc1"
	serfConfig.Tags["dc"] = a.config.Datacenter
	// 默认 "global"
	serfConfig.Tags["region"] = a.config.Region
	serfConfig.Tags["version"] = Version
	if a.config.Server {
		serfConfig.Tags["server"] = strconv.FormatBool(a.config.Server)
	}
	if a.config.Bootstrap {
		serfConfig.Tags["bootstrap"] = "1"
	}
	if a.config.BootstrapExpect != 0 {
		serfConfig.Tags["expect"] = fmt.Sprintf("%d", a.config.BootstrapExpect)
	}

	// gossip 默认连接环境/连接参数
	switch config.Profile {
	case "lan":
		serfConfig.MemberlistConfig = memberlist.DefaultLANConfig()
	case "wan":
		serfConfig.MemberlistConfig = memberlist.DefaultWANConfig()
	case "local":
		serfConfig.MemberlistConfig = memberlist.DefaultLocalConfig()
	default:
		return nil, fmt.Errorf("unknown profile: %s", config.Profile)
	}

	serfConfig.MemberlistConfig.BindAddr = bindIP
	serfConfig.MemberlistConfig.BindPort = bindPort
	serfConfig.MemberlistConfig.AdvertiseAddr = advertiseIP
	serfConfig.MemberlistConfig.AdvertisePort = advertisePort
	serfConfig.MemberlistConfig.SecretKey = encryptKey
	serfConfig.NodeName = config.NodeName
	serfConfig.Tags = config.Tags

	if err != nil {
		a.logger.Fatal(err)
	}

	// Create a channel to listen for events from Serf
	a.eventCh = make(chan serf.Event, 2048)
	serfConfig.EventCh = a.eventCh

	// Start Serf
	a.logger.Info("agent: Dkron agent starting")

	// Create serf first
	return serf.Create(serfConfig)
}
</code></pre>
<p><a href="https://github.com/hashicorp/memberlist">hashicorp/memberlist</a> 提供三个默认配置，局域网络的配置 DefaultLANConfig、外网 DefaultWANConfig、本地 DefaultLocalConfig，后两者都是在 DefaultLANConfig 基础上修改，区别是根据网速，调整了Gossip interval、TCP 超时时间等。</p>
<pre><code class="language-go">// dkron/retry_join.go
func (r *retryJoiner) retryJoin(logger *logrus.Entry) error {
	...
		if len(addrs) > 0 {
			// Serf 加入集群
			n, err := r.join(addrs)
			if err == nil {
				logger.Infof("agent: Join %s completed. Synced with %d initial agents", r.cluster, n)
				return nil
			}
		}

		attempt++
		time.Sleep(r.interval)
	...
}

</code></pre>
<p>程序启动后反复尝试加入 Gossip 集群。</p>
<h4>2.2.3. 端口复用</h4>
<p>Dkron 默认使用 <code>6868</code> 端口作为 Dkron 的 Peers RPC 通信端口。</p>
<p>默认会挂载 GRPC Server 用于服务调用。在节点为 Server 时同时开启 Raft 协议，复用端口。</p>
<pre><code class="language-go">// StartServer launch a new dkron server process
// 启动 Server
func (a *Agent) StartServer() {
    ...
	// 创建端口复用 cmux
	// Create a cmux object.
	tcpm := cmux.New(a.listener)
	var grpcl, raftl net.Listener

	// If TLS config present listen to TLS
	if a.TLSConfig != nil {
		...
        // 默认不启用 TLS
	} else {
		// 实现 raft.StreamLayer
		a.raftLayer = NewRaftLayer(a.logger)

		// cmux match grpc 通信
		grpcl = tcpm.MatchWithWriters(cmux.HTTP2MatchHeaderFieldSendSettings("content-type", "application/grpc"))

		// raft TCP 通信
		raftl = tcpm.Match(cmux.Any())
	}

	// 创建 GRPC Server
	if a.GRPCServer == nil {
		a.GRPCServer = NewGRPCServer(a, a.logger)
	}

	// 启动 GRPC Server
	if err := a.GRPCServer.Serve(grpcl); err != nil {
		a.logger.WithError(err).Fatal("agent: RPC server failed to start")
	}

	// 绑定 net Listener
	if err := a.raftLayer.Open(raftl); err != nil {
		a.logger.Fatal(err)
	}

	// 启动 Raft
	if err := a.setupRaft(); err != nil {
		a.logger.WithError(err).Fatal("agent: Raft layer failed to start")
	}

	// Start serving everything
	go func() {
		// 正式开始 listen
		if err := tcpm.Serve(); err != nil {
			a.logger.Fatal(err)
		}
	}()
   ...
}
</code></pre>
<p>使用 <a href="https://github.com/soheilhy/cmux">soheilhy/cmux</a> 包进行端口复用，读取字节流前 N 字节（Match 规则）匹配 HTTP/2 GRPC 请求。</p>
<h3>2.3. Raft 协议</h3>
<p>Dkron 使用的是 <a href="https://github.com/hashicorp/raft">hashicorp/raft</a> 实现的 Raft 协议。</p>
<h4>2.3.1. 集群初始化</h4>
<h5>创建 Raft</h5>
<p>Dkron 可以指定一台节点 Bootstrap，也可以设置 <code>bootstrap-expect</code> 等待集群 Peers 数量达到指定个数，再进行 Raft 集群 Bootstrap。</p>
<pre><code class="language-go">// 初始化 Raft
func (a *Agent) setupRaft() error {
	if a.config.BootstrapExpect > 0 {
		if a.config.BootstrapExpect == 1 {
			// 进行 bootstrap
			a.config.Bootstrap = true
		}
	}

	// 创建 raft transport
	// （Raft 节点间的通信通道），节点之间需要通过这个通道来进行日志同步、领导者选举等等
	// 方法内部启动协程 listen listener
	transport := raft.NewNetworkTransport(a.raftLayer, 3, raftTimeout, logger)
	a.raftTransport = transport

	config := raft.DefaultConfig()

	// Raft performance
	// 默认值为 1
	raftMultiplier := a.config.RaftMultiplier
	if raftMultiplier &#x3C; 1 || raftMultiplier > 10 {
		return fmt.Errorf("raft-multiplier cannot be %d. Must be between 1 and 10", raftMultiplier)
	}
	config.HeartbeatTimeout = config.HeartbeatTimeout * time.Duration(raftMultiplier)
	config.ElectionTimeout = config.ElectionTimeout * time.Duration(raftMultiplier)
	config.LeaderLeaseTimeout = config.LeaderLeaseTimeout * time.Duration(a.config.RaftMultiplier)

	config.LogOutput = logger
	config.LocalID = raft.ServerID(a.config.NodeName)

	// Build an all in-memory setup for dev mode, otherwise prepare a full
	// disk-based setup.
	var logStore raft.LogStore
	var stableStore raft.StableStore
	var snapshots raft.SnapshotStore
	if a.config.DevMode {
		...
	} else {
		var err error

		// （快照存储，用来存储节点的快照信息），也就是压缩后的日志数据
		snapshots, err = raft.NewFileSnapshotStore(filepath.Join(a.config.DataDir, "raft"), 3, logger)

		// Create the BoltDB backend
		s, err := raftboltdb.NewBoltStore(filepath.Join(a.config.DataDir, "raft", "raft.db"))

		a.raftStore = s
		// （稳定存储，用来存储 Raft 集群的节点信息等），比如，当前任期编号、最新投票时的任期编号等
		stableStore = s

		// 512 size 内存缓存
		cacheStore, err := raft.NewLogCache(raftLogCacheSize, s)
		if err != nil {
			s.Close()
			return err
		}
		// 用来存储 Raft 的日志
		logStore = cacheStore
        ...
	}

    ...
	//	参数: 实现了 Storage 的 DB, 默认 nil, logger
	// 创建实现 raft FSM 接口
	// raft 只是定义了一个接口，最终交给应用层实现。应用层收到 Log 后按 业务需求 还原为 应用数据保存起来
	//	Raft 启动时 便 Raft.runFSM 起一个goroutine 从 fsmMutateCh channel 消费log ==> FSM.Apply
	fsm := newFSM(a.Store, a.ProAppliers, a.logger)
	// 创建 raft 节点
	rft, err := raft.NewRaft(config, fsm, logStore, stableStore, snapshots, transport)
	if err != nil {
		return fmt.Errorf("new raft: %s", err)
	}
	// 选举 leader 信号 chan
	a.leaderCh = rft.LeaderCh()
	a.raft = rft

	return nil
}
</code></pre>
<p>默认使用 <code>/dkron/raft</code> 目录储存 Raft Log 和快照。使用 <a href="https://github.com/hashicorp/raft-boltdb">hashicorp/raft-boltdb</a> 作为 Raft Log 和 stable 储存，快照使用文件储存。</p>
<blockquote>
<p>线上使用情况：Server 运行 7 天每 30s 执行 5 个 Job ，<code>raft.db</code> 大小为 25M，内存占用在 200M 左右。</p>
</blockquote>
<p>数据量不大应该不用考虑大量磁盘和内存占用。</p>
<h5>创建集群</h5>
<p>如果是初始化集群，指定 <code>bootstrap-expect</code> 参数，则会监听 Gossip 集群成员变化，等待 Peers 数量达到预期时，进行 Raft 的 Bootstrap 操作。</p>
<p>如果已存在 Raft 集群，则新的 Server 加入 Raft 集群的成员中。</p>
<pre><code class="language-go">// dkron/agent.go
// 处理 Serf 事件
func (a *Agent) eventLoop() {
    // 只读的 Shutdown channel
	serfShutdownCh := a.serf.ShutdownCh()
	for {
		select {
			// Serf event 事件处理
		case e := &#x3C;-a.eventCh:
			// 有三种事件 MemberEvent/UserEvent/Query
			// 这里只处理 MemberEvent, 只使用 Serf 做集群 member 的管理
			// 实际运行时主要的 MemberEvent 事件有 update/failed/join
			// 没有使用到自定义 event 以及 query 命令

			// Log all member events
			if me, ok := e.(serf.MemberEvent); ok {
				for _, member := range me.Members {
					a.logger.WithFields(logrus.Fields{
						"node":   a.config.NodeName,
						"member": member.Name,
						"event":  e.EventType(),
					}).Debug("agent: Member event")
				}

				// serfEventHandler is used to handle events from the serf cluster
				switch e.EventType() {
				case serf.EventMemberJoin:
					a.nodeJoin(me)
					a.localMemberEvent(me)
				case serf.EventMemberLeave, serf.EventMemberFailed:
					a.nodeFailed(me)
					a.localMemberEvent(me)
				case serf.EventMemberReap:
					a.localMemberEvent(me)
				case serf.EventMemberUpdate, serf.EventUser, serf.EventQuery: // Ignore
				default:
					a.logger.WithField("event", e.String()).Warn("agent: Unhandled serf event")
				}
			}

		case &#x3C;-serfShutdownCh:
			a.logger.Warn("agent: Serf shutdown detected, quitting")
			return
		}
	}
}
</code></pre>
<p>Serf 的事件有三种类型：MemberEvent / UserEvent / Query，Dkron 只处理 MemberEvent，只使用 Serf 做集群 Peers 的管理，没有使用到自定义 event 以及 query 命令。实际运行时主要的 MemberEvent 事件有 <code>member-update</code> / <code>member-failed</code> / <code>member-join</code> / <code>member-reap</code>。</p>
<pre><code class="language-go">// dkron/serf.go
// maybeBootstrap is used to handle bootstrapping when a new server joins
func (a *Agent) maybeBootstrap() {
	...
	// 存在 raft commit 日志, 不需要 bootstrap
	if index != 0 {
		a.config.BootstrapExpect = 0
		return
	}

	// Scan for all the known servers
	members := a.serf.Members()
	var servers []ServerParts
	voters := 0
	for _, member := range members {
		valid, p := isServer(member)
		if !valid {
			// 跳过非 dkron server 的 member
			continue
		}
		if p.Region != a.config.Region {
			continue
		}
		...
		if valid {
			voters++
		}
		servers = append(servers, *p)
	}

	// Skip if we haven't met the minimum expect count
	if voters &#x3C; a.config.BootstrapExpect {
		return
	}

    ...
	for _, server := range servers {
		addr := server.Addr.String()
		addrs = append(addrs, addr)
		id := raft.ServerID(server.ID)
		suffrage := raft.Voter // 允许仲裁的角色
		peer := raft.Server{
			ID:       id,
			Address:  raft.ServerAddress(addr),
			Suffrage: suffrage,
		}
		configuration.Servers = append(configuration.Servers, peer)
	}

	// raft 集群初始化
	future := a.raft.BootstrapCluster(configuration)
	if err := future.Error(); err != nil {
		a.logger.WithError(err).Error("agent: failed to bootstrap cluster")
	}

	// Bootstrapping complete, or failed for some reason, don't enter this again
	a.config.BootstrapExpect = 0
}
</code></pre>
<p>Raft 集群初始化的条件：</p>
<ol>
<li>没有 Commit 日志；</li>
<li>Server 集群达到 expect 的数量；</li>
<li>属于同一 region</li>
</ol>
<h5>加入节点</h5>
<p>若 Raft 集群已存在，则 Leader 节点接收到 Serf Member 加入 Gossip 集群的事件时，进行 Raft 节点 Join 的逻辑：</p>
<pre><code class="language-go">// dkron/serf.go
func (a *Agent) localMemberEvent(me serf.MemberEvent) {
	// 只有 Leader 进行操作
	if !a.config.Server || !a.IsLeader() {
		return
	}

	for _, m := range me.Members {
		select {
            // 加入重新同步的 chan
		case a.reconcileCh &#x3C;- m:
		default:
		}
	}
}

// reconcileCh 触发 reconcile 执行
func (a *Agent) reconcile() error {
	members := a.serf.Members()
	for _, member := range members {
		if err := a.reconcileMember(member); err != nil {
			return err
		}
	}
	return nil
}
</code></pre>
<pre><code class="language-go">// dkron/leader.go
func (a *Agent) reconcileMember(member serf.Member) error {
	// 判断是否在一个 region, 是否是 Server
	valid, parts := isServer(member)
	if !valid || parts.Region != a.config.Region {
		return nil
	}

	var err error
	switch member.Status {
	case serf.StatusAlive:
		err = a.addRaftPeer(member, parts)
	case serf.StatusLeft:
		err = a.removeRaftPeer(member, parts)
	}
	if err != nil {
		return err
	}
	return nil
}
</code></pre>
<pre><code class="language-go">// dkron/leader.go
func (a *Agent) addRaftPeer(m serf.Member, parts *ServerParts) error {
   ...
	// 获取 raft peers
	for _, server := range configFuture.Configuration().Servers {

		// 要加入的 server 已存在于 raft peers 中, 先移除再添加
		if server.Address == raft.ServerAddress(addr) || server.ID == raft.ServerID(parts.ID) {
            ...
			future := a.raft.RemoveServer(server.ID, 0, 0)
			...
		}
	}

	// 添加一个 raft peer
	switch {
	case minRaftProtocol >= 3:
		addFuture := a.raft.AddVoter(raft.ServerID(parts.ID), raft.ServerAddress(addr), 0, 0)
		if err := addFuture.Error(); err != nil {
			a.logger.WithError(err).Error("dkron: failed to add raft peer")
			return err
		}
	}

	return nil
}
</code></pre>
<h4>2.3.2. FSM</h4>
<h5>内存 DB</h5>
<p>Dkron 使用内存 KV <a href="https://github.com/tidwall/buntdb">tidwall/buntdb</a> 储存 Job 及执行历史记录。</p>
<pre><code class="language-go">// dkron/storage.go
// Storage is the interface that should be used by any
// storage engine implemented for dkron. It contains the
// minumum set of operations that are needed to have a working
// dkron store.
type Storage interface {
	SetJob(job *Job, copyDependentJobs bool) error
	DeleteJob(name string) (*Job, error)
	SetExecution(execution *Execution) (string, error)
	SetExecutionDone(execution *Execution) (bool, error)
	GetJobs(options *JobOptions) ([]*Job, error)
	GetJob(name string, options *JobOptions) (*Job, error)
	GetExecutions(jobName string, opts *ExecutionOptions) ([]*Execution, error)
	GetExecutionGroup(execution *Execution, opts *ExecutionOptions) ([]*Execution, error)
	GetGroupedExecutions(jobName string, opts *ExecutionOptions) (map[int64][]*Execution, []int64, error)
	Shutdown() error
	Snapshot(w io.WriteCloser) error
	Restore(r io.ReadCloser) error
}
</code></pre>
<p><code>store</code> 实现了 Storage 接口，使用的是 buntdb：</p>
<pre><code class="language-go">// dkron/store.go
// NewStore creates a new Storage instance.
// 创建基于内存的 buntdb
func NewStore(logger *logrus.Entry) (*Store, error) {
	db, err := buntdb.Open(":memory:")
	db.CreateIndex("name", jobsPrefix+":*", buntdb.IndexJSON("name"))
	db.CreateIndex("started_at", executionsPrefix+":*", buntdb.IndexJSON("started_at"))
	db.CreateIndex("finished_at", executionsPrefix+":*", buntdb.IndexJSON("finished_at"))
	db.CreateIndex("attempt", executionsPrefix+":*", buntdb.IndexJSON("attempt"))
	db.CreateIndex("displayname", jobsPrefix+":*", buntdb.IndexJSON("displayname"))
	db.CreateIndex("schedule", jobsPrefix+":*", buntdb.IndexJSON("schedule"))
	db.CreateIndex("success_count", jobsPrefix+":*", buntdb.IndexJSON("success_count"))
	db.CreateIndex("error_count", jobsPrefix+":*", buntdb.IndexJSON("error_count"))
	db.CreateIndex("last_success", jobsPrefix+":*", buntdb.IndexJSON("last_success"))
	db.CreateIndex("last_error", jobsPrefix+":*", buntdb.IndexJSON("last_error"))
	db.CreateIndex("next", jobsPrefix+":*", buntdb.IndexJSON("next"))
	if err != nil {
		return nil, err
	}

	store := &#x26;Store{
		db:     db,
		lock:   &#x26;sync.Mutex{},
		logger: logger,
	}

	return store, nil
}
</code></pre>
<p>官方的 Pro 版本实现的 etcd 储存，可能是实现了 Storage 的另一个实例，或者是在 Raft FSM 进行了额外的处理执行外部数据库同步。</p>
<h5>实现 FSM 接口</h5>
<p>Raft FSM 接口：</p>
<pre><code class="language-go">// FSM provides an interface that can be implemented by
// clients to make use of the replicated log.
type FSM interface {
	// Apply log is invoked once a log entry is committed.
	// It returns a value which will be made available in the
	// ApplyFuture returned by Raft.Apply method if that
	// method was called on the same Raft node as the FSM.
	Apply(*Log) interface{}

	// Snapshot is used to support log compaction. This call should
	// return an FSMSnapshot which can be used to save a point-in-time
	// snapshot of the FSM. Apply and Snapshot are not called in multiple
	// threads, but Apply will be called concurrently with Persist. This means
	// the FSM should be implemented in a fashion that allows for concurrent
	// updates while a snapshot is happening.
	Snapshot() (FSMSnapshot, error)

	// Restore is used to restore an FSM from a snapshot. It is not called
	// concurrently with any other command. The FSM must discard all previous
	// state.
	Restore(io.ReadCloser) error
}
</code></pre>
<p><code>Apply</code> 函数会在从 Log 载入一条数据的时候被调用。<em>Leader 会调用 Apply 进行数据同步。</em></p>
<pre><code class="language-go">// dkron/fsm.go
type dkronFSM struct {
	store Storage

	// proAppliers holds the set of pro only LogAppliers
	proAppliers LogAppliers
	logger      *logrus.Entry
}

// 创建/同步一条 Log 到 state
func (d *dkronFSM) Apply(l *raft.Log) interface{} {
	buf := l.Data
	msgType := MessageType(buf[0])

	switch msgType {
	case SetJobType:
		return d.applySetJob(buf[1:])
	case DeleteJobType:
		return d.applyDeleteJob(buf[1:])
	case ExecutionDoneType:
		return d.applyExecutionDone(buf[1:])
	case SetExecutionType:
		return d.applySetExecution(buf[1:])
	}

	// Dkron Pro 版本额外的操作, 可能在此处执行外部数据库同步
	if applier, ok := d.proAppliers[msgType]; ok {
		return applier(buf[1:], l.Index)
	}

	return nil
}
</code></pre>
<p>实现的快照和恢复方法，使用 buntdb 的 load/save：</p>
<pre><code class="language-go">// dkron/fsm.go
// FSM 的快照和恢复方法，使用 buntdb 的 load/save 方法。

// Snapshot returns a snapshot of the key-value store. We wrap
// the things we need in dkronSnapshot and then send that over to Persist.
// Persist encodes the needed data from dkronSnapshot and transport it to
// Restore where the necessary data is replicated into the finite state machine.
// This allows the consensus algorithm to truncate the replicated log.
func (d *dkronFSM) Snapshot() (raft.FSMSnapshot, error) {
	return &#x26;dkronSnapshot{store: d.store}, nil
}

// Restore stores the key-value store to a previous state.
func (d *dkronFSM) Restore(r io.ReadCloser) error {
	defer r.Close()
	return d.store.Restore(r)
}

type dkronSnapshot struct {
	store Storage
}

func (d *dkronSnapshot) Persist(sink raft.SnapshotSink) error {
	if err := d.store.Snapshot(sink); err != nil {
		sink.Cancel()
		return err
	}

	// Close the sink.
	if err := sink.Close(); err != nil {
		return err
	}

	return nil
}

func (d *dkronSnapshot) Release() {}
</code></pre>
<h5>数据格式</h5>
<p>通过 Raft Log (Bytes) 储存的数据格式为：</p>
<pre><code class="language-go">// dkron/grpc.go
// Proto 转换成 Bytes
func Encode(t MessageType, msg interface{}) ([]byte, error) {
	var buf bytes.Buffer
	buf.WriteByte(uint8(t))
	m, err := pb.Marshal(msg.(pb.Message))
	if err != nil {
		return nil, err
	}
	_, err = buf.Write(m)
	return buf.Bytes(), err
}
</code></pre>
<p>首个字节标记 Message 的类型，后续为 Protobuf 序列化后的结果。</p>
<h3>2.4. 插件机制</h3>
<h4>2.4.1. 概述</h4>
<p>插件机制优先于任务调度本身进行分析，因为任务调度依赖于插件的通信机制。</p>
<p>Dkron 插件使用 <a href="https://github.com/hashicorp/go-plugin">hashicorp/go-plugin</a> 实现，简而概之，通过两种 RPC 方式（netrpc / GRPC）客户端与插件监听的端口进行通信，插件与主程序分离，作为插件机制。</p>
<p>引用 go-plugin 项目的说明：</p>
<blockquote>
<p>Shared libraries have one major advantage over our system which is much higher performance. In real world scenarios across our various tools, we've never required any more performance out of our plugin system and it has seen very high throughput, so this isn't a concern for us at the moment.</p>
</blockquote>
<p>Golang 出的基于链接库（.so）的插件机制还不成熟，并且 go-plugin 已经在大量软件中运用多年，对比起来前者没有优势，go-plugin 的考量在于性能对于插件机制来说不是首要的。</p>
<p>这里说一句，其他 Golang 的 HTTP 服务器例如 Traefik、Caddy 的插件机制都是需要修改源代码，自行增加包实现 Golang Interface，重新进行编译实现的，这种侵入性更大，但是性能更好。</p>
<p>Golang 官方的插件机制到目前还不支持 Windows，高性能的 WASM Golang 运行时库（用作 WASM 插件机制） <a href="https://github.com/wasmerio/wasmer-go">wasmerio/wasmer-go</a> 目前也不支持 Windows。</p>
<h4>2.4.2. 加载插件</h4>
<p>Dkron 默认会在一些目录查找插件的二进制文件：</p>
<pre><code class="language-go">// cmd/plugins.go
func (p *Plugins) DiscoverPlugins() error {
	p.Processors = make(map[string]dkplugin.Processor)
	p.Executors = make(map[string]dkplugin.Executor)

	// Look in /etc/dkron/plugins
	// 匹配目录下的文件列表
	processors, err := plugin.Discover("dkron-processor-*", filepath.Join("/etc", "dkron", "plugins"))
	if err != nil {
		return err
	}

	// Look in /etc/dkron/plugins
	executors, err := plugin.Discover("dkron-executor-*", filepath.Join("/etc", "dkron", "plugins"))
	if err != nil {
		return err
	}

	exePath, err := osext.Executable()
	if err != nil {
		logrus.WithError(err).Error("Error loading exe directory")
	} else {
		// 从当前 agent 执行目录查找，同目录的可执行文件
		// 默认执行目录为 /opt/local/dkron/
		// 若我们需要添加自定义插件, 将编译后的二进制文件放入同目录即可
		p, err := plugin.Discover("dkron-processor-*", filepath.Dir(exePath))
		if err != nil {
			return err
		}
		processors = append(processors, p...)
		e, err := plugin.Discover("dkron-executor-*", filepath.Dir(exePath))
		if err != nil {
			return err
		}
		executors = append(executors, e...)
	}

	// pluginFactory
	// 创建运行时
	for _, file := range processors {
		// 文件名按 "-" 分隔，取最后一个元素
		// 问题点: 插件名不支持带 "-"
		pluginName, ok := getPluginName(file)
		if !ok {
			continue
		}

		raw, err := p.pluginFactory(file, dkplugin.ProcessorPluginName)
		if err != nil {
			return err
		}
		p.Processors[pluginName] = raw.(dkplugin.Processor)
	}

	...

	return nil
}
</code></pre>
<h4>2.4.3. 插件实现</h4>
<p>Dkron 封装的 Plugin 结构, 规定可以有 processor 或 executor 两个插件。</p>
<pre><code class="language-go">// PluginMap should be used by clients for the map of plugins.
// Dkron 封装的 Plugin 结构, 规定可以有 processor 或 executor 两个插件
var PluginMap = map[string]plugin.Plugin{
	"processor": &#x26;ProcessorPlugin{},
	"executor":  &#x26;ExecutorPlugin{},
}
</code></pre>
<h5>构建插件</h5>
<pre><code class="language-go">// cmd/plugins.go
// 创建 Plugin Client
func (p *Plugins) pluginFactory(path string, pluginType string) (interface{}, error) {
	var config plugin.ClientConfig
	// 可执行文件
	config.Cmd = exec.Command(path)

	// handshake 配置是 client 与 server 约定的 TOKEN,
	//	不通过环境变量包含同样的 TOKEN 则无法启动 plugin 程序.
	//	具体查看 go-plugin 项目 https://github.com/mayocream/go-plugin/blob/044aadd925bf9f027cb301b2af9bc6b60775dd22/server.go#L248
	config.HandshakeConfig = dkplugin.Handshake

	// go-plugin 包会在 NewClient 的时候储存 Client 的指针,
	//	能够调用 cleanClients 统一 Kill 所有的 Client
	config.Managed = true

	// 定义一个二进制所能包含的不同插件
	config.Plugins = dkplugin.PluginMap
	config.SyncStdout = os.Stdout
	config.SyncStderr = os.Stderr

	switch pluginType {
	case dkplugin.ProcessorPluginName:
		// processor 使用 golang net/rpc 进行通信
		config.AllowedProtocols = []plugin.Protocol{plugin.ProtocolNetRPC}
	case dkplugin.ExecutorPluginName:
		// executor 使用 gprc 进行通信
		config.AllowedProtocols = []plugin.Protocol{plugin.ProtocolGRPC}
	}

	// 初始化客户端
	client := plugin.NewClient(&#x26;config)

	// go-plugin Client() 会用 exec.Start 启动 plugin server,
	//	创建 rpc client, 这个库将连接复用, 以及 rpc/grpc service 挂载
	//	等细节屏蔽了, 开发者只要创建业务逻辑 Interface 并实现 plugin.Plguin 的
	//	Serve/Client 方法就能够进行 rpc 通信
	rpcClient, err := client.Client()
	if err != nil {
		return nil, err
	}

	// 调用指定的 service
	raw, err := rpcClient.Dispense(pluginType)
	if err != nil {
		return nil, err
	}

	return raw, nil
}
</code></pre>
<h5>运行插件</h5>
<pre><code class="language-go">// go-plugin 插件 start 函数
func (c *Client) Start() (addr net.Addr, err error) {
   ...
    // 启动的环境变量
	env := []string{
		fmt.Sprintf("%s=%s", c.config.MagicCookieKey, c.config.MagicCookieValue),
		fmt.Sprintf("PLUGIN_MIN_PORT=%d", c.config.MinPort),
		fmt.Sprintf("PLUGIN_MAX_PORT=%d", c.config.MaxPort),
		fmt.Sprintf("PLUGIN_PROTOCOL_VERSIONS=%s", strings.Join(versionStrings, ",")),
	}

	cmd := c.config.Cmd
	cmd.Env = append(cmd.Env, os.Environ()...)
	cmd.Env = append(cmd.Env, env...)
	cmd.Stdin = os.Stdin

    // pipe 读取 std 输出
	cmdStdout, err := cmd.StdoutPipe()
	cmdStderr, err := cmd.StderrPipe()

	// 启动二进制程序
	err = cmd.Start()
	if err != nil {
		return
	}

    ...
    // 读取插件执行的 Output
	linesCh := make(chan string)
	c.clientWaitGroup.Add(1)
	go func() {
		defer c.clientWaitGroup.Done()
		defer close(linesCh)

		scanner := bufio.NewScanner(cmdStdout)
		for scanner.Scan() {
			linesCh &#x3C;- scanner.Text()
		}
	}()

	...
	return
}
</code></pre>
<h4>2.4.4. 插件通信</h4>
<h5>插件通信数据结构</h5>
<pre><code class="language-protobuf">message ExecuteRequest {
  string job_name = 1;
  map&#x3C;string, string> config = 2;
  uint32 status_server = 3;
}

message ExecuteResponse {
    bytes output = 1;
    string error = 2;
}

service Executor {
    rpc Execute (ExecuteRequest) returns (ExecuteResponse);
}

message StatusUpdateRequest {
  bytes output = 2;
  bool error = 3;
}

message StatusUpdateResponse {
  int64 r = 1;
}

service StatusHelper {
  rpc Update(StatusUpdateRequest) returns (StatusUpdateResponse);
}
</code></pre>
<h5>GRPC 双向流通信</h5>
<pre><code class="language-go">// Here is the gRPC client that GRPCClient talks to.
type ExecutorClient struct {
	// This is the real implementation
	client types.ExecutorClient
	broker *plugin.GRPCBroker
}

// ref: https://github.com/distribworks/dkron/pull/719
// 实现执行时实时传输 output, 双向流
func (m *ExecutorClient) Execute(args *types.ExecuteRequest, cb StatusHelper) (*types.ExecuteResponse, error) {
	// This is where the magic conversion to Proto happens
	statusHelperServer := &#x26;GRPCStatusHelperServer{Impl: cb}

	var s *grpc.Server
	serverFunc := func(opts []grpc.ServerOption) *grpc.Server {
		s = grpc.NewServer(opts...)
		types.RegisterStatusHelperServer(s, statusHelperServer)

		return s
	}

	brokerID := m.broker.NextId()
	go m.broker.AcceptAndServe(brokerID, serverFunc)

	args.StatusServer = brokerID
	r, err := m.client.Execute(context.Background(), args)

	s.Stop()
	return r, err
}
</code></pre>
<p>目前线上出现过 <code>rpc: transport is closing</code> 的错误，是 GRPC 通信的错误，推测在插件通信的该部分出现错误。</p>
<h3>2.5. 任务调度</h3>
<h4>2.5.1. 概述</h4>
<p>Dkron Server 中为 Raft Leader 的服务器成为调度服务器，负责定时任务的分发。</p>
<ol>
<li>通过 GRPC 调用下发任务到 agent 节点，Agent 执行任务并通过 GRPC 流实时返回输出；</li>
<li>执行完 Job 后 Leader 通过 Raft Apply 储存 Job 执行记录到各 Server 节点。</li>
</ol>
<h4>2.5.2. cron 封装</h4>
<p>Dkron 基于 <a href="https://github.com/robfig/cron">robfig/cron</a> 增加了 <code>@at</code> 时间定义，允许指定只允许一次的定时任务。</p>
<pre><code class="language-go">// extcron/extparser.go
// NewParser creates an ExtParser instance
// 启用 second 字段
func NewParser() cron.ScheduleParser {
	return ExtParser{cron.NewParser(cron.Second | cron.Minute | cron.Hour | cron.Dom | cron.Month | cron.Dow | cron.Descriptor)}
}

// Parse parses a cron schedule specification. It accepts the cron spec with
// mandatory seconds parameter, descriptors and the custom descriptors
// "@at &#x3C;date>" and "@manually".
// 添加了自定义解析的部分
func (p ExtParser) Parse(spec string) (cron.Schedule, error) {
	if spec == "@manually" {
		return At(time.Time{}), nil
	}

	const at = "@at "
	if strings.HasPrefix(spec, at) {
		date, err := time.Parse(time.RFC3339, spec[len(at):])
		if err != nil {
			return nil, fmt.Errorf("failed to parse date %s: %s", spec, err)
		}
		return At(date), nil
	}

	// It's not a dkron specific spec: Let the regular cron schedule parser have it
	return p.parser.Parse(spec)
}
</code></pre>
<h4>2.5.3. 数据一致性</h4>
<pre><code class="language-go">// dkron/leader.go
// 处理变成 leader 事件
func (a *Agent) leaderLoop(stopCh chan struct{}) {
RECONCILE:
   ...
	// Apply a raft barrier to ensure our FSM is caught up
	start := time.Now()
	barrier := a.raft.Barrier(barrierWriteTimeout)
	if err := barrier.Error(); err != nil {
		a.logger.WithError(err).Error("dkron: failed to wait for barrier")
		goto WAIT
	}
   ...
}
</code></pre>
<p>变成 Leader 后调用 <code>raft.Barrier</code> 确保 FSM 同步到最新状态。</p>
<h4>2.5.4. 任务下发</h4>
<h5>启动任务调度</h5>
<pre><code class="language-go">// dkron/scheduler.go
// 启动调度器
func (s *Scheduler) Start(jobs []*Job, agent *Agent) error {
	s.Cron = cron.New(cron.WithParser(extcron.NewParser()))

	for _, job := range jobs {
		job.Agent = agent
		// 添加所有 Job
		s.AddJob(job)
	}
	// 开始定时执行
	s.Cron.Start()
	s.Started = true
	schedulerStarted.Set(1)

	return nil
}
</code></pre>
<p>添加单个 Job 到 cron 定时触发：</p>
<pre><code class="language-go">// dkron/scheduler.go
// AddJob Adds a job to the cron scheduler
// 调度器添加 Job
func (s *Scheduler) AddJob(job *Job) error {
	// Check if the job is already set and remove it if exists
	if _, ok := s.EntryJobMap.Load(job.Name); ok {
		s.RemoveJob(job)
	}

	if job.Disabled || job.ParentJob != "" {
		return nil
	}
    ...

	// 为 cron 添加一个 job
	// Job 的 Run 是 cron 触发的执行方法
	id, err := s.Cron.AddJob(schedule, job)
	if err != nil {
		return err
	}
	// 储存 cron 的 id
	s.EntryJobMap.Store(job.Name, id)
    ...
	return nil
}
</code></pre>
<h5>触发任务调度</h5>
<pre><code class="language-go">// dkron/job.go
// job 的 run 方法实现 cron.Job 接口
func (j *Job) Run() {
	// Check if it's runnable
	if j.isRunnable(j.logger) {
		...
		cronInspect.Set(j.Name, j)

		// Simple execution wrapper
		ex := NewExecution(j.Name)

		// 触发调度运行 Job
		if _, err := j.Agent.Run(j.Name, ex); err != nil {
			j.logger.WithError(err).Error("job: Error running job")
		}
	}
}
</code></pre>
<p><code>isRunnable</code> 检查任务是否被禁止，同时通过 GRPC 查询所有 Agent 当前正在执行的任务，是否有相同的 JobName，如果“不允许并发调度”则停止本次调度。</p>
<h5>任务分发到 Agent</h5>
<p>注意：Dkron 调度会将任务调度到所有符合 tags 的、同一 region 的节点（Serf Members）上。</p>
<pre><code class="language-go">// dkron/run.go
// 调度运行 Job -> 分发到 agent 执行任务
func (a *Agent) Run(jobName string, ex *Execution) (*Job, error) {
	job, err := a.Store.GetJob(jobName, nil)
    ...
	// In case the job is not a child job, compute the next execution time
	if job.ParentJob == "" {
		// 获取 cron.Entry
		if e, ok := a.sched.GetEntry(jobName); ok {
			// 获取下一次执行时间
			job.Next = e.Next
			// 同步 job 数据
			if err := a.applySetJob(job.ToProto()); err != nil {
				return nil, fmt.Errorf("agent: Run error storing job %s before running: %w", jobName, err)
			}
		} else {
			return nil, fmt.Errorf("agent: Run error retrieving job: %s from scheduler", jobName)
		}
	}

	// In the first execution attempt we build and filter the target nodes
	// but we use the existing node target in case of retry.
	var filterMap map[string]string
	if ex.Attempt &#x3C;= 1 {
		filterMap, _, err = a.processFilteredNodes(job)
		if err != nil {
			return nil, fmt.Errorf("run error processing filtered nodes: %w", err)
		}
	} else {
		// In case of retrying, find the rpc address of the node or return with an error
		// 重试使用同样的 Node 执行
		var addr string
		for _, m := range a.serf.Members() {
			if ex.NodeName == m.Name {
				if m.Status == serf.StatusAlive {
					addr = m.Tags["rpc_addr"]
				} else {
					return nil, fmt.Errorf("retry node is gone: %s for job %s", ex.NodeName, ex.JobName)
				}
			}
		}
		filterMap = map[string]string{ex.NodeName: addr}
	}

   ...
	var wg sync.WaitGroup
	for _, v := range filterMap {
		// Call here client GRPC AgentRun
		wg.Add(1)
		go func(node string, wg *sync.WaitGroup) {
			defer wg.Done()

            // 这里真正调用 GRPC 到 agent 执行
			err := a.GRPCClient.AgentRun(node, job.ToProto(), ex.ToProto())
			if err != nil {
				...
			}
		}(v, &#x26;wg)
	}

	// 等待所有节点执行完
	wg.Wait()
	return job, nil
}

</code></pre>
<p>Server 对执行状态的处理：</p>
<ol>
<li>GRPC 通信开始，Agent 接收到任务；</li>
<li>GRPC 结束，Agent 执行完成（成功或失败）任务；</li>
</ol>
<pre><code class="language-go">// dkron/grpc_client.go
// Dkron server 调用此方法通过 GRPC 下发 Job 到 server/agent 执行
func (grpcc *GRPCClient) AgentRun(addr string, job *proto.Job, execution *proto.Execution) error {
	var conn *grpc.ClientConn

	// (MAYO): remove string type wrap
	conn, err := grpcc.Connect(addr)
	if err != nil {
		return err
	}
	defer conn.Close()

	// Streaming call
	a := proto.NewAgentClient(conn)
	stream, err := a.AgentRun(context.Background(), &#x26;proto.AgentRunRequest{
		Job:       job,
		Execution: execution,
	})
	if err != nil {
		return err
	}

	var first bool
	for {
		// 读取 GRPC 流
		ars, err := stream.Recv()

		// Stream ends
		if err == io.EOF {
			// 任务执行结束, 发送 done 命令给 leader 持久化储存
			addr := grpcc.agent.raft.Leader()
			if err := grpcc.ExecutionDone(string(addr), NewExecutionFromProto(execution)); err != nil {
				return err
			}
			return nil
		}

		// Error received from the stream
		if err != nil {
			// At this point the execution status will be unknown, set the FinshedAt time and an explanatory message
			execution.FinishedAt = ptypes.TimestampNow()
			execution.Output = []byte(err.Error())

			grpcc.logger.WithError(err).Error(ErrBrokenStream)

			addr := grpcc.agent.raft.Leader()
			if err := grpcc.ExecutionDone(string(addr), NewExecutionFromProto(execution)); err != nil {
				return err
			}
			return err
		}

		// Registers an active stream
		grpcc.agent.activeExecutions.Store(ars.Execution.Key(), ars.Execution)
		execution = ars.Execution
		defer grpcc.agent.activeExecutions.Delete(execution.Key())

		// Store the received execution in the raft log and store
		if !first {
			// 储存执行状态
			if err := grpcc.SetExecution(ars.Execution); err != nil {
				return err
			}
			first = true
		}
	}
}

</code></pre>
<h5>Agent 任务执行</h5>
<p>任务执行过程：</p>
<ol>
<li>实时发送执行情况到 Server；</li>
<li>Server 宕机，切换一个 Server 发送最终状态；</li>
</ol>
<pre><code class="language-go">// dkron/grpc_agent.go
// Dkron agent 执行任务, GRPC 客户端推流
func (as *AgentServer) AgentRun(req *types.AgentRunRequest, stream types.Agent_AgentRunServer) error {
	job := req.Job
	execution := req.Execution
    // buffer 创建, 储存执行输出
	output, _ := circbuf.NewBuffer(maxBufSize)

	var success bool

	jex := job.Executor
	exc := job.ExecutorConfig

	execution.StartedAt = ptypes.TimestampNow()
	execution.NodeName = as.agent.config.NodeName

	// 发送执行前状态
	if err := stream.Send(&#x26;types.AgentRunStream{
		Execution: execution,
	}); err != nil {
		return err
	}

	...

	// Check if executor exists
	// 找到对应的执行插件
	if executor, ok := as.agent.ExecutorPlugins[jex]; ok {
		as.logger.WithField("plugin", jex).Debug("grpc_agent: calling executor plugin")
		runningExecutions.Store(execution.GetGroup(), execution)
		// go-plugin grpc 调用执行
		out, err := executor.Execute(&#x26;types.ExecuteRequest{
			JobName: job.Name,
			Config:  exc,
			// callback, 将执行输出结果赋值到 output, 通过 stream 发送给服务端
			// ref: https://github.com/distribworks/dkron/pull/719
		}, &#x26;statusAgentHelper{
			stream:    stream,
			execution: execution,
		})
        ...

		if out != nil {
			output.Write(out.Output)
		}
	} else {
	   ...
		output.Write([]byte("grpc_agent: Specified executor is not present"))
	}

	// 执行完成
	execution.FinishedAt = ptypes.TimestampNow()
	execution.Success = success
	execution.Output = output.Bytes()

	runningExecutions.Delete(execution.GetGroup())

	// 发送最终状态
	if err := stream.Send(&#x26;types.AgentRunStream{
		Execution: execution,
	}); err != nil {
		// 有可能 server 没能接收到最后执行状态
        ...
		// TCP 连接筛选一个 server
		rpcServer, err := as.agent.checkAndSelectServer()
		if err != nil {
			return err
		}
		// 调用执行完成
		return as.agent.GRPCClient.ExecutionDone(rpcServer, NewExecutionFromProto(execution))
	}

	return nil
}

</code></pre>
<h5>任务执行后处理</h5>
<p>Job 执行完成后处理：</p>
<ol>
<li>必须为 Leader 进行数据持久化；</li>
<li>通过 Raft Apply 同步 Server 间执行记录；</li>
<li>错误重试，再次进行任务调度；</li>
<li>执行 Job 定义中依赖的 Job。</li>
</ol>
<pre><code class="language-go">// dkron/grpc.go
// 执行完成, 进行后续处理
func (grpcs *GRPCServer) ExecutionDone(ctx context.Context, execDoneReq *proto.ExecutionDoneRequest) (*proto.ExecutionDoneResponse, error) {
    ...
	if !grpcs.agent.IsLeader() {
		addr := grpcs.agent.raft.Leader()
		// 如果我不是 leader , 将请求发给 leader 执行
		grpcs.agent.GRPCClient.ExecutionDone(string(addr), NewExecutionFromProto(execDoneReq.Execution))
		return nil, ErrNotLeader
	}

	// This is the leader at this point, so process the execution, encode the value and apply the log to the cluster.
	// Get the defined output types for the job, and call them
	job, err := grpcs.agent.Store.GetJob(execDoneReq.Execution.JobName, nil)
	if err != nil {
		return nil, err
	}

	// 执行 processor, 不需要双向通信
	// 由此推测 exector 使用 grpc 执行是需要 GRPC 双向流
	pbex := *execDoneReq.Execution
	for k, v := range job.Processors {
		if processor, ok := grpcs.agent.ProcessorPlugins[k]; ok {
			v["reporting_node"] = grpcs.agent.config.NodeName
			pbex = processor.Process(&#x26;plugin.ProcessorArgs{Execution: pbex, Config: v})
		} else {
			...
		}
	}

	// 同步集群状态
	execDoneReq.Execution = &#x26;pbex
	cmd, err := Encode(ExecutionDoneType, execDoneReq)
	if err != nil {
		return nil, err
	}
	af := grpcs.agent.raft.Apply(cmd, raftTimeout)
	if err := af.Error(); err != nil {
		return nil, err
	}

	// Retrieve the fresh, updated job from the store to work on stored values
	job, err = grpcs.agent.Store.GetJob(job.Name, nil)
	if err != nil {
		grpcs.logger.WithError(err).WithField("job", execDoneReq.Execution.JobName).Error("grpc: Error retrieving job from store")
		return nil, err
	}

	// 任务执行重试
	execution := NewExecutionFromProto(&#x26;pbex)
	if !execution.Success &#x26;&#x26; uint(execution.Attempt) &#x3C; job.Retries+1 {
		execution.Attempt++

		// Keep all execution properties intact except the last output
		execution.Output = ""
        ...

		if _, err := grpcs.agent.Run(job.Name, execution); err != nil {
			return nil, err
		}
		return &#x26;proto.ExecutionDoneResponse{
			From:    grpcs.agent.config.NodeName,
			Payload: []byte("retry"),
		}, nil
	}

	exg, err := grpcs.agent.Store.GetExecutionGroup(execution,
		&#x26;ExecutionOptions{
			Timezone: job.GetTimeLocation(),
		},
	)
	if err != nil {
		...
		return nil, err
	}

    ...

	// 执行依赖的任务
	if len(job.DependentJobs) > 0 &#x26;&#x26; job.Status == StatusSuccess {
		for _, djn := range job.DependentJobs {
			dj, err := grpcs.agent.Store.GetJob(djn, nil)
			dj.Agent = grpcs.agent
			if err != nil {
				return nil, err
			}
		}
	}

	return &#x26;proto.ExecutionDoneResponse{
		From:    grpcs.agent.config.NodeName,
		Payload: []byte("saved"),
	}, nil
}
</code></pre>
<h3>2.6. 学习笔记</h3>
<h4>2.6.1. Serf CLI</h4>
<p><img src="/images/2021-09-01-10.png" alt=""></p>
<p>Serf 官方文档的示例主要是分布式运行脚本，通过启动在多个服务器上启动 serf agent（CLI 命令）。</p>
<p>通过配置 Event handler 参数启动：</p>
<pre><code class="language-bash">$ serf agent -event-handler=query:ssh=/bin/bash
</code></pre>
<p>发送 Query 命令就可以执行程序了：</p>
<pre><code class="language-bash">$ serf query ssh uptime
Query 'ssh' dispatched
Ack from 'node1.pocketstudio.net'
Ack from 'node2.pocketstudio.net'
Response from 'node2.pocketstudio.net':  05:25:34 up 21:31,  1 user,  load average: 0.00, 0.00, 0.00
Total Acks: 2
Total Responses: 1
</code></pre>
<p>另外 Consul 的 <code>go.mod</code> 里也引用了 Serf 包。</p></article><script src="/_next/static/chunks/webpack-072f062dc024cc52.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[5803,[],\"\"]\n3:I[695,[],\"\"]\n5:I[2576,[],\"OutletBoundary\"]\n7:I[2576,[],\"MetadataBoundary\"]\n9:I[2576,[],\"ViewportBoundary\"]\nb:I[7614,[],\"\"]\n:HL[\"/_next/static/css/c3b55921f92a131e.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"m6A-Tc8bnMTyQXrjIe8w0\",\"p\":\"\",\"c\":[\"\",\"2021\",\"09\",\"dkron-%25E6%25BA%2590%25E7%25A0%2581%25E5%2588%2586%25E6%259E%2590\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"(posts)\",{\"children\":[[\"year\",\"2021\",\"d\"],{\"children\":[[\"month\",\"09\",\"d\"],{\"children\":[[\"slug\",\"dkron-%25E6%25BA%2590%25E7%25A0%2581%25E5%2588%2586%25E6%259E%2590\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/c3b55921f92a131e.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"children\":[\"$\",\"body\",null,{\"className\":\"antialiased\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[],[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[\"(posts)\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(posts)\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[],[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":\"$0:f:0:1:1:props:children:1:props:children:props:children:props:notFound:1:1:props:style\",\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":\"$0:f:0:1:1:props:children:1:props:children:props:children:props:notFound:1:1:props:children:props:children:1:props:style\",\"children\":404}],[\"$\",\"div\",null,{\"style\":\"$0:f:0:1:1:props:children:1:props:children:props:children:props:notFound:1:1:props:children:props:children:2:props:style\",\"children\":[\"$\",\"h2\",null,{\"style\":\"$0:f:0:1:1:props:children:1:props:children:props:children:props:notFound:1:1:props:children:props:children:2:props:children:props:style\",\"children\":\"This page could not be found.\"}]}]]}]}]]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"year\",\"2021\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(posts)\",\"children\",\"$0:f:0:1:2:children:2:children:0\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"month\",\"09\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(posts)\",\"children\",\"$0:f:0:1:2:children:2:children:0\",\"children\",\"$0:f:0:1:2:children:2:children:2:children:0\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"slug\",\"dkron-%25E6%25BA%2590%25E7%25A0%2581%25E5%2588%2586%25E6%259E%2590\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(posts)\",\"children\",\"$0:f:0:1:2:children:2:children:0\",\"children\",\"$0:f:0:1:2:children:2:children:2:children:0\",\"children\",\"$0:f:0:1:2:children:2:children:2:children:2:children:0\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L4\",null,[\"$\",\"$L5\",null,{\"children\":\"$L6\"}]]}],{},null,false]},null,false]},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"GcsGWqrgpkbfK1torZFr_\",{\"children\":[[\"$\",\"$L7\",null,{\"children\":\"$L8\"}],[\"$\",\"$L9\",null,{\"children\":\"$La\"}],null]}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$b\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"a:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n8:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"1\",{\"children\":\"Mayo Rocks!\"}],[\"$\",\"meta\",\"2\",{\"name\":\"description\",\"content\":\"Mayo's Blog\"}],[\"$\",\"link\",\"3\",{\"rel\":\"icon\",\"href\":\"/icon.png?14d5a92fbe70e82a\",\"type\":\"image/png\",\"sizes\":\"460x460\"}]]\n"])</script><script>self.__next_f.push([1,"6:null\n"])</script><script>self.__next_f.push([1,"c:Tc90d,"])</script><script>self.__next_f.push([1,"\u003cblockquote\u003e\n\u003cp\u003e本文于 2021.05.21 发表于敝司内部，现做部分修订与批注公开发表。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/distribworks/dkron\"\u003eDkron\u003c/a\u003e 是基于 Google 白皮书理论编写的分布式任务调度系统，内部技术利用到了 \u003ca href=\"https://www.serf.io/\"\u003eSerf\u003c/a\u003e（Gossip）进行病毒式消息扩散，\u003ca href=\"https://github.com/hashicorp/raft\"\u003eRaft\u003c/a\u003e 进行分布式数据强一致性同步，设计上采用 Server / Agent 模式，只有一个中心调度主节点，其他节点作为工作节点，通过 \u003ca href=\"https://github.com/hashicorp/go-plugin\"\u003ego-plugin\u003c/a\u003e 插件机制定义多种类型、可自由扩展的任务处理器，使用 \u003ca href=\"https://grpc.io/\"\u003eGRPC\u003c/a\u003e 的双向流传输进行任务分发与状态上报，同时使用 \u003ca href=\"https://pkg.go.dev/net/rpc\"\u003enet/rpc\u003c/a\u003e 进行跨进程调用，还使用高性能 KV 储存 \u003ca href=\"https://github.com/tidwall/buntdb\"\u003ebuntdb\u003c/a\u003e 的 in-memory 模式缓存与结构化储存任务元数据。\u003c/p\u003e\n\u003cp\u003eDkron 可谓是站在了巨人的肩膀上，利用了诸多优秀开源组件，汲取了优秀设计思想实现的产品（官方还运营了商业版本）。\u003c/p\u003e\n\u003cp\u003e我在将其引入公司落地途中，对其进行了代码审计，本文便是落地过程中的产物之一（学习笔记）。由于 Dkron 涉及到的开源组件较多，光是 Goosip 与 Raft 协议就够复杂了，我抱着学习的心态进行工作，逐渐对这些开源组件有了认知，阅读代码也知道具体的用法是什么样子，有一定的收获。\u003c/p\u003e\n\u003cp\u003e本文基于 2021.05.19 \u003ca href=\"https://github.com/distribworks/dkron\"\u003edkron\u003c/a\u003e 最新 master 分支（对应 v3.1.6 版本）代码阅读， 详细源码笔记于 \u003ca href=\"https://github.com/mayocream/dkron\"\u003emayocream/dkron\u003c/a\u003e review 分支。\u003c/p\u003e\n\u003ch2\u003e1. 架构设计\u003c/h2\u003e\n\u003ch3\u003e1.1. 概述\u003c/h3\u003e\n\u003cp\u003e引用 Dkron 官网的表述：\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eDkron 是分布式的 Cron 服务，以 Golang 编写，并利用 Raft 协议和 Serf 提供可容错性，可靠性和可伸缩性，同时易于使用与安装。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3\u003e1.2. 设计思路\u003c/h3\u003e\n\u003cp\u003eDkron 参考 Google 的\u003ca href=\"https://queue.acm.org/detail.cfm?id=2745840\"\u003e分布式 Cron 系统白皮书\u003c/a\u003e，实现了与 Google 内部任务系统“相同”的功能。\u003c/p\u003e\n\u003ch4\u003e1.2.1. 可靠性\u003c/h4\u003e\n\u003cblockquote\u003e\n\u003cp\u003eRunning these jobs is facilitated by keeping a file containing timestamps of the last launch for all registered Cron jobs.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eDkron 的 Job 数据结构包含了任务最后一次执行的时间戳：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-protobuf\"\u003emessage Job {\n  ...\n  // 最后一次执行成功/失败时间戳\n  NullableTime last_success = 25;\n  NullableTime last_error = 26;\n  // 下一次执行时间戳\n  google.protobuf.Timestamp next = 23;\n  ...\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cblockquote\u003e\n\u003cp\u003eTo make matters more complicated, failure to launch is acceptable for  some Cron jobs but not for others. A garbage collection Cron job  scheduled to run every five minutes may be able to skip one launch, but a payroll job scheduled to run once a month probably should not.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e一些任务设计上不是幂等的，简单地错误重试会导致严重的问题，任务所有者应有任务执行、重试等操作的控制权。\u003c/p\u003e\n\u003cp\u003eDkron 能够让用户控制 Job 的执行、重试次数：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-protobuf\"\u003emessage Job {\n  ...\n  // 启用/禁用\n  bool disabled = 11;\n  // 错误尝试次数\n  uint32 retries = 13;\n  // 是否允许同时调度\n  string concurrency = 16;\n  ...\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003e1.2.2. 可伸缩性\u003c/h4\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIf you want to run a service, simply specify which data center it should run in and what it requires — the data center scheduling system (which  itself should be reliable) takes care of figuring out which machine or  machines to deploy it on, as well as handling machine deaths. Launching a job in a data center then effectively turns into sending one or more  RPCs to the data center scheduler.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e定时任务分布式地运行，设计上要能够避免集群中一台机器宕机影响。\u003c/p\u003e\n\u003cp\u003eDkron 在任务调度和集群通信上定义了“数据中心”，所有的 Dkron 实例可以通过 Gossip 组建集群。\u003cem\u003e但 Dkron 目前只支持同一数据中心下进行任务调度，不支持跨数据中心调度任务。\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eDkron 任务调度程序和执行节点通过 RPC 通信。\u003c/p\u003e\n\u003ch4\u003e1.2.3. 状态储存\u003c/h4\u003e\n\u003cblockquote\u003e\n\u003cp\u003eWe have two options: store data externally in a generally available  distributed storage, or store a small amount of state as part of the  Cron service itself. When designing the distributed Cron, we opted for  the second option.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e任务执行状态在 Dkron 被视为是与 Server 一体的，Dkron 通过 Raft FSM 在调度集群 Server 之间同步 Job 和 Job  执行历史的数据。\u003cem\u003e社区版本数据储存在内存 KV (Buntdb) 中，Pro 版本支持 etcd 储存。\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e持久化数据通过 Raft Snapshot 实现。\u003c/p\u003e\n\u003ch4\u003e1.2.4. 任务调度\u003c/h4\u003e\n\u003ch5\u003e集群\u003c/h5\u003e\n\u003cp\u003e\u003cimg src=\"/images/2021-09-01-11.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eDkron 中服务器节点分为 Server / Agent，Server 集群负责 Job 的数据存储，Server 中选举出的 Leader 负责任务调度。\u003cem\u003eServer 同时也是 Agent，可以执行 Job。\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eDkron 通过 Gossip 组建集群，通过 Raft 选举 Leader，Leader 担当 Scheduler，通过 GRPC 下发 Job 到 Gossip 集群的 Peers 节点执行。当 Leader 节点宕机，会在 Server 集群中选举出新的 Leader。\u003c/p\u003e\n\u003ch5\u003e任务执行\u003c/h5\u003e\n\u003cp\u003e\u003cimg src=\"/images/2021-09-01-12.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eDkron Job 执行中 Agent 与 Server GRPC 双向流通信，Agent 持续发送任务执行情况，Server 记录任务开始和任务结束时的执行状态，通过 Raft Apply 同步到 Server 集群。\u003c/p\u003e\n\u003ch2\u003e2. 源码分析\u003c/h2\u003e\n\u003ch3\u003e2.1. 基本概述\u003c/h3\u003e\n\u003ch4\u003e2.1.1. 主要机制\u003c/h4\u003e\n\u003cp\u003e\u003cimg src=\"/images/2021-09-01-13.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eDkron 主要基于三大机制实现分布式任务调度：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e通过 Gossip 库 Serf 实现节点发现、集群组建，监听 Gossip Member 变化事件，发现 Server/Agent 的 Peer 通信地址及活跃状态；\u003c/li\u003e\n\u003cli\u003e通过 Raft 实现 Server 集群 Leader 选举，实现 FSM 接口，通过 Raft Log 实现 Server 集群数据同步、数据快照、数据最终一致性；\u003c/li\u003e\n\u003cli\u003e利用 go-plugin 实现插件机制，基于 GRPC 通信 Agent 实时返回任务执行状态。\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch4\u003e2.1.2. 目录结构\u003c/h4\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003e$ tree -A -L 1\n.\n# 自带的插件 /Processor 的 main 包\n├── builtin\n# CLI\n├── cmd\n# 主要功能实现\n├── dkron\n├── Dockerfile\n# 封装 cron\n├── extcron\n├── go.mod\n├── main.go\n# 生成的 pb.go 以及通信逻辑\n├── plugin\n# 定义 Job 以及通信协议\n├── proto\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e2.2. 组建集群\u003c/h3\u003e\n\u003ch4\u003e2.2.1. 节点发现\u003c/h4\u003e\n\u003cp\u003eDkron 使用 \u003ca href=\"hashicorp/go-discover\"\u003ehashicorp/go-discover\u003c/a\u003e 进行节点 IP 的自动发现，方便通过云厂商接口、K8s 进行初始化服务发现。\u003cem\u003e用于发现 Peer 节点，进行 Bootstrap 或者 Join 集群。\u003c/em\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// dkron/retry_join.go\nfunc (r *retryJoiner) retryJoin(logger *logrus.Entry) error {\n    ...\n\t// 使用 go-discovery 发通过不同的基础设施自动发现 IP\n\t// Copy the default providers, and then add the non-default\n\tproviders := make(map[string]discover.Provider)\n\tfor k, v := range discover.Providers {\n\t\tproviders[k] = v\n\t}\n\n\t// 尝试 In-Cluster 方式从 \"/var/run/secrets/kubernetes.io/serviceaccount\"\n\t//\t获取 kubeconfig\n\t//\tref: https://github.com/kubernetes/client-go/tree/master/examples/in-cluster-client-configuration\n\t//\t获取 PodIP 列表\n\tproviders[\"k8s\"] = \u0026#x26;discoverk8s.Provider{}\n\n\tdisco, err := discover.New(\n\t\tdiscover.WithUserAgent(UserAgent()),\n\t\tdiscover.WithProviders(providers),\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tattempt := 0\n\tfor {\n\t\tvar addrs []string\n\t\tvar err error\n\n\t\tfor _, addr := range r.addrs {\n\t\t\tswitch {\n\t\t\t\t// 使用 go-discovery 发现 IP\n\t\t\tcase strings.Contains(addr, \"provider=\"):\n\t\t\t\tservers, err := disco.Addrs(addr, log.New(logger.Logger.Writer(), \"\", log.LstdFlags|log.Lshortfile))\n\t\t\t\tif err != nil {\n                    ...\n\t\t\t\t} else {\n\t\t\t\t\taddrs = append(addrs, servers...)\n\t\t\t\t}\n\n\t\t\tdefault:\n\t\t\t\tipAddr, err := ParseSingleIPTemplate(addr)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlogger.WithField(\"addr\", addr).WithError(err).Error(\"agent: Error parsing retry-join ip template\")\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\taddrs = append(addrs, ipAddr)\n\t\t\t}\n\t\t}\n\n\t\tif len(addrs) \u003e 0 {\n\t\t\t// Serf 加入集群\n\t\t\tn, err := r.join(addrs)\n\t\t\tif err == nil {\n\t\t\t\tlogger.Infof(\"agent: Join %s completed. Synced with %d initial agents\", r.cluster, n)\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\n\t\tattempt++\n\t}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e在 K8s 部署方式中程序通过创建 go-client，请求 K8s API Server 筛选获取一批 Dkron 的 Pod IP。\u003c/p\u003e\n\u003ch4\u003e2.2.2. Gossip 集群\u003c/h4\u003e\n\u003cp\u003eDkron 使用 \u003ca href=\"https://github.com/hashicorp/serf\"\u003ehashicorp/serf\u003c/a\u003e 库（Gossip 封装）的 Member 事件 Handler，监听集群成员变化事件，进行集群 Peers 的管理。\u003c/p\u003e\n\u003cp\u003e默认使用 \u003ccode\u003e8946\u003c/code\u003e 端口进行 Gossip 集群的 Peers 通信。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// dkron/agent.go\n// setupSerf is used to create the agent we use\nfunc (a *Agent) setupSerf() (*serf.Serf, error) {\n    ...\n\tserfConfig := serf.DefaultConfig()\n\tserfConfig.Init()\n\n\t// metadata\n\tserfConfig.Tags = a.config.Tags\n\tserfConfig.Tags[\"role\"] = \"dkron\"\n\t// 默认 \"dc1\"\n\tserfConfig.Tags[\"dc\"] = a.config.Datacenter\n\t// 默认 \"global\"\n\tserfConfig.Tags[\"region\"] = a.config.Region\n\tserfConfig.Tags[\"version\"] = Version\n\tif a.config.Server {\n\t\tserfConfig.Tags[\"server\"] = strconv.FormatBool(a.config.Server)\n\t}\n\tif a.config.Bootstrap {\n\t\tserfConfig.Tags[\"bootstrap\"] = \"1\"\n\t}\n\tif a.config.BootstrapExpect != 0 {\n\t\tserfConfig.Tags[\"expect\"] = fmt.Sprintf(\"%d\", a.config.BootstrapExpect)\n\t}\n\n\t// gossip 默认连接环境/连接参数\n\tswitch config.Profile {\n\tcase \"lan\":\n\t\tserfConfig.MemberlistConfig = memberlist.DefaultLANConfig()\n\tcase \"wan\":\n\t\tserfConfig.MemberlistConfig = memberlist.DefaultWANConfig()\n\tcase \"local\":\n\t\tserfConfig.MemberlistConfig = memberlist.DefaultLocalConfig()\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unknown profile: %s\", config.Profile)\n\t}\n\n\tserfConfig.MemberlistConfig.BindAddr = bindIP\n\tserfConfig.MemberlistConfig.BindPort = bindPort\n\tserfConfig.MemberlistConfig.AdvertiseAddr = advertiseIP\n\tserfConfig.MemberlistConfig.AdvertisePort = advertisePort\n\tserfConfig.MemberlistConfig.SecretKey = encryptKey\n\tserfConfig.NodeName = config.NodeName\n\tserfConfig.Tags = config.Tags\n\n\tif err != nil {\n\t\ta.logger.Fatal(err)\n\t}\n\n\t// Create a channel to listen for events from Serf\n\ta.eventCh = make(chan serf.Event, 2048)\n\tserfConfig.EventCh = a.eventCh\n\n\t// Start Serf\n\ta.logger.Info(\"agent: Dkron agent starting\")\n\n\t// Create serf first\n\treturn serf.Create(serfConfig)\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/hashicorp/memberlist\"\u003ehashicorp/memberlist\u003c/a\u003e 提供三个默认配置，局域网络的配置 DefaultLANConfig、外网 DefaultWANConfig、本地 DefaultLocalConfig，后两者都是在 DefaultLANConfig 基础上修改，区别是根据网速，调整了Gossip interval、TCP 超时时间等。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// dkron/retry_join.go\nfunc (r *retryJoiner) retryJoin(logger *logrus.Entry) error {\n\t...\n\t\tif len(addrs) \u003e 0 {\n\t\t\t// Serf 加入集群\n\t\t\tn, err := r.join(addrs)\n\t\t\tif err == nil {\n\t\t\t\tlogger.Infof(\"agent: Join %s completed. Synced with %d initial agents\", r.cluster, n)\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\n\t\tattempt++\n\t\ttime.Sleep(r.interval)\n\t...\n}\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e程序启动后反复尝试加入 Gossip 集群。\u003c/p\u003e\n\u003ch4\u003e2.2.3. 端口复用\u003c/h4\u003e\n\u003cp\u003eDkron 默认使用 \u003ccode\u003e6868\u003c/code\u003e 端口作为 Dkron 的 Peers RPC 通信端口。\u003c/p\u003e\n\u003cp\u003e默认会挂载 GRPC Server 用于服务调用。在节点为 Server 时同时开启 Raft 协议，复用端口。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// StartServer launch a new dkron server process\n// 启动 Server\nfunc (a *Agent) StartServer() {\n    ...\n\t// 创建端口复用 cmux\n\t// Create a cmux object.\n\ttcpm := cmux.New(a.listener)\n\tvar grpcl, raftl net.Listener\n\n\t// If TLS config present listen to TLS\n\tif a.TLSConfig != nil {\n\t\t...\n        // 默认不启用 TLS\n\t} else {\n\t\t// 实现 raft.StreamLayer\n\t\ta.raftLayer = NewRaftLayer(a.logger)\n\n\t\t// cmux match grpc 通信\n\t\tgrpcl = tcpm.MatchWithWriters(cmux.HTTP2MatchHeaderFieldSendSettings(\"content-type\", \"application/grpc\"))\n\n\t\t// raft TCP 通信\n\t\traftl = tcpm.Match(cmux.Any())\n\t}\n\n\t// 创建 GRPC Server\n\tif a.GRPCServer == nil {\n\t\ta.GRPCServer = NewGRPCServer(a, a.logger)\n\t}\n\n\t// 启动 GRPC Server\n\tif err := a.GRPCServer.Serve(grpcl); err != nil {\n\t\ta.logger.WithError(err).Fatal(\"agent: RPC server failed to start\")\n\t}\n\n\t// 绑定 net Listener\n\tif err := a.raftLayer.Open(raftl); err != nil {\n\t\ta.logger.Fatal(err)\n\t}\n\n\t// 启动 Raft\n\tif err := a.setupRaft(); err != nil {\n\t\ta.logger.WithError(err).Fatal(\"agent: Raft layer failed to start\")\n\t}\n\n\t// Start serving everything\n\tgo func() {\n\t\t// 正式开始 listen\n\t\tif err := tcpm.Serve(); err != nil {\n\t\t\ta.logger.Fatal(err)\n\t\t}\n\t}()\n   ...\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e使用 \u003ca href=\"https://github.com/soheilhy/cmux\"\u003esoheilhy/cmux\u003c/a\u003e 包进行端口复用，读取字节流前 N 字节（Match 规则）匹配 HTTP/2 GRPC 请求。\u003c/p\u003e\n\u003ch3\u003e2.3. Raft 协议\u003c/h3\u003e\n\u003cp\u003eDkron 使用的是 \u003ca href=\"https://github.com/hashicorp/raft\"\u003ehashicorp/raft\u003c/a\u003e 实现的 Raft 协议。\u003c/p\u003e\n\u003ch4\u003e2.3.1. 集群初始化\u003c/h4\u003e\n\u003ch5\u003e创建 Raft\u003c/h5\u003e\n\u003cp\u003eDkron 可以指定一台节点 Bootstrap，也可以设置 \u003ccode\u003ebootstrap-expect\u003c/code\u003e 等待集群 Peers 数量达到指定个数，再进行 Raft 集群 Bootstrap。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// 初始化 Raft\nfunc (a *Agent) setupRaft() error {\n\tif a.config.BootstrapExpect \u003e 0 {\n\t\tif a.config.BootstrapExpect == 1 {\n\t\t\t// 进行 bootstrap\n\t\t\ta.config.Bootstrap = true\n\t\t}\n\t}\n\n\t// 创建 raft transport\n\t// （Raft 节点间的通信通道），节点之间需要通过这个通道来进行日志同步、领导者选举等等\n\t// 方法内部启动协程 listen listener\n\ttransport := raft.NewNetworkTransport(a.raftLayer, 3, raftTimeout, logger)\n\ta.raftTransport = transport\n\n\tconfig := raft.DefaultConfig()\n\n\t// Raft performance\n\t// 默认值为 1\n\traftMultiplier := a.config.RaftMultiplier\n\tif raftMultiplier \u0026#x3C; 1 || raftMultiplier \u003e 10 {\n\t\treturn fmt.Errorf(\"raft-multiplier cannot be %d. Must be between 1 and 10\", raftMultiplier)\n\t}\n\tconfig.HeartbeatTimeout = config.HeartbeatTimeout * time.Duration(raftMultiplier)\n\tconfig.ElectionTimeout = config.ElectionTimeout * time.Duration(raftMultiplier)\n\tconfig.LeaderLeaseTimeout = config.LeaderLeaseTimeout * time.Duration(a.config.RaftMultiplier)\n\n\tconfig.LogOutput = logger\n\tconfig.LocalID = raft.ServerID(a.config.NodeName)\n\n\t// Build an all in-memory setup for dev mode, otherwise prepare a full\n\t// disk-based setup.\n\tvar logStore raft.LogStore\n\tvar stableStore raft.StableStore\n\tvar snapshots raft.SnapshotStore\n\tif a.config.DevMode {\n\t\t...\n\t} else {\n\t\tvar err error\n\n\t\t// （快照存储，用来存储节点的快照信息），也就是压缩后的日志数据\n\t\tsnapshots, err = raft.NewFileSnapshotStore(filepath.Join(a.config.DataDir, \"raft\"), 3, logger)\n\n\t\t// Create the BoltDB backend\n\t\ts, err := raftboltdb.NewBoltStore(filepath.Join(a.config.DataDir, \"raft\", \"raft.db\"))\n\n\t\ta.raftStore = s\n\t\t// （稳定存储，用来存储 Raft 集群的节点信息等），比如，当前任期编号、最新投票时的任期编号等\n\t\tstableStore = s\n\n\t\t// 512 size 内存缓存\n\t\tcacheStore, err := raft.NewLogCache(raftLogCacheSize, s)\n\t\tif err != nil {\n\t\t\ts.Close()\n\t\t\treturn err\n\t\t}\n\t\t// 用来存储 Raft 的日志\n\t\tlogStore = cacheStore\n        ...\n\t}\n\n    ...\n\t//\t参数: 实现了 Storage 的 DB, 默认 nil, logger\n\t// 创建实现 raft FSM 接口\n\t// raft 只是定义了一个接口，最终交给应用层实现。应用层收到 Log 后按 业务需求 还原为 应用数据保存起来\n\t//\tRaft 启动时 便 Raft.runFSM 起一个goroutine 从 fsmMutateCh channel 消费log ==\u003e FSM.Apply\n\tfsm := newFSM(a.Store, a.ProAppliers, a.logger)\n\t// 创建 raft 节点\n\trft, err := raft.NewRaft(config, fsm, logStore, stableStore, snapshots, transport)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"new raft: %s\", err)\n\t}\n\t// 选举 leader 信号 chan\n\ta.leaderCh = rft.LeaderCh()\n\ta.raft = rft\n\n\treturn nil\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e默认使用 \u003ccode\u003e/dkron/raft\u003c/code\u003e 目录储存 Raft Log 和快照。使用 \u003ca href=\"https://github.com/hashicorp/raft-boltdb\"\u003ehashicorp/raft-boltdb\u003c/a\u003e 作为 Raft Log 和 stable 储存，快照使用文件储存。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e线上使用情况：Server 运行 7 天每 30s 执行 5 个 Job ，\u003ccode\u003eraft.db\u003c/code\u003e 大小为 25M，内存占用在 200M 左右。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e数据量不大应该不用考虑大量磁盘和内存占用。\u003c/p\u003e\n\u003ch5\u003e创建集群\u003c/h5\u003e\n\u003cp\u003e如果是初始化集群，指定 \u003ccode\u003ebootstrap-expect\u003c/code\u003e 参数，则会监听 Gossip 集群成员变化，等待 Peers 数量达到预期时，进行 Raft 的 Bootstrap 操作。\u003c/p\u003e\n\u003cp\u003e如果已存在 Raft 集群，则新的 Server 加入 Raft 集群的成员中。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// dkron/agent.go\n// 处理 Serf 事件\nfunc (a *Agent) eventLoop() {\n    // 只读的 Shutdown channel\n\tserfShutdownCh := a.serf.ShutdownCh()\n\tfor {\n\t\tselect {\n\t\t\t// Serf event 事件处理\n\t\tcase e := \u0026#x3C;-a.eventCh:\n\t\t\t// 有三种事件 MemberEvent/UserEvent/Query\n\t\t\t// 这里只处理 MemberEvent, 只使用 Serf 做集群 member 的管理\n\t\t\t// 实际运行时主要的 MemberEvent 事件有 update/failed/join\n\t\t\t// 没有使用到自定义 event 以及 query 命令\n\n\t\t\t// Log all member events\n\t\t\tif me, ok := e.(serf.MemberEvent); ok {\n\t\t\t\tfor _, member := range me.Members {\n\t\t\t\t\ta.logger.WithFields(logrus.Fields{\n\t\t\t\t\t\t\"node\":   a.config.NodeName,\n\t\t\t\t\t\t\"member\": member.Name,\n\t\t\t\t\t\t\"event\":  e.EventType(),\n\t\t\t\t\t}).Debug(\"agent: Member event\")\n\t\t\t\t}\n\n\t\t\t\t// serfEventHandler is used to handle events from the serf cluster\n\t\t\t\tswitch e.EventType() {\n\t\t\t\tcase serf.EventMemberJoin:\n\t\t\t\t\ta.nodeJoin(me)\n\t\t\t\t\ta.localMemberEvent(me)\n\t\t\t\tcase serf.EventMemberLeave, serf.EventMemberFailed:\n\t\t\t\t\ta.nodeFailed(me)\n\t\t\t\t\ta.localMemberEvent(me)\n\t\t\t\tcase serf.EventMemberReap:\n\t\t\t\t\ta.localMemberEvent(me)\n\t\t\t\tcase serf.EventMemberUpdate, serf.EventUser, serf.EventQuery: // Ignore\n\t\t\t\tdefault:\n\t\t\t\t\ta.logger.WithField(\"event\", e.String()).Warn(\"agent: Unhandled serf event\")\n\t\t\t\t}\n\t\t\t}\n\n\t\tcase \u0026#x3C;-serfShutdownCh:\n\t\t\ta.logger.Warn(\"agent: Serf shutdown detected, quitting\")\n\t\t\treturn\n\t\t}\n\t}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSerf 的事件有三种类型：MemberEvent / UserEvent / Query，Dkron 只处理 MemberEvent，只使用 Serf 做集群 Peers 的管理，没有使用到自定义 event 以及 query 命令。实际运行时主要的 MemberEvent 事件有 \u003ccode\u003emember-update\u003c/code\u003e / \u003ccode\u003emember-failed\u003c/code\u003e / \u003ccode\u003emember-join\u003c/code\u003e / \u003ccode\u003emember-reap\u003c/code\u003e。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// dkron/serf.go\n// maybeBootstrap is used to handle bootstrapping when a new server joins\nfunc (a *Agent) maybeBootstrap() {\n\t...\n\t// 存在 raft commit 日志, 不需要 bootstrap\n\tif index != 0 {\n\t\ta.config.BootstrapExpect = 0\n\t\treturn\n\t}\n\n\t// Scan for all the known servers\n\tmembers := a.serf.Members()\n\tvar servers []ServerParts\n\tvoters := 0\n\tfor _, member := range members {\n\t\tvalid, p := isServer(member)\n\t\tif !valid {\n\t\t\t// 跳过非 dkron server 的 member\n\t\t\tcontinue\n\t\t}\n\t\tif p.Region != a.config.Region {\n\t\t\tcontinue\n\t\t}\n\t\t...\n\t\tif valid {\n\t\t\tvoters++\n\t\t}\n\t\tservers = append(servers, *p)\n\t}\n\n\t// Skip if we haven't met the minimum expect count\n\tif voters \u0026#x3C; a.config.BootstrapExpect {\n\t\treturn\n\t}\n\n    ...\n\tfor _, server := range servers {\n\t\taddr := server.Addr.String()\n\t\taddrs = append(addrs, addr)\n\t\tid := raft.ServerID(server.ID)\n\t\tsuffrage := raft.Voter // 允许仲裁的角色\n\t\tpeer := raft.Server{\n\t\t\tID:       id,\n\t\t\tAddress:  raft.ServerAddress(addr),\n\t\t\tSuffrage: suffrage,\n\t\t}\n\t\tconfiguration.Servers = append(configuration.Servers, peer)\n\t}\n\n\t// raft 集群初始化\n\tfuture := a.raft.BootstrapCluster(configuration)\n\tif err := future.Error(); err != nil {\n\t\ta.logger.WithError(err).Error(\"agent: failed to bootstrap cluster\")\n\t}\n\n\t// Bootstrapping complete, or failed for some reason, don't enter this again\n\ta.config.BootstrapExpect = 0\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRaft 集群初始化的条件：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e没有 Commit 日志；\u003c/li\u003e\n\u003cli\u003eServer 集群达到 expect 的数量；\u003c/li\u003e\n\u003cli\u003e属于同一 region\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch5\u003e加入节点\u003c/h5\u003e\n\u003cp\u003e若 Raft 集群已存在，则 Leader 节点接收到 Serf Member 加入 Gossip 集群的事件时，进行 Raft 节点 Join 的逻辑：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// dkron/serf.go\nfunc (a *Agent) localMemberEvent(me serf.MemberEvent) {\n\t// 只有 Leader 进行操作\n\tif !a.config.Server || !a.IsLeader() {\n\t\treturn\n\t}\n\n\tfor _, m := range me.Members {\n\t\tselect {\n            // 加入重新同步的 chan\n\t\tcase a.reconcileCh \u0026#x3C;- m:\n\t\tdefault:\n\t\t}\n\t}\n}\n\n// reconcileCh 触发 reconcile 执行\nfunc (a *Agent) reconcile() error {\n\tmembers := a.serf.Members()\n\tfor _, member := range members {\n\t\tif err := a.reconcileMember(member); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// dkron/leader.go\nfunc (a *Agent) reconcileMember(member serf.Member) error {\n\t// 判断是否在一个 region, 是否是 Server\n\tvalid, parts := isServer(member)\n\tif !valid || parts.Region != a.config.Region {\n\t\treturn nil\n\t}\n\n\tvar err error\n\tswitch member.Status {\n\tcase serf.StatusAlive:\n\t\terr = a.addRaftPeer(member, parts)\n\tcase serf.StatusLeft:\n\t\terr = a.removeRaftPeer(member, parts)\n\t}\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// dkron/leader.go\nfunc (a *Agent) addRaftPeer(m serf.Member, parts *ServerParts) error {\n   ...\n\t// 获取 raft peers\n\tfor _, server := range configFuture.Configuration().Servers {\n\n\t\t// 要加入的 server 已存在于 raft peers 中, 先移除再添加\n\t\tif server.Address == raft.ServerAddress(addr) || server.ID == raft.ServerID(parts.ID) {\n            ...\n\t\t\tfuture := a.raft.RemoveServer(server.ID, 0, 0)\n\t\t\t...\n\t\t}\n\t}\n\n\t// 添加一个 raft peer\n\tswitch {\n\tcase minRaftProtocol \u003e= 3:\n\t\taddFuture := a.raft.AddVoter(raft.ServerID(parts.ID), raft.ServerAddress(addr), 0, 0)\n\t\tif err := addFuture.Error(); err != nil {\n\t\t\ta.logger.WithError(err).Error(\"dkron: failed to add raft peer\")\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003e2.3.2. FSM\u003c/h4\u003e\n\u003ch5\u003e内存 DB\u003c/h5\u003e\n\u003cp\u003eDkron 使用内存 KV \u003ca href=\"https://github.com/tidwall/buntdb\"\u003etidwall/buntdb\u003c/a\u003e 储存 Job 及执行历史记录。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// dkron/storage.go\n// Storage is the interface that should be used by any\n// storage engine implemented for dkron. It contains the\n// minumum set of operations that are needed to have a working\n// dkron store.\ntype Storage interface {\n\tSetJob(job *Job, copyDependentJobs bool) error\n\tDeleteJob(name string) (*Job, error)\n\tSetExecution(execution *Execution) (string, error)\n\tSetExecutionDone(execution *Execution) (bool, error)\n\tGetJobs(options *JobOptions) ([]*Job, error)\n\tGetJob(name string, options *JobOptions) (*Job, error)\n\tGetExecutions(jobName string, opts *ExecutionOptions) ([]*Execution, error)\n\tGetExecutionGroup(execution *Execution, opts *ExecutionOptions) ([]*Execution, error)\n\tGetGroupedExecutions(jobName string, opts *ExecutionOptions) (map[int64][]*Execution, []int64, error)\n\tShutdown() error\n\tSnapshot(w io.WriteCloser) error\n\tRestore(r io.ReadCloser) error\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ccode\u003estore\u003c/code\u003e 实现了 Storage 接口，使用的是 buntdb：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// dkron/store.go\n// NewStore creates a new Storage instance.\n// 创建基于内存的 buntdb\nfunc NewStore(logger *logrus.Entry) (*Store, error) {\n\tdb, err := buntdb.Open(\":memory:\")\n\tdb.CreateIndex(\"name\", jobsPrefix+\":*\", buntdb.IndexJSON(\"name\"))\n\tdb.CreateIndex(\"started_at\", executionsPrefix+\":*\", buntdb.IndexJSON(\"started_at\"))\n\tdb.CreateIndex(\"finished_at\", executionsPrefix+\":*\", buntdb.IndexJSON(\"finished_at\"))\n\tdb.CreateIndex(\"attempt\", executionsPrefix+\":*\", buntdb.IndexJSON(\"attempt\"))\n\tdb.CreateIndex(\"displayname\", jobsPrefix+\":*\", buntdb.IndexJSON(\"displayname\"))\n\tdb.CreateIndex(\"schedule\", jobsPrefix+\":*\", buntdb.IndexJSON(\"schedule\"))\n\tdb.CreateIndex(\"success_count\", jobsPrefix+\":*\", buntdb.IndexJSON(\"success_count\"))\n\tdb.CreateIndex(\"error_count\", jobsPrefix+\":*\", buntdb.IndexJSON(\"error_count\"))\n\tdb.CreateIndex(\"last_success\", jobsPrefix+\":*\", buntdb.IndexJSON(\"last_success\"))\n\tdb.CreateIndex(\"last_error\", jobsPrefix+\":*\", buntdb.IndexJSON(\"last_error\"))\n\tdb.CreateIndex(\"next\", jobsPrefix+\":*\", buntdb.IndexJSON(\"next\"))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tstore := \u0026#x26;Store{\n\t\tdb:     db,\n\t\tlock:   \u0026#x26;sync.Mutex{},\n\t\tlogger: logger,\n\t}\n\n\treturn store, nil\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e官方的 Pro 版本实现的 etcd 储存，可能是实现了 Storage 的另一个实例，或者是在 Raft FSM 进行了额外的处理执行外部数据库同步。\u003c/p\u003e\n\u003ch5\u003e实现 FSM 接口\u003c/h5\u003e\n\u003cp\u003eRaft FSM 接口：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// FSM provides an interface that can be implemented by\n// clients to make use of the replicated log.\ntype FSM interface {\n\t// Apply log is invoked once a log entry is committed.\n\t// It returns a value which will be made available in the\n\t// ApplyFuture returned by Raft.Apply method if that\n\t// method was called on the same Raft node as the FSM.\n\tApply(*Log) interface{}\n\n\t// Snapshot is used to support log compaction. This call should\n\t// return an FSMSnapshot which can be used to save a point-in-time\n\t// snapshot of the FSM. Apply and Snapshot are not called in multiple\n\t// threads, but Apply will be called concurrently with Persist. This means\n\t// the FSM should be implemented in a fashion that allows for concurrent\n\t// updates while a snapshot is happening.\n\tSnapshot() (FSMSnapshot, error)\n\n\t// Restore is used to restore an FSM from a snapshot. It is not called\n\t// concurrently with any other command. The FSM must discard all previous\n\t// state.\n\tRestore(io.ReadCloser) error\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ccode\u003eApply\u003c/code\u003e 函数会在从 Log 载入一条数据的时候被调用。\u003cem\u003eLeader 会调用 Apply 进行数据同步。\u003c/em\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// dkron/fsm.go\ntype dkronFSM struct {\n\tstore Storage\n\n\t// proAppliers holds the set of pro only LogAppliers\n\tproAppliers LogAppliers\n\tlogger      *logrus.Entry\n}\n\n// 创建/同步一条 Log 到 state\nfunc (d *dkronFSM) Apply(l *raft.Log) interface{} {\n\tbuf := l.Data\n\tmsgType := MessageType(buf[0])\n\n\tswitch msgType {\n\tcase SetJobType:\n\t\treturn d.applySetJob(buf[1:])\n\tcase DeleteJobType:\n\t\treturn d.applyDeleteJob(buf[1:])\n\tcase ExecutionDoneType:\n\t\treturn d.applyExecutionDone(buf[1:])\n\tcase SetExecutionType:\n\t\treturn d.applySetExecution(buf[1:])\n\t}\n\n\t// Dkron Pro 版本额外的操作, 可能在此处执行外部数据库同步\n\tif applier, ok := d.proAppliers[msgType]; ok {\n\t\treturn applier(buf[1:], l.Index)\n\t}\n\n\treturn nil\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e实现的快照和恢复方法，使用 buntdb 的 load/save：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// dkron/fsm.go\n// FSM 的快照和恢复方法，使用 buntdb 的 load/save 方法。\n\n// Snapshot returns a snapshot of the key-value store. We wrap\n// the things we need in dkronSnapshot and then send that over to Persist.\n// Persist encodes the needed data from dkronSnapshot and transport it to\n// Restore where the necessary data is replicated into the finite state machine.\n// This allows the consensus algorithm to truncate the replicated log.\nfunc (d *dkronFSM) Snapshot() (raft.FSMSnapshot, error) {\n\treturn \u0026#x26;dkronSnapshot{store: d.store}, nil\n}\n\n// Restore stores the key-value store to a previous state.\nfunc (d *dkronFSM) Restore(r io.ReadCloser) error {\n\tdefer r.Close()\n\treturn d.store.Restore(r)\n}\n\ntype dkronSnapshot struct {\n\tstore Storage\n}\n\nfunc (d *dkronSnapshot) Persist(sink raft.SnapshotSink) error {\n\tif err := d.store.Snapshot(sink); err != nil {\n\t\tsink.Cancel()\n\t\treturn err\n\t}\n\n\t// Close the sink.\n\tif err := sink.Close(); err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\nfunc (d *dkronSnapshot) Release() {}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch5\u003e数据格式\u003c/h5\u003e\n\u003cp\u003e通过 Raft Log (Bytes) 储存的数据格式为：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// dkron/grpc.go\n// Proto 转换成 Bytes\nfunc Encode(t MessageType, msg interface{}) ([]byte, error) {\n\tvar buf bytes.Buffer\n\tbuf.WriteByte(uint8(t))\n\tm, err := pb.Marshal(msg.(pb.Message))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t_, err = buf.Write(m)\n\treturn buf.Bytes(), err\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e首个字节标记 Message 的类型，后续为 Protobuf 序列化后的结果。\u003c/p\u003e\n\u003ch3\u003e2.4. 插件机制\u003c/h3\u003e\n\u003ch4\u003e2.4.1. 概述\u003c/h4\u003e\n\u003cp\u003e插件机制优先于任务调度本身进行分析，因为任务调度依赖于插件的通信机制。\u003c/p\u003e\n\u003cp\u003eDkron 插件使用 \u003ca href=\"https://github.com/hashicorp/go-plugin\"\u003ehashicorp/go-plugin\u003c/a\u003e 实现，简而概之，通过两种 RPC 方式（netrpc / GRPC）客户端与插件监听的端口进行通信，插件与主程序分离，作为插件机制。\u003c/p\u003e\n\u003cp\u003e引用 go-plugin 项目的说明：\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eShared libraries have one major advantage over our system which is much higher performance. In real world scenarios across our various tools, we've never required any more performance out of our plugin system and it has seen very high throughput, so this isn't a concern for us at the moment.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eGolang 出的基于链接库（.so）的插件机制还不成熟，并且 go-plugin 已经在大量软件中运用多年，对比起来前者没有优势，go-plugin 的考量在于性能对于插件机制来说不是首要的。\u003c/p\u003e\n\u003cp\u003e这里说一句，其他 Golang 的 HTTP 服务器例如 Traefik、Caddy 的插件机制都是需要修改源代码，自行增加包实现 Golang Interface，重新进行编译实现的，这种侵入性更大，但是性能更好。\u003c/p\u003e\n\u003cp\u003eGolang 官方的插件机制到目前还不支持 Windows，高性能的 WASM Golang 运行时库（用作 WASM 插件机制） \u003ca href=\"https://github.com/wasmerio/wasmer-go\"\u003ewasmerio/wasmer-go\u003c/a\u003e 目前也不支持 Windows。\u003c/p\u003e\n\u003ch4\u003e2.4.2. 加载插件\u003c/h4\u003e\n\u003cp\u003eDkron 默认会在一些目录查找插件的二进制文件：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// cmd/plugins.go\nfunc (p *Plugins) DiscoverPlugins() error {\n\tp.Processors = make(map[string]dkplugin.Processor)\n\tp.Executors = make(map[string]dkplugin.Executor)\n\n\t// Look in /etc/dkron/plugins\n\t// 匹配目录下的文件列表\n\tprocessors, err := plugin.Discover(\"dkron-processor-*\", filepath.Join(\"/etc\", \"dkron\", \"plugins\"))\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Look in /etc/dkron/plugins\n\texecutors, err := plugin.Discover(\"dkron-executor-*\", filepath.Join(\"/etc\", \"dkron\", \"plugins\"))\n\tif err != nil {\n\t\treturn err\n\t}\n\n\texePath, err := osext.Executable()\n\tif err != nil {\n\t\tlogrus.WithError(err).Error(\"Error loading exe directory\")\n\t} else {\n\t\t// 从当前 agent 执行目录查找，同目录的可执行文件\n\t\t// 默认执行目录为 /opt/local/dkron/\n\t\t// 若我们需要添加自定义插件, 将编译后的二进制文件放入同目录即可\n\t\tp, err := plugin.Discover(\"dkron-processor-*\", filepath.Dir(exePath))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tprocessors = append(processors, p...)\n\t\te, err := plugin.Discover(\"dkron-executor-*\", filepath.Dir(exePath))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\texecutors = append(executors, e...)\n\t}\n\n\t// pluginFactory\n\t// 创建运行时\n\tfor _, file := range processors {\n\t\t// 文件名按 \"-\" 分隔，取最后一个元素\n\t\t// 问题点: 插件名不支持带 \"-\"\n\t\tpluginName, ok := getPluginName(file)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\n\t\traw, err := p.pluginFactory(file, dkplugin.ProcessorPluginName)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tp.Processors[pluginName] = raw.(dkplugin.Processor)\n\t}\n\n\t...\n\n\treturn nil\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003e2.4.3. 插件实现\u003c/h4\u003e\n\u003cp\u003eDkron 封装的 Plugin 结构, 规定可以有 processor 或 executor 两个插件。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// PluginMap should be used by clients for the map of plugins.\n// Dkron 封装的 Plugin 结构, 规定可以有 processor 或 executor 两个插件\nvar PluginMap = map[string]plugin.Plugin{\n\t\"processor\": \u0026#x26;ProcessorPlugin{},\n\t\"executor\":  \u0026#x26;ExecutorPlugin{},\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch5\u003e构建插件\u003c/h5\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// cmd/plugins.go\n// 创建 Plugin Client\nfunc (p *Plugins) pluginFactory(path string, pluginType string) (interface{}, error) {\n\tvar config plugin.ClientConfig\n\t// 可执行文件\n\tconfig.Cmd = exec.Command(path)\n\n\t// handshake 配置是 client 与 server 约定的 TOKEN,\n\t//\t不通过环境变量包含同样的 TOKEN 则无法启动 plugin 程序.\n\t//\t具体查看 go-plugin 项目 https://github.com/mayocream/go-plugin/blob/044aadd925bf9f027cb301b2af9bc6b60775dd22/server.go#L248\n\tconfig.HandshakeConfig = dkplugin.Handshake\n\n\t// go-plugin 包会在 NewClient 的时候储存 Client 的指针,\n\t//\t能够调用 cleanClients 统一 Kill 所有的 Client\n\tconfig.Managed = true\n\n\t// 定义一个二进制所能包含的不同插件\n\tconfig.Plugins = dkplugin.PluginMap\n\tconfig.SyncStdout = os.Stdout\n\tconfig.SyncStderr = os.Stderr\n\n\tswitch pluginType {\n\tcase dkplugin.ProcessorPluginName:\n\t\t// processor 使用 golang net/rpc 进行通信\n\t\tconfig.AllowedProtocols = []plugin.Protocol{plugin.ProtocolNetRPC}\n\tcase dkplugin.ExecutorPluginName:\n\t\t// executor 使用 gprc 进行通信\n\t\tconfig.AllowedProtocols = []plugin.Protocol{plugin.ProtocolGRPC}\n\t}\n\n\t// 初始化客户端\n\tclient := plugin.NewClient(\u0026#x26;config)\n\n\t// go-plugin Client() 会用 exec.Start 启动 plugin server,\n\t//\t创建 rpc client, 这个库将连接复用, 以及 rpc/grpc service 挂载\n\t//\t等细节屏蔽了, 开发者只要创建业务逻辑 Interface 并实现 plugin.Plguin 的\n\t//\tServe/Client 方法就能够进行 rpc 通信\n\trpcClient, err := client.Client()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// 调用指定的 service\n\traw, err := rpcClient.Dispense(pluginType)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn raw, nil\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch5\u003e运行插件\u003c/h5\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// go-plugin 插件 start 函数\nfunc (c *Client) Start() (addr net.Addr, err error) {\n   ...\n    // 启动的环境变量\n\tenv := []string{\n\t\tfmt.Sprintf(\"%s=%s\", c.config.MagicCookieKey, c.config.MagicCookieValue),\n\t\tfmt.Sprintf(\"PLUGIN_MIN_PORT=%d\", c.config.MinPort),\n\t\tfmt.Sprintf(\"PLUGIN_MAX_PORT=%d\", c.config.MaxPort),\n\t\tfmt.Sprintf(\"PLUGIN_PROTOCOL_VERSIONS=%s\", strings.Join(versionStrings, \",\")),\n\t}\n\n\tcmd := c.config.Cmd\n\tcmd.Env = append(cmd.Env, os.Environ()...)\n\tcmd.Env = append(cmd.Env, env...)\n\tcmd.Stdin = os.Stdin\n\n    // pipe 读取 std 输出\n\tcmdStdout, err := cmd.StdoutPipe()\n\tcmdStderr, err := cmd.StderrPipe()\n\n\t// 启动二进制程序\n\terr = cmd.Start()\n\tif err != nil {\n\t\treturn\n\t}\n\n    ...\n    // 读取插件执行的 Output\n\tlinesCh := make(chan string)\n\tc.clientWaitGroup.Add(1)\n\tgo func() {\n\t\tdefer c.clientWaitGroup.Done()\n\t\tdefer close(linesCh)\n\n\t\tscanner := bufio.NewScanner(cmdStdout)\n\t\tfor scanner.Scan() {\n\t\t\tlinesCh \u0026#x3C;- scanner.Text()\n\t\t}\n\t}()\n\n\t...\n\treturn\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003e2.4.4. 插件通信\u003c/h4\u003e\n\u003ch5\u003e插件通信数据结构\u003c/h5\u003e\n\u003cpre\u003e\u003ccode class=\"language-protobuf\"\u003emessage ExecuteRequest {\n  string job_name = 1;\n  map\u0026#x3C;string, string\u003e config = 2;\n  uint32 status_server = 3;\n}\n\nmessage ExecuteResponse {\n    bytes output = 1;\n    string error = 2;\n}\n\nservice Executor {\n    rpc Execute (ExecuteRequest) returns (ExecuteResponse);\n}\n\nmessage StatusUpdateRequest {\n  bytes output = 2;\n  bool error = 3;\n}\n\nmessage StatusUpdateResponse {\n  int64 r = 1;\n}\n\nservice StatusHelper {\n  rpc Update(StatusUpdateRequest) returns (StatusUpdateResponse);\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch5\u003eGRPC 双向流通信\u003c/h5\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// Here is the gRPC client that GRPCClient talks to.\ntype ExecutorClient struct {\n\t// This is the real implementation\n\tclient types.ExecutorClient\n\tbroker *plugin.GRPCBroker\n}\n\n// ref: https://github.com/distribworks/dkron/pull/719\n// 实现执行时实时传输 output, 双向流\nfunc (m *ExecutorClient) Execute(args *types.ExecuteRequest, cb StatusHelper) (*types.ExecuteResponse, error) {\n\t// This is where the magic conversion to Proto happens\n\tstatusHelperServer := \u0026#x26;GRPCStatusHelperServer{Impl: cb}\n\n\tvar s *grpc.Server\n\tserverFunc := func(opts []grpc.ServerOption) *grpc.Server {\n\t\ts = grpc.NewServer(opts...)\n\t\ttypes.RegisterStatusHelperServer(s, statusHelperServer)\n\n\t\treturn s\n\t}\n\n\tbrokerID := m.broker.NextId()\n\tgo m.broker.AcceptAndServe(brokerID, serverFunc)\n\n\targs.StatusServer = brokerID\n\tr, err := m.client.Execute(context.Background(), args)\n\n\ts.Stop()\n\treturn r, err\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e目前线上出现过 \u003ccode\u003erpc: transport is closing\u003c/code\u003e 的错误，是 GRPC 通信的错误，推测在插件通信的该部分出现错误。\u003c/p\u003e\n\u003ch3\u003e2.5. 任务调度\u003c/h3\u003e\n\u003ch4\u003e2.5.1. 概述\u003c/h4\u003e\n\u003cp\u003eDkron Server 中为 Raft Leader 的服务器成为调度服务器，负责定时任务的分发。\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e通过 GRPC 调用下发任务到 agent 节点，Agent 执行任务并通过 GRPC 流实时返回输出；\u003c/li\u003e\n\u003cli\u003e执行完 Job 后 Leader 通过 Raft Apply 储存 Job 执行记录到各 Server 节点。\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch4\u003e2.5.2. cron 封装\u003c/h4\u003e\n\u003cp\u003eDkron 基于 \u003ca href=\"https://github.com/robfig/cron\"\u003erobfig/cron\u003c/a\u003e 增加了 \u003ccode\u003e@at\u003c/code\u003e 时间定义，允许指定只允许一次的定时任务。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// extcron/extparser.go\n// NewParser creates an ExtParser instance\n// 启用 second 字段\nfunc NewParser() cron.ScheduleParser {\n\treturn ExtParser{cron.NewParser(cron.Second | cron.Minute | cron.Hour | cron.Dom | cron.Month | cron.Dow | cron.Descriptor)}\n}\n\n// Parse parses a cron schedule specification. It accepts the cron spec with\n// mandatory seconds parameter, descriptors and the custom descriptors\n// \"@at \u0026#x3C;date\u003e\" and \"@manually\".\n// 添加了自定义解析的部分\nfunc (p ExtParser) Parse(spec string) (cron.Schedule, error) {\n\tif spec == \"@manually\" {\n\t\treturn At(time.Time{}), nil\n\t}\n\n\tconst at = \"@at \"\n\tif strings.HasPrefix(spec, at) {\n\t\tdate, err := time.Parse(time.RFC3339, spec[len(at):])\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to parse date %s: %s\", spec, err)\n\t\t}\n\t\treturn At(date), nil\n\t}\n\n\t// It's not a dkron specific spec: Let the regular cron schedule parser have it\n\treturn p.parser.Parse(spec)\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003e2.5.3. 数据一致性\u003c/h4\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// dkron/leader.go\n// 处理变成 leader 事件\nfunc (a *Agent) leaderLoop(stopCh chan struct{}) {\nRECONCILE:\n   ...\n\t// Apply a raft barrier to ensure our FSM is caught up\n\tstart := time.Now()\n\tbarrier := a.raft.Barrier(barrierWriteTimeout)\n\tif err := barrier.Error(); err != nil {\n\t\ta.logger.WithError(err).Error(\"dkron: failed to wait for barrier\")\n\t\tgoto WAIT\n\t}\n   ...\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e变成 Leader 后调用 \u003ccode\u003eraft.Barrier\u003c/code\u003e 确保 FSM 同步到最新状态。\u003c/p\u003e\n\u003ch4\u003e2.5.4. 任务下发\u003c/h4\u003e\n\u003ch5\u003e启动任务调度\u003c/h5\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// dkron/scheduler.go\n// 启动调度器\nfunc (s *Scheduler) Start(jobs []*Job, agent *Agent) error {\n\ts.Cron = cron.New(cron.WithParser(extcron.NewParser()))\n\n\tfor _, job := range jobs {\n\t\tjob.Agent = agent\n\t\t// 添加所有 Job\n\t\ts.AddJob(job)\n\t}\n\t// 开始定时执行\n\ts.Cron.Start()\n\ts.Started = true\n\tschedulerStarted.Set(1)\n\n\treturn nil\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e添加单个 Job 到 cron 定时触发：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// dkron/scheduler.go\n// AddJob Adds a job to the cron scheduler\n// 调度器添加 Job\nfunc (s *Scheduler) AddJob(job *Job) error {\n\t// Check if the job is already set and remove it if exists\n\tif _, ok := s.EntryJobMap.Load(job.Name); ok {\n\t\ts.RemoveJob(job)\n\t}\n\n\tif job.Disabled || job.ParentJob != \"\" {\n\t\treturn nil\n\t}\n    ...\n\n\t// 为 cron 添加一个 job\n\t// Job 的 Run 是 cron 触发的执行方法\n\tid, err := s.Cron.AddJob(schedule, job)\n\tif err != nil {\n\t\treturn err\n\t}\n\t// 储存 cron 的 id\n\ts.EntryJobMap.Store(job.Name, id)\n    ...\n\treturn nil\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch5\u003e触发任务调度\u003c/h5\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// dkron/job.go\n// job 的 run 方法实现 cron.Job 接口\nfunc (j *Job) Run() {\n\t// Check if it's runnable\n\tif j.isRunnable(j.logger) {\n\t\t...\n\t\tcronInspect.Set(j.Name, j)\n\n\t\t// Simple execution wrapper\n\t\tex := NewExecution(j.Name)\n\n\t\t// 触发调度运行 Job\n\t\tif _, err := j.Agent.Run(j.Name, ex); err != nil {\n\t\t\tj.logger.WithError(err).Error(\"job: Error running job\")\n\t\t}\n\t}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ccode\u003eisRunnable\u003c/code\u003e 检查任务是否被禁止，同时通过 GRPC 查询所有 Agent 当前正在执行的任务，是否有相同的 JobName，如果“不允许并发调度”则停止本次调度。\u003c/p\u003e\n\u003ch5\u003e任务分发到 Agent\u003c/h5\u003e\n\u003cp\u003e注意：Dkron 调度会将任务调度到所有符合 tags 的、同一 region 的节点（Serf Members）上。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// dkron/run.go\n// 调度运行 Job -\u003e 分发到 agent 执行任务\nfunc (a *Agent) Run(jobName string, ex *Execution) (*Job, error) {\n\tjob, err := a.Store.GetJob(jobName, nil)\n    ...\n\t// In case the job is not a child job, compute the next execution time\n\tif job.ParentJob == \"\" {\n\t\t// 获取 cron.Entry\n\t\tif e, ok := a.sched.GetEntry(jobName); ok {\n\t\t\t// 获取下一次执行时间\n\t\t\tjob.Next = e.Next\n\t\t\t// 同步 job 数据\n\t\t\tif err := a.applySetJob(job.ToProto()); err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\"agent: Run error storing job %s before running: %w\", jobName, err)\n\t\t\t}\n\t\t} else {\n\t\t\treturn nil, fmt.Errorf(\"agent: Run error retrieving job: %s from scheduler\", jobName)\n\t\t}\n\t}\n\n\t// In the first execution attempt we build and filter the target nodes\n\t// but we use the existing node target in case of retry.\n\tvar filterMap map[string]string\n\tif ex.Attempt \u0026#x3C;= 1 {\n\t\tfilterMap, _, err = a.processFilteredNodes(job)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"run error processing filtered nodes: %w\", err)\n\t\t}\n\t} else {\n\t\t// In case of retrying, find the rpc address of the node or return with an error\n\t\t// 重试使用同样的 Node 执行\n\t\tvar addr string\n\t\tfor _, m := range a.serf.Members() {\n\t\t\tif ex.NodeName == m.Name {\n\t\t\t\tif m.Status == serf.StatusAlive {\n\t\t\t\t\taddr = m.Tags[\"rpc_addr\"]\n\t\t\t\t} else {\n\t\t\t\t\treturn nil, fmt.Errorf(\"retry node is gone: %s for job %s\", ex.NodeName, ex.JobName)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tfilterMap = map[string]string{ex.NodeName: addr}\n\t}\n\n   ...\n\tvar wg sync.WaitGroup\n\tfor _, v := range filterMap {\n\t\t// Call here client GRPC AgentRun\n\t\twg.Add(1)\n\t\tgo func(node string, wg *sync.WaitGroup) {\n\t\t\tdefer wg.Done()\n\n            // 这里真正调用 GRPC 到 agent 执行\n\t\t\terr := a.GRPCClient.AgentRun(node, job.ToProto(), ex.ToProto())\n\t\t\tif err != nil {\n\t\t\t\t...\n\t\t\t}\n\t\t}(v, \u0026#x26;wg)\n\t}\n\n\t// 等待所有节点执行完\n\twg.Wait()\n\treturn job, nil\n}\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eServer 对执行状态的处理：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eGRPC 通信开始，Agent 接收到任务；\u003c/li\u003e\n\u003cli\u003eGRPC 结束，Agent 执行完成（成功或失败）任务；\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// dkron/grpc_client.go\n// Dkron server 调用此方法通过 GRPC 下发 Job 到 server/agent 执行\nfunc (grpcc *GRPCClient) AgentRun(addr string, job *proto.Job, execution *proto.Execution) error {\n\tvar conn *grpc.ClientConn\n\n\t// (MAYO): remove string type wrap\n\tconn, err := grpcc.Connect(addr)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer conn.Close()\n\n\t// Streaming call\n\ta := proto.NewAgentClient(conn)\n\tstream, err := a.AgentRun(context.Background(), \u0026#x26;proto.AgentRunRequest{\n\t\tJob:       job,\n\t\tExecution: execution,\n\t})\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tvar first bool\n\tfor {\n\t\t// 读取 GRPC 流\n\t\tars, err := stream.Recv()\n\n\t\t// Stream ends\n\t\tif err == io.EOF {\n\t\t\t// 任务执行结束, 发送 done 命令给 leader 持久化储存\n\t\t\taddr := grpcc.agent.raft.Leader()\n\t\t\tif err := grpcc.ExecutionDone(string(addr), NewExecutionFromProto(execution)); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\n\t\t// Error received from the stream\n\t\tif err != nil {\n\t\t\t// At this point the execution status will be unknown, set the FinshedAt time and an explanatory message\n\t\t\texecution.FinishedAt = ptypes.TimestampNow()\n\t\t\texecution.Output = []byte(err.Error())\n\n\t\t\tgrpcc.logger.WithError(err).Error(ErrBrokenStream)\n\n\t\t\taddr := grpcc.agent.raft.Leader()\n\t\t\tif err := grpcc.ExecutionDone(string(addr), NewExecutionFromProto(execution)); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\treturn err\n\t\t}\n\n\t\t// Registers an active stream\n\t\tgrpcc.agent.activeExecutions.Store(ars.Execution.Key(), ars.Execution)\n\t\texecution = ars.Execution\n\t\tdefer grpcc.agent.activeExecutions.Delete(execution.Key())\n\n\t\t// Store the received execution in the raft log and store\n\t\tif !first {\n\t\t\t// 储存执行状态\n\t\t\tif err := grpcc.SetExecution(ars.Execution); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tfirst = true\n\t\t}\n\t}\n}\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch5\u003eAgent 任务执行\u003c/h5\u003e\n\u003cp\u003e任务执行过程：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e实时发送执行情况到 Server；\u003c/li\u003e\n\u003cli\u003eServer 宕机，切换一个 Server 发送最终状态；\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// dkron/grpc_agent.go\n// Dkron agent 执行任务, GRPC 客户端推流\nfunc (as *AgentServer) AgentRun(req *types.AgentRunRequest, stream types.Agent_AgentRunServer) error {\n\tjob := req.Job\n\texecution := req.Execution\n    // buffer 创建, 储存执行输出\n\toutput, _ := circbuf.NewBuffer(maxBufSize)\n\n\tvar success bool\n\n\tjex := job.Executor\n\texc := job.ExecutorConfig\n\n\texecution.StartedAt = ptypes.TimestampNow()\n\texecution.NodeName = as.agent.config.NodeName\n\n\t// 发送执行前状态\n\tif err := stream.Send(\u0026#x26;types.AgentRunStream{\n\t\tExecution: execution,\n\t}); err != nil {\n\t\treturn err\n\t}\n\n\t...\n\n\t// Check if executor exists\n\t// 找到对应的执行插件\n\tif executor, ok := as.agent.ExecutorPlugins[jex]; ok {\n\t\tas.logger.WithField(\"plugin\", jex).Debug(\"grpc_agent: calling executor plugin\")\n\t\trunningExecutions.Store(execution.GetGroup(), execution)\n\t\t// go-plugin grpc 调用执行\n\t\tout, err := executor.Execute(\u0026#x26;types.ExecuteRequest{\n\t\t\tJobName: job.Name,\n\t\t\tConfig:  exc,\n\t\t\t// callback, 将执行输出结果赋值到 output, 通过 stream 发送给服务端\n\t\t\t// ref: https://github.com/distribworks/dkron/pull/719\n\t\t}, \u0026#x26;statusAgentHelper{\n\t\t\tstream:    stream,\n\t\t\texecution: execution,\n\t\t})\n        ...\n\n\t\tif out != nil {\n\t\t\toutput.Write(out.Output)\n\t\t}\n\t} else {\n\t   ...\n\t\toutput.Write([]byte(\"grpc_agent: Specified executor is not present\"))\n\t}\n\n\t// 执行完成\n\texecution.FinishedAt = ptypes.TimestampNow()\n\texecution.Success = success\n\texecution.Output = output.Bytes()\n\n\trunningExecutions.Delete(execution.GetGroup())\n\n\t// 发送最终状态\n\tif err := stream.Send(\u0026#x26;types.AgentRunStream{\n\t\tExecution: execution,\n\t}); err != nil {\n\t\t// 有可能 server 没能接收到最后执行状态\n        ...\n\t\t// TCP 连接筛选一个 server\n\t\trpcServer, err := as.agent.checkAndSelectServer()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// 调用执行完成\n\t\treturn as.agent.GRPCClient.ExecutionDone(rpcServer, NewExecutionFromProto(execution))\n\t}\n\n\treturn nil\n}\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch5\u003e任务执行后处理\u003c/h5\u003e\n\u003cp\u003eJob 执行完成后处理：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e必须为 Leader 进行数据持久化；\u003c/li\u003e\n\u003cli\u003e通过 Raft Apply 同步 Server 间执行记录；\u003c/li\u003e\n\u003cli\u003e错误重试，再次进行任务调度；\u003c/li\u003e\n\u003cli\u003e执行 Job 定义中依赖的 Job。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode class=\"language-go\"\u003e// dkron/grpc.go\n// 执行完成, 进行后续处理\nfunc (grpcs *GRPCServer) ExecutionDone(ctx context.Context, execDoneReq *proto.ExecutionDoneRequest) (*proto.ExecutionDoneResponse, error) {\n    ...\n\tif !grpcs.agent.IsLeader() {\n\t\taddr := grpcs.agent.raft.Leader()\n\t\t// 如果我不是 leader , 将请求发给 leader 执行\n\t\tgrpcs.agent.GRPCClient.ExecutionDone(string(addr), NewExecutionFromProto(execDoneReq.Execution))\n\t\treturn nil, ErrNotLeader\n\t}\n\n\t// This is the leader at this point, so process the execution, encode the value and apply the log to the cluster.\n\t// Get the defined output types for the job, and call them\n\tjob, err := grpcs.agent.Store.GetJob(execDoneReq.Execution.JobName, nil)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// 执行 processor, 不需要双向通信\n\t// 由此推测 exector 使用 grpc 执行是需要 GRPC 双向流\n\tpbex := *execDoneReq.Execution\n\tfor k, v := range job.Processors {\n\t\tif processor, ok := grpcs.agent.ProcessorPlugins[k]; ok {\n\t\t\tv[\"reporting_node\"] = grpcs.agent.config.NodeName\n\t\t\tpbex = processor.Process(\u0026#x26;plugin.ProcessorArgs{Execution: pbex, Config: v})\n\t\t} else {\n\t\t\t...\n\t\t}\n\t}\n\n\t// 同步集群状态\n\texecDoneReq.Execution = \u0026#x26;pbex\n\tcmd, err := Encode(ExecutionDoneType, execDoneReq)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\taf := grpcs.agent.raft.Apply(cmd, raftTimeout)\n\tif err := af.Error(); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Retrieve the fresh, updated job from the store to work on stored values\n\tjob, err = grpcs.agent.Store.GetJob(job.Name, nil)\n\tif err != nil {\n\t\tgrpcs.logger.WithError(err).WithField(\"job\", execDoneReq.Execution.JobName).Error(\"grpc: Error retrieving job from store\")\n\t\treturn nil, err\n\t}\n\n\t// 任务执行重试\n\texecution := NewExecutionFromProto(\u0026#x26;pbex)\n\tif !execution.Success \u0026#x26;\u0026#x26; uint(execution.Attempt) \u0026#x3C; job.Retries+1 {\n\t\texecution.Attempt++\n\n\t\t// Keep all execution properties intact except the last output\n\t\texecution.Output = \"\"\n        ...\n\n\t\tif _, err := grpcs.agent.Run(job.Name, execution); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn \u0026#x26;proto.ExecutionDoneResponse{\n\t\t\tFrom:    grpcs.agent.config.NodeName,\n\t\t\tPayload: []byte(\"retry\"),\n\t\t}, nil\n\t}\n\n\texg, err := grpcs.agent.Store.GetExecutionGroup(execution,\n\t\t\u0026#x26;ExecutionOptions{\n\t\t\tTimezone: job.GetTimeLocation(),\n\t\t},\n\t)\n\tif err != nil {\n\t\t...\n\t\treturn nil, err\n\t}\n\n    ...\n\n\t// 执行依赖的任务\n\tif len(job.DependentJobs) \u003e 0 \u0026#x26;\u0026#x26; job.Status == StatusSuccess {\n\t\tfor _, djn := range job.DependentJobs {\n\t\t\tdj, err := grpcs.agent.Store.GetJob(djn, nil)\n\t\t\tdj.Agent = grpcs.agent\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t}\n\n\treturn \u0026#x26;proto.ExecutionDoneResponse{\n\t\tFrom:    grpcs.agent.config.NodeName,\n\t\tPayload: []byte(\"saved\"),\n\t}, nil\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e2.6. 学习笔记\u003c/h3\u003e\n\u003ch4\u003e2.6.1. Serf CLI\u003c/h4\u003e\n\u003cp\u003e\u003cimg src=\"/images/2021-09-01-10.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eSerf 官方文档的示例主要是分布式运行脚本，通过启动在多个服务器上启动 serf agent（CLI 命令）。\u003c/p\u003e\n\u003cp\u003e通过配置 Event handler 参数启动：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003e$ serf agent -event-handler=query:ssh=/bin/bash\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e发送 Query 命令就可以执行程序了：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003e$ serf query ssh uptime\nQuery 'ssh' dispatched\nAck from 'node1.pocketstudio.net'\nAck from 'node2.pocketstudio.net'\nResponse from 'node2.pocketstudio.net':  05:25:34 up 21:31,  1 user,  load average: 0.00, 0.00, 0.00\nTotal Acks: 2\nTotal Responses: 1\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e另外 Consul 的 \u003ccode\u003ego.mod\u003c/code\u003e 里也引用了 Serf 包。\u003c/p\u003e"])</script><script>self.__next_f.push([1,"4:[\"$\",\"article\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"$c\"}}]\n"])</script></body></html>