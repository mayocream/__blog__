<!doctype html><html lang=zh-hans><head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1">
<title>Dkron 源码分析 - Mayo's Blog</title>
<meta name=description content="本文于 2021.05.21 发表于敝司内部，现做部分修订与批注公开发表。 Dkron 是基于 Google 白皮书理论编写的分布式任务调度系统，内部技术利用到了 Serf（Gossip）进">
<meta name=author content>
<link rel=preconnect href=https://cdn.jsdelivr.net>
<link rel=preconnect href=https://fonts.googleapis.com>
<link rel=preconnect href=https://fonts.gstatic.com crossorigin>
<link href="https://fonts.googleapis.com/css?family=Crimson+Text:400,700|Press+Start+2P" rel=stylesheet>
<link href="https://fonts.googleapis.com/css2?family=Noto+Sans:wght@300;400&family=Noto+Sans+JP:wght@300;400&family=Noto+Sans+SC:wght@300;400&family=Noto+Serif:wght@300;400&family=Noto+Serif+JP:wght@300;400&family=Noto+Serif+SC:wght@300;400&family=Source+Code+Pro:wght@300;400" rel=stylesheet>
<link rel=stylesheet href="/bundle.css?v=1633052487" media=all>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/tocbot/dist/tocbot.css>
<link rel=icon href=https://unavatar.io/twitter/Freeze_Mayo><meta name=generator content="Hugo 0.88.1">
<link rel=canonical href=https://shoujo.ink/2021/09/dkron-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga('create','G-JKVXF5HSKT','auto'),ga('send','pageview'))</script>
<script async src=https://www.google-analytics.com/analytics.js></script>
<meta property="og:title" content="Dkron 源码分析">
<meta property="og:description" content="本文于 2021.05.21 发表于敝司内部，现做部分修订与批注公开发表。 Dkron 是基于 Google 白皮书理论编写的分布式任务调度系统，内部技术利用到了 Serf（Gossip）进">
<meta property="og:type" content="article">
<meta property="og:url" content="https://shoujo.ink/2021/09/dkron-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-09-23T02:16:00+08:00">
<meta property="article:modified_time" content="2021-09-23T02:16:00+08:00"><meta property="og:site_name" content="Mayo's Blog">
</head>
</html>
<body><script src=https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js></script>
<script>const opts={bottom:'32px',right:'unset',left:'32px',time:'0.3s',mixColor:'#fff',backgroundColor:'#fff',buttonColorDark:'#100f2c',buttonColorLight:'#fff',saveInCookies:!0,label:'🌓',autoMatchOsTheme:!0};let darkmode;window.matchMedia('only screen and (min-width: 680px)').matches?(darkmode=new Darkmode(opts),darkmode.showWidget()):(localStorage.removeItem('darkmode'),darkmode=new Darkmode(opts)),window.darkmode=darkmode,document.addEventListener('DOMContentLoaded',function(){document.body.style.opacity=1})</script>
<main class=page-content aria-label=Content>
<div class=wrapper><nav class=site-nav>
<a class=page-link href=/posts/>Posts</a>
<a class=page-link href=/about/>About</a>
</nav><header class=post-header>
<a class=site-title href=https://shoujo.ink>Mayo's Blog</a>
<h1 class="post-title baseline-fix typeface-sans" lang=zh-hans itemprop="name headline">Dkron 源码分析</h1>
</header>
<div class=post-cover aria-label=Cover>
<div class=post-cover-wrapper>
<img src alt>
</div>
<div class=cover-meta></div>
</div>
<div class=post-layout>
<article class="post typeface-sans" lang=zh-hans itemscope itemtype=http://schema.org/BlogPosting>
<div class=post-content itemprop=articleBody>
<blockquote>
<p>本文于 2021.05.21 发表于敝司内部，现做部分修订与批注公开发表。</p>
</blockquote>
<p><a href=https://github.com/distribworks/dkron>Dkron</a> 是基于 Google 白皮书理论编写的分布式任务调度系统，内部技术利用到了 <a href=https://www.serf.io/>Serf</a>（Gossip）进行病毒式消息扩散，<a href=https://github.com/hashicorp/raft>Raft</a> 进行分布式数据强一致性同步，设计上采用 Server / Agent 模式，只有一个中心调度主节点，其他节点作为工作节点，通过 <a href=https://github.com/hashicorp/go-plugin>go-plugin</a> 插件机制定义多种类型、可自由扩展的任务处理器，使用 <a href=https://grpc.io/>GRPC</a> 的双向流传输进行任务分发与状态上报，同时使用 <a href=https://pkg.go.dev/net/rpc>net/rpc</a> 进行跨进程调用，还使用高性能 KV 储存 <a href=https://github.com/tidwall/buntdb>buntdb</a> 的 in-memory 模式缓存与结构化储存任务元数据。</p>
<p>Dkron 可谓是站在了巨人的肩膀上，利用了诸多优秀开源组件，汲取了优秀设计思想实现的产品（官方还运营了商业版本）。</p>
<p>我在将其引入公司落地途中，对其进行了代码审计，本文便是落地过程中的产物之一（学习笔记）。由于 Dkron 涉及到的开源组件较多，光是 Goosip 与 Raft 协议就够复杂了，我抱着学习的心态进行工作，逐渐对这些开源组件有了认知，阅读代码也知道具体的用法是什么样子，有一定的收获。</p>
<p>本文基于 2021.05.19 <a href=https://github.com/distribworks/dkron>dkron</a> 最新 master 分支（对应 v3.1.6 版本）代码阅读， 详细源码笔记于 <a href=https://github.com/mayocream/dkron>mayocream/dkron</a> review 分支。</p>
<h2 id=1-架构设计>1. 架构设计</h2>
<h3 id=11-概述>1.1. 概述</h3>
<p>引用 Dkron 官网的表述：</p>
<blockquote>
<p>Dkron 是分布式的 Cron 服务，以 Golang 编写，并利用 Raft 协议和 Serf 提供可容错性，可靠性和可伸缩性，同时易于使用与安装。</p>
</blockquote>
<h3 id=12-设计思路>1.2. 设计思路</h3>
<p>Dkron 参考 Google 的<a href="https://queue.acm.org/detail.cfm?id=2745840">分布式 Cron 系统白皮书</a>，实现了与 Google 内部任务系统“相同”的功能。</p>
<h4 id=121-可靠性>1.2.1. 可靠性</h4>
<blockquote>
<p>Running these jobs is facilitated by keeping a file containing timestamps of the last launch for all registered Cron jobs.</p>
</blockquote>
<p>Dkron 的 Job 数据结构包含了任务最后一次执行的时间戳：</p>
<pre><code class=language-protobuf>message Job {
  ...
  // 最后一次执行成功/失败时间戳
  NullableTime last_success = 25;
  NullableTime last_error = 26;
  // 下一次执行时间戳
  google.protobuf.Timestamp next = 23;
  ...
}
</code></pre>
<blockquote>
<p>To make matters more complicated, failure to launch is acceptable for some Cron jobs but not for others. A garbage collection Cron job scheduled to run every five minutes may be able to skip one launch, but a payroll job scheduled to run once a month probably should not.</p>
</blockquote>
<p>一些任务设计上不是幂等的，简单地错误重试会导致严重的问题，任务所有者应有任务执行、重试等操作的控制权。</p>
<p>Dkron 能够让用户控制 Job 的执行、重试次数：</p>
<pre><code class=language-protobuf>message Job {
  ...
  // 启用/禁用
  bool disabled = 11;
  // 错误尝试次数
  uint32 retries = 13;
  // 是否允许同时调度
  string concurrency = 16;
  ...
}
</code></pre>
<h4 id=122-可伸缩性>1.2.2. 可伸缩性</h4>
<blockquote>
<p>If you want to run a service, simply specify which data center it should run in and what it requires — the data center scheduling system (which itself should be reliable) takes care of figuring out which machine or machines to deploy it on, as well as handling machine deaths. Launching a job in a data center then effectively turns into sending one or more RPCs to the data center scheduler.</p>
</blockquote>
<p>定时任务分布式地运行，设计上要能够避免集群中一台机器宕机影响。</p>
<p>Dkron 在任务调度和集群通信上定义了“数据中心”，所有的 Dkron 实例可以通过 Gossip 组建集群。<em>但 Dkron 目前只支持同一数据中心下进行任务调度，不支持跨数据中心调度任务。</em></p>
<p>Dkron 任务调度程序和执行节点通过 RPC 通信。</p>
<h4 id=123-状态储存>1.2.3. 状态储存</h4>
<blockquote>
<p>We have two options: store data externally in a generally available distributed storage, or store a small amount of state as part of the Cron service itself. When designing the distributed Cron, we opted for the second option.</p>
</blockquote>
<p>任务执行状态在 Dkron 被视为是与 Server 一体的，Dkron 通过 Raft FSM 在调度集群 Server 之间同步 Job 和 Job 执行历史的数据。<em>社区版本数据储存在内存 KV (Buntdb) 中，Pro 版本支持 etcd 储存。</em></p>
<p>持久化数据通过 Raft Snapshot 实现。</p>
<h4 id=124-任务调度>1.2.4. 任务调度</h4>
<h5 id=集群>集群</h5>
<p>
<figure class=image>
<img src=/2021/09/dkron-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/images/image-20210521145533658.png alt loading=lazy>
</figure></p>
<p>Dkron 中服务器节点分为 Server / Agent，Server 集群负责 Job 的数据存储，Server 中选举出的 Leader 负责任务调度。<em>Server 同时也是 Agent，可以执行 Job。</em></p>
<p>Dkron 通过 Gossip 组建集群，通过 Raft 选举 Leader，Leader 担当 Scheduler，通过 GRPC 下发 Job 到 Gossip 集群的 Peers 节点执行。当 Leader 节点宕机，会在 Server 集群中选举出新的 Leader。</p>
<h5 id=任务执行>任务执行</h5>
<p>
<figure class=image>
<img src=/2021/09/dkron-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/images/image-20210521150603300.png alt loading=lazy>
</figure></p>
<p>Dkron Job 执行中 Agent 与 Server GRPC 双向流通信，Agent 持续发送任务执行情况，Server 记录任务开始和任务结束时的执行状态，通过 Raft Apply 同步到 Server 集群。</p>
<h2 id=2-源码分析>2. 源码分析</h2>
<h3 id=21-基本概述>2.1. 基本概述</h3>
<h4 id=211-主要机制>2.1.1. 主要机制</h4>
<p>
<figure class=image>
<img src=/2021/09/dkron-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/images/image-20210521151256884.png alt loading=lazy>
</figure></p>
<p>Dkron 主要基于三大机制实现分布式任务调度：</p>
<ol>
<li>通过 Gossip 库 Serf 实现节点发现、集群组建，监听 Gossip Member 变化事件，发现 Server/Agent 的 Peer 通信地址及活跃状态；</li>
<li>通过 Raft 实现 Server 集群 Leader 选举，实现 FSM 接口，通过 Raft Log 实现 Server 集群数据同步、数据快照、数据最终一致性；</li>
<li>利用 go-plugin 实现插件机制，基于 GRPC 通信 Agent 实时返回任务执行状态。</li>
</ol>
<h4 id=212-目录结构>2.1.2. 目录结构</h4>
<pre><code class=language-bash>$ tree -A -L 1
.
# 自带的插件 /Processor 的 main 包
├── builtin
# CLI
├── cmd
# 主要功能实现
├── dkron
├── Dockerfile
# 封装 cron
├── extcron
├── go.mod
├── main.go
# 生成的 pb.go 以及通信逻辑
├── plugin
# 定义 Job 以及通信协议
├── proto
</code></pre>
<h3 id=22-组建集群>2.2. 组建集群</h3>
<h4 id=221-节点发现>2.2.1. 节点发现</h4>
<p>Dkron 使用 <a href=hashicorp/go-discover>hashicorp/go-discover</a> 进行节点 IP 的自动发现，方便通过云厂商接口、K8s 进行初始化服务发现。<em>用于发现 Peer 节点，进行 Bootstrap 或者 Join 集群。</em></p>
<pre><code class=language-go>// dkron/retry_join.go
func (r *retryJoiner) retryJoin(logger *logrus.Entry) error {
    ...
	// 使用 go-discovery 发通过不同的基础设施自动发现 IP
	// Copy the default providers, and then add the non-default
	providers := make(map[string]discover.Provider)
	for k, v := range discover.Providers {
		providers[k] = v
	}

	// 尝试 In-Cluster 方式从 &quot;/var/run/secrets/kubernetes.io/serviceaccount&quot;
	//	获取 kubeconfig
	//	ref: https://github.com/kubernetes/client-go/tree/master/examples/in-cluster-client-configuration
	//	获取 PodIP 列表
	providers[&quot;k8s&quot;] = &amp;discoverk8s.Provider{}

	disco, err := discover.New(
		discover.WithUserAgent(UserAgent()),
		discover.WithProviders(providers),
	)
	if err != nil {
		return err
	}

	attempt := 0
	for {
		var addrs []string
		var err error

		for _, addr := range r.addrs {
			switch {
				// 使用 go-discovery 发现 IP
			case strings.Contains(addr, &quot;provider=&quot;):
				servers, err := disco.Addrs(addr, log.New(logger.Logger.Writer(), &quot;&quot;, log.LstdFlags|log.Lshortfile))
				if err != nil {
                    ...
				} else {
					addrs = append(addrs, servers...)
				}

			default:
				ipAddr, err := ParseSingleIPTemplate(addr)
				if err != nil {
					logger.WithField(&quot;addr&quot;, addr).WithError(err).Error(&quot;agent: Error parsing retry-join ip template&quot;)
					continue
				}
				addrs = append(addrs, ipAddr)
			}
		}

		if len(addrs) &gt; 0 {
			// Serf 加入集群
			n, err := r.join(addrs)
			if err == nil {
				logger.Infof(&quot;agent: Join %s completed. Synced with %d initial agents&quot;, r.cluster, n)
				return nil
			}
		}

		attempt++
	}
}
</code></pre>
<p>在 K8s 部署方式中程序通过创建 go-client，请求 K8s API Server 筛选获取一批 Dkron 的 Pod IP。</p>
<h4 id=222-gossip-集群>2.2.2. Gossip 集群</h4>
<p>Dkron 使用 <a href=https://github.com/hashicorp/serf>hashicorp/serf</a> 库（Gossip 封装）的 Member 事件 Handler，监听集群成员变化事件，进行集群 Peers 的管理。</p>
<p>默认使用 <code>8946</code> 端口进行 Gossip 集群的 Peers 通信。</p>
<pre><code class=language-go>// dkron/agent.go
// setupSerf is used to create the agent we use
func (a *Agent) setupSerf() (*serf.Serf, error) {
    ...
	serfConfig := serf.DefaultConfig()
	serfConfig.Init()

	// metadata
	serfConfig.Tags = a.config.Tags
	serfConfig.Tags[&quot;role&quot;] = &quot;dkron&quot;
	// 默认 &quot;dc1&quot;
	serfConfig.Tags[&quot;dc&quot;] = a.config.Datacenter
	// 默认 &quot;global&quot;
	serfConfig.Tags[&quot;region&quot;] = a.config.Region
	serfConfig.Tags[&quot;version&quot;] = Version
	if a.config.Server {
		serfConfig.Tags[&quot;server&quot;] = strconv.FormatBool(a.config.Server)
	}
	if a.config.Bootstrap {
		serfConfig.Tags[&quot;bootstrap&quot;] = &quot;1&quot;
	}
	if a.config.BootstrapExpect != 0 {
		serfConfig.Tags[&quot;expect&quot;] = fmt.Sprintf(&quot;%d&quot;, a.config.BootstrapExpect)
	}

	// gossip 默认连接环境/连接参数
	switch config.Profile {
	case &quot;lan&quot;:
		serfConfig.MemberlistConfig = memberlist.DefaultLANConfig()
	case &quot;wan&quot;:
		serfConfig.MemberlistConfig = memberlist.DefaultWANConfig()
	case &quot;local&quot;:
		serfConfig.MemberlistConfig = memberlist.DefaultLocalConfig()
	default:
		return nil, fmt.Errorf(&quot;unknown profile: %s&quot;, config.Profile)
	}

	serfConfig.MemberlistConfig.BindAddr = bindIP
	serfConfig.MemberlistConfig.BindPort = bindPort
	serfConfig.MemberlistConfig.AdvertiseAddr = advertiseIP
	serfConfig.MemberlistConfig.AdvertisePort = advertisePort
	serfConfig.MemberlistConfig.SecretKey = encryptKey
	serfConfig.NodeName = config.NodeName
	serfConfig.Tags = config.Tags

	if err != nil {
		a.logger.Fatal(err)
	}

	// Create a channel to listen for events from Serf
	a.eventCh = make(chan serf.Event, 2048)
	serfConfig.EventCh = a.eventCh

	// Start Serf
	a.logger.Info(&quot;agent: Dkron agent starting&quot;)

	// Create serf first
	return serf.Create(serfConfig)
}
</code></pre>
<p><a href=https://github.com/hashicorp/memberlist>hashicorp/memberlist</a> 提供三个默认配置，局域网络的配置 DefaultLANConfig、外网 DefaultWANConfig、本地 DefaultLocalConfig，后两者都是在 DefaultLANConfig 基础上修改，区别是根据网速，调整了Gossip interval、TCP 超时时间等。</p>
<pre><code class=language-go>// dkron/retry_join.go
func (r *retryJoiner) retryJoin(logger *logrus.Entry) error {
	...
		if len(addrs) &gt; 0 {
			// Serf 加入集群
			n, err := r.join(addrs)
			if err == nil {
				logger.Infof(&quot;agent: Join %s completed. Synced with %d initial agents&quot;, r.cluster, n)
				return nil
			}
		}

		attempt++
		time.Sleep(r.interval)
	...
}

</code></pre>
<p>程序启动后反复尝试加入 Gossip 集群。</p>
<h4 id=223-端口复用>2.2.3. 端口复用</h4>
<p>Dkron 默认使用 <code>6868</code> 端口作为 Dkron 的 Peers RPC 通信端口。</p>
<p>默认会挂载 GRPC Server 用于服务调用。在节点为 Server 时同时开启 Raft 协议，复用端口。</p>
<pre><code class=language-go>// StartServer launch a new dkron server process
// 启动 Server
func (a *Agent) StartServer() {
    ...
	// 创建端口复用 cmux
	// Create a cmux object.
	tcpm := cmux.New(a.listener)
	var grpcl, raftl net.Listener

	// If TLS config present listen to TLS
	if a.TLSConfig != nil {
		...
        // 默认不启用 TLS
	} else {
		// 实现 raft.StreamLayer
		a.raftLayer = NewRaftLayer(a.logger)

		// cmux match grpc 通信
		grpcl = tcpm.MatchWithWriters(cmux.HTTP2MatchHeaderFieldSendSettings(&quot;content-type&quot;, &quot;application/grpc&quot;))

		// raft TCP 通信
		raftl = tcpm.Match(cmux.Any())
	}

	// 创建 GRPC Server
	if a.GRPCServer == nil {
		a.GRPCServer = NewGRPCServer(a, a.logger)
	}

	// 启动 GRPC Server
	if err := a.GRPCServer.Serve(grpcl); err != nil {
		a.logger.WithError(err).Fatal(&quot;agent: RPC server failed to start&quot;)
	}

	// 绑定 net Listener
	if err := a.raftLayer.Open(raftl); err != nil {
		a.logger.Fatal(err)
	}

	// 启动 Raft
	if err := a.setupRaft(); err != nil {
		a.logger.WithError(err).Fatal(&quot;agent: Raft layer failed to start&quot;)
	}

	// Start serving everything
	go func() {
		// 正式开始 listen
		if err := tcpm.Serve(); err != nil {
			a.logger.Fatal(err)
		}
	}()
   ...
}
</code></pre>
<p>使用 <a href=https://github.com/soheilhy/cmux>soheilhy/cmux</a> 包进行端口复用，读取字节流前 N 字节（Match 规则）匹配 HTTP/2 GRPC 请求。</p>
<h3 id=23-raft-协议>2.3. Raft 协议</h3>
<p>Dkron 使用的是 <a href=https://github.com/hashicorp/raft>hashicorp/raft</a> 实现的 Raft 协议。</p>
<h4 id=231-集群初始化>2.3.1. 集群初始化</h4>
<h5 id=创建-raft>创建 Raft</h5>
<p>Dkron 可以指定一台节点 Bootstrap，也可以设置 <code>bootstrap-expect</code> 等待集群 Peers 数量达到指定个数，再进行 Raft 集群 Bootstrap。</p>
<pre><code class=language-go>// 初始化 Raft
func (a *Agent) setupRaft() error {
	if a.config.BootstrapExpect &gt; 0 {
		if a.config.BootstrapExpect == 1 {
			// 进行 bootstrap
			a.config.Bootstrap = true
		}
	}

	// 创建 raft transport
	// （Raft 节点间的通信通道），节点之间需要通过这个通道来进行日志同步、领导者选举等等
	// 方法内部启动协程 listen listener
	transport := raft.NewNetworkTransport(a.raftLayer, 3, raftTimeout, logger)
	a.raftTransport = transport

	config := raft.DefaultConfig()

	// Raft performance
	// 默认值为 1
	raftMultiplier := a.config.RaftMultiplier
	if raftMultiplier &lt; 1 || raftMultiplier &gt; 10 {
		return fmt.Errorf(&quot;raft-multiplier cannot be %d. Must be between 1 and 10&quot;, raftMultiplier)
	}
	config.HeartbeatTimeout = config.HeartbeatTimeout * time.Duration(raftMultiplier)
	config.ElectionTimeout = config.ElectionTimeout * time.Duration(raftMultiplier)
	config.LeaderLeaseTimeout = config.LeaderLeaseTimeout * time.Duration(a.config.RaftMultiplier)

	config.LogOutput = logger
	config.LocalID = raft.ServerID(a.config.NodeName)

	// Build an all in-memory setup for dev mode, otherwise prepare a full
	// disk-based setup.
	var logStore raft.LogStore
	var stableStore raft.StableStore
	var snapshots raft.SnapshotStore
	if a.config.DevMode {
		...
	} else {
		var err error

		// （快照存储，用来存储节点的快照信息），也就是压缩后的日志数据
		snapshots, err = raft.NewFileSnapshotStore(filepath.Join(a.config.DataDir, &quot;raft&quot;), 3, logger)

		// Create the BoltDB backend
		s, err := raftboltdb.NewBoltStore(filepath.Join(a.config.DataDir, &quot;raft&quot;, &quot;raft.db&quot;))

		a.raftStore = s
		// （稳定存储，用来存储 Raft 集群的节点信息等），比如，当前任期编号、最新投票时的任期编号等
		stableStore = s

		// 512 size 内存缓存
		cacheStore, err := raft.NewLogCache(raftLogCacheSize, s)
		if err != nil {
			s.Close()
			return err
		}
		// 用来存储 Raft 的日志
		logStore = cacheStore
        ...
	}

    ...
	//	参数: 实现了 Storage 的 DB, 默认 nil, logger 
	// 创建实现 raft FSM 接口
	// raft 只是定义了一个接口，最终交给应用层实现。应用层收到 Log 后按 业务需求 还原为 应用数据保存起来
	//	Raft 启动时 便 Raft.runFSM 起一个goroutine 从 fsmMutateCh channel 消费log ==&gt; FSM.Apply
	fsm := newFSM(a.Store, a.ProAppliers, a.logger)
	// 创建 raft 节点
	rft, err := raft.NewRaft(config, fsm, logStore, stableStore, snapshots, transport)
	if err != nil {
		return fmt.Errorf(&quot;new raft: %s&quot;, err)
	}
	// 选举 leader 信号 chan
	a.leaderCh = rft.LeaderCh()
	a.raft = rft

	return nil
}
</code></pre>
<p>默认使用 <code>/dkron/raft</code> 目录储存 Raft Log 和快照。使用 <a href=https://github.com/hashicorp/raft-boltdb>hashicorp/raft-boltdb</a> 作为 Raft Log 和 stable 储存，快照使用文件储存。</p>
<blockquote>
<p>线上使用情况：Server 运行 7 天每 30s 执行 5 个 Job ，<code>raft.db</code> 大小为 25M，内存占用在 200M 左右。</p>
</blockquote>
<p>数据量不大应该不用考虑大量磁盘和内存占用。</p>
<h5 id=创建集群>创建集群</h5>
<p>如果是初始化集群，指定 <code>bootstrap-expect</code> 参数，则会监听 Gossip 集群成员变化，等待 Peers 数量达到预期时，进行 Raft 的 Bootstrap 操作。</p>
<p>如果已存在 Raft 集群，则新的 Server 加入 Raft 集群的成员中。</p>
<pre><code class=language-go>// dkron/agent.go
// 处理 Serf 事件
func (a *Agent) eventLoop() {
    // 只读的 Shutdown channel
	serfShutdownCh := a.serf.ShutdownCh()
	for {
		select {
			// Serf event 事件处理
		case e := &lt;-a.eventCh:
			// 有三种事件 MemberEvent/UserEvent/Query
			// 这里只处理 MemberEvent, 只使用 Serf 做集群 member 的管理
			// 实际运行时主要的 MemberEvent 事件有 update/failed/join
			// 没有使用到自定义 event 以及 query 命令

			// Log all member events
			if me, ok := e.(serf.MemberEvent); ok {
				for _, member := range me.Members {
					a.logger.WithFields(logrus.Fields{
						&quot;node&quot;:   a.config.NodeName,
						&quot;member&quot;: member.Name,
						&quot;event&quot;:  e.EventType(),
					}).Debug(&quot;agent: Member event&quot;)
				}

				// serfEventHandler is used to handle events from the serf cluster
				switch e.EventType() {
				case serf.EventMemberJoin:
					a.nodeJoin(me)
					a.localMemberEvent(me)
				case serf.EventMemberLeave, serf.EventMemberFailed:
					a.nodeFailed(me)
					a.localMemberEvent(me)
				case serf.EventMemberReap:
					a.localMemberEvent(me)
				case serf.EventMemberUpdate, serf.EventUser, serf.EventQuery: // Ignore
				default:
					a.logger.WithField(&quot;event&quot;, e.String()).Warn(&quot;agent: Unhandled serf event&quot;)
				}
			}

		case &lt;-serfShutdownCh:
			a.logger.Warn(&quot;agent: Serf shutdown detected, quitting&quot;)
			return
		}
	}
}
</code></pre>
<p>Serf 的事件有三种类型：MemberEvent / UserEvent / Query，Dkron 只处理 MemberEvent，只使用 Serf 做集群 Peers 的管理，没有使用到自定义 event 以及 query 命令。实际运行时主要的 MemberEvent 事件有 <code>member-update</code> / <code>member-failed</code> / <code>member-join</code> / <code>member-reap</code>。</p>
<pre><code class=language-go>// dkron/serf.go
// maybeBootstrap is used to handle bootstrapping when a new server joins
func (a *Agent) maybeBootstrap() {
	...
	// 存在 raft commit 日志, 不需要 bootstrap
	if index != 0 {
		a.config.BootstrapExpect = 0
		return
	}

	// Scan for all the known servers
	members := a.serf.Members()
	var servers []ServerParts
	voters := 0
	for _, member := range members {
		valid, p := isServer(member)
		if !valid {
			// 跳过非 dkron server 的 member
			continue
		}
		if p.Region != a.config.Region {
			continue
		}
		...
		if valid {
			voters++
		}
		servers = append(servers, *p)
	}

	// Skip if we haven't met the minimum expect count
	if voters &lt; a.config.BootstrapExpect {
		return
	}

    ...
	for _, server := range servers {
		addr := server.Addr.String()
		addrs = append(addrs, addr)
		id := raft.ServerID(server.ID)
		suffrage := raft.Voter // 允许仲裁的角色
		peer := raft.Server{
			ID:       id,
			Address:  raft.ServerAddress(addr),
			Suffrage: suffrage,
		}
		configuration.Servers = append(configuration.Servers, peer)
	}

	// raft 集群初始化
	future := a.raft.BootstrapCluster(configuration)
	if err := future.Error(); err != nil {
		a.logger.WithError(err).Error(&quot;agent: failed to bootstrap cluster&quot;)
	}

	// Bootstrapping complete, or failed for some reason, don't enter this again
	a.config.BootstrapExpect = 0
}
</code></pre>
<p>Raft 集群初始化的条件：</p>
<ol>
<li>没有 Commit 日志；</li>
<li>Server 集群达到 expect 的数量；</li>
<li>属于同一 region</li>
</ol>
<h5 id=加入节点>加入节点</h5>
<p>若 Raft 集群已存在，则 Leader 节点接收到 Serf Member 加入 Gossip 集群的事件时，进行 Raft 节点 Join 的逻辑：</p>
<pre><code class=language-go>// dkron/serf.go
func (a *Agent) localMemberEvent(me serf.MemberEvent) {
	// 只有 Leader 进行操作
	if !a.config.Server || !a.IsLeader() {
		return
	}

	for _, m := range me.Members {
		select {
            // 加入重新同步的 chan
		case a.reconcileCh &lt;- m:
		default:
		}
	}
}

// reconcileCh 触发 reconcile 执行
func (a *Agent) reconcile() error {
	members := a.serf.Members()
	for _, member := range members {
		if err := a.reconcileMember(member); err != nil {
			return err
		}
	}
	return nil
}
</code></pre>
<pre><code class=language-go>// dkron/leader.go
func (a *Agent) reconcileMember(member serf.Member) error {
	// 判断是否在一个 region, 是否是 Server
	valid, parts := isServer(member)
	if !valid || parts.Region != a.config.Region {
		return nil
	}

	var err error
	switch member.Status {
	case serf.StatusAlive:
		err = a.addRaftPeer(member, parts)
	case serf.StatusLeft:
		err = a.removeRaftPeer(member, parts)
	}
	if err != nil {
		return err
	}
	return nil
}
</code></pre>
<pre><code class=language-go>// dkron/leader.go
func (a *Agent) addRaftPeer(m serf.Member, parts *ServerParts) error {
   ...
	// 获取 raft peers
	for _, server := range configFuture.Configuration().Servers {

		// 要加入的 server 已存在于 raft peers 中, 先移除再添加
		if server.Address == raft.ServerAddress(addr) || server.ID == raft.ServerID(parts.ID) {
            ...
			future := a.raft.RemoveServer(server.ID, 0, 0)
			...
		}
	}

	// 添加一个 raft peer
	switch {
	case minRaftProtocol &gt;= 3:
		addFuture := a.raft.AddVoter(raft.ServerID(parts.ID), raft.ServerAddress(addr), 0, 0)
		if err := addFuture.Error(); err != nil {
			a.logger.WithError(err).Error(&quot;dkron: failed to add raft peer&quot;)
			return err
		}
	}

	return nil
}
</code></pre>
<h4 id=232-fsm>2.3.2. FSM</h4>
<h5 id=内存-db>内存 DB</h5>
<p>Dkron 使用内存 KV <a href=https://github.com/tidwall/buntdb>tidwall/buntdb</a> 储存 Job 及执行历史记录。</p>
<pre><code class=language-go>// dkron/storage.go
// Storage is the interface that should be used by any
// storage engine implemented for dkron. It contains the
// minumum set of operations that are needed to have a working
// dkron store.
type Storage interface {
	SetJob(job *Job, copyDependentJobs bool) error
	DeleteJob(name string) (*Job, error)
	SetExecution(execution *Execution) (string, error)
	SetExecutionDone(execution *Execution) (bool, error)
	GetJobs(options *JobOptions) ([]*Job, error)
	GetJob(name string, options *JobOptions) (*Job, error)
	GetExecutions(jobName string, opts *ExecutionOptions) ([]*Execution, error)
	GetExecutionGroup(execution *Execution, opts *ExecutionOptions) ([]*Execution, error)
	GetGroupedExecutions(jobName string, opts *ExecutionOptions) (map[int64][]*Execution, []int64, error)
	Shutdown() error
	Snapshot(w io.WriteCloser) error
	Restore(r io.ReadCloser) error
}
</code></pre>
<p><code>store</code> 实现了 Storage 接口，使用的是 buntdb：</p>
<pre><code class=language-go>// dkron/store.go
// NewStore creates a new Storage instance.
// 创建基于内存的 buntdb
func NewStore(logger *logrus.Entry) (*Store, error) {
	db, err := buntdb.Open(&quot;:memory:&quot;)
	db.CreateIndex(&quot;name&quot;, jobsPrefix+&quot;:*&quot;, buntdb.IndexJSON(&quot;name&quot;))
	db.CreateIndex(&quot;started_at&quot;, executionsPrefix+&quot;:*&quot;, buntdb.IndexJSON(&quot;started_at&quot;))
	db.CreateIndex(&quot;finished_at&quot;, executionsPrefix+&quot;:*&quot;, buntdb.IndexJSON(&quot;finished_at&quot;))
	db.CreateIndex(&quot;attempt&quot;, executionsPrefix+&quot;:*&quot;, buntdb.IndexJSON(&quot;attempt&quot;))
	db.CreateIndex(&quot;displayname&quot;, jobsPrefix+&quot;:*&quot;, buntdb.IndexJSON(&quot;displayname&quot;))
	db.CreateIndex(&quot;schedule&quot;, jobsPrefix+&quot;:*&quot;, buntdb.IndexJSON(&quot;schedule&quot;))
	db.CreateIndex(&quot;success_count&quot;, jobsPrefix+&quot;:*&quot;, buntdb.IndexJSON(&quot;success_count&quot;))
	db.CreateIndex(&quot;error_count&quot;, jobsPrefix+&quot;:*&quot;, buntdb.IndexJSON(&quot;error_count&quot;))
	db.CreateIndex(&quot;last_success&quot;, jobsPrefix+&quot;:*&quot;, buntdb.IndexJSON(&quot;last_success&quot;))
	db.CreateIndex(&quot;last_error&quot;, jobsPrefix+&quot;:*&quot;, buntdb.IndexJSON(&quot;last_error&quot;))
	db.CreateIndex(&quot;next&quot;, jobsPrefix+&quot;:*&quot;, buntdb.IndexJSON(&quot;next&quot;))
	if err != nil {
		return nil, err
	}

	store := &amp;Store{
		db:     db,
		lock:   &amp;sync.Mutex{},
		logger: logger,
	}

	return store, nil
}
</code></pre>
<p>官方的 Pro 版本实现的 etcd 储存，可能是实现了 Storage 的另一个实例，或者是在 Raft FSM 进行了额外的处理执行外部数据库同步。</p>
<h5 id=实现-fsm-接口>实现 FSM 接口</h5>
<p>Raft FSM 接口：</p>
<pre><code class=language-go>// FSM provides an interface that can be implemented by
// clients to make use of the replicated log.
type FSM interface {
	// Apply log is invoked once a log entry is committed.
	// It returns a value which will be made available in the
	// ApplyFuture returned by Raft.Apply method if that
	// method was called on the same Raft node as the FSM.
	Apply(*Log) interface{}

	// Snapshot is used to support log compaction. This call should
	// return an FSMSnapshot which can be used to save a point-in-time
	// snapshot of the FSM. Apply and Snapshot are not called in multiple
	// threads, but Apply will be called concurrently with Persist. This means
	// the FSM should be implemented in a fashion that allows for concurrent
	// updates while a snapshot is happening.
	Snapshot() (FSMSnapshot, error)

	// Restore is used to restore an FSM from a snapshot. It is not called
	// concurrently with any other command. The FSM must discard all previous
	// state.
	Restore(io.ReadCloser) error
}
</code></pre>
<p><code>Apply</code> 函数会在从 Log 载入一条数据的时候被调用。<em>Leader 会调用 Apply 进行数据同步。</em></p>
<pre><code class=language-go>// dkron/fsm.go
type dkronFSM struct {
	store Storage

	// proAppliers holds the set of pro only LogAppliers
	proAppliers LogAppliers
	logger      *logrus.Entry
}

// 创建/同步一条 Log 到 state
func (d *dkronFSM) Apply(l *raft.Log) interface{} {
	buf := l.Data
	msgType := MessageType(buf[0])

	switch msgType {
	case SetJobType:
		return d.applySetJob(buf[1:])
	case DeleteJobType:
		return d.applyDeleteJob(buf[1:])
	case ExecutionDoneType:
		return d.applyExecutionDone(buf[1:])
	case SetExecutionType:
		return d.applySetExecution(buf[1:])
	}

	// Dkron Pro 版本额外的操作, 可能在此处执行外部数据库同步
	if applier, ok := d.proAppliers[msgType]; ok {
		return applier(buf[1:], l.Index)
	}

	return nil
}
</code></pre>
<p>实现的快照和恢复方法，使用 buntdb 的 load/save：</p>
<pre><code class=language-go>// dkron/fsm.go
// FSM 的快照和恢复方法，使用 buntdb 的 load/save 方法。

// Snapshot returns a snapshot of the key-value store. We wrap
// the things we need in dkronSnapshot and then send that over to Persist.
// Persist encodes the needed data from dkronSnapshot and transport it to
// Restore where the necessary data is replicated into the finite state machine.
// This allows the consensus algorithm to truncate the replicated log.
func (d *dkronFSM) Snapshot() (raft.FSMSnapshot, error) {
	return &amp;dkronSnapshot{store: d.store}, nil
}

// Restore stores the key-value store to a previous state.
func (d *dkronFSM) Restore(r io.ReadCloser) error {
	defer r.Close()
	return d.store.Restore(r)
}

type dkronSnapshot struct {
	store Storage
}

func (d *dkronSnapshot) Persist(sink raft.SnapshotSink) error {
	if err := d.store.Snapshot(sink); err != nil {
		sink.Cancel()
		return err
	}

	// Close the sink.
	if err := sink.Close(); err != nil {
		return err
	}

	return nil
}

func (d *dkronSnapshot) Release() {}
</code></pre>
<h5 id=数据格式>数据格式</h5>
<p>通过 Raft Log (Bytes) 储存的数据格式为：</p>
<pre><code class=language-go>// dkron/grpc.go
// Proto 转换成 Bytes
func Encode(t MessageType, msg interface{}) ([]byte, error) {
	var buf bytes.Buffer
	buf.WriteByte(uint8(t))
	m, err := pb.Marshal(msg.(pb.Message))
	if err != nil {
		return nil, err
	}
	_, err = buf.Write(m)
	return buf.Bytes(), err
}
</code></pre>
<p>首个字节标记 Message 的类型，后续为 Protobuf 序列化后的结果。</p>
<h3 id=24-插件机制>2.4. 插件机制</h3>
<h4 id=241-概述>2.4.1. 概述</h4>
<p>插件机制优先于任务调度本身进行分析，因为任务调度依赖于插件的通信机制。</p>
<p>Dkron 插件使用 <a href=https://github.com/hashicorp/go-plugin>hashicorp/go-plugin</a> 实现，简而概之，通过两种 RPC 方式（netrpc / GRPC）客户端与插件监听的端口进行通信，插件与主程序分离，作为插件机制。</p>
<p>引用 go-plugin 项目的说明：</p>
<blockquote>
<p>Shared libraries have one major advantage over our system which is much higher performance. In real world scenarios across our various tools, we&rsquo;ve never required any more performance out of our plugin system and it has seen very high throughput, so this isn&rsquo;t a concern for us at the moment.</p>
</blockquote>
<p>Golang 出的基于链接库（.so）的插件机制还不成熟，并且 go-plugin 已经在大量软件中运用多年，对比起来前者没有优势，go-plugin 的考量在于性能对于插件机制来说不是首要的。</p>
<p>这里说一句，其他 Golang 的 HTTP 服务器例如 Traefik、Caddy 的插件机制都是需要修改源代码，自行增加包实现 Golang Interface，重新进行编译实现的，这种侵入性更大，但是性能更好。</p>
<p>Golang 官方的插件机制到目前还不支持 Windows，高性能的 WASM Golang 运行时库（用作 WASM 插件机制） <a href=https://github.com/wasmerio/wasmer-go>wasmerio/wasmer-go</a> 目前也不支持 Windows。</p>
<h4 id=242-加载插件>2.4.2. 加载插件</h4>
<p>Dkron 默认会在一些目录查找插件的二进制文件：</p>
<pre><code class=language-go>// cmd/plugins.go
func (p *Plugins) DiscoverPlugins() error {
	p.Processors = make(map[string]dkplugin.Processor)
	p.Executors = make(map[string]dkplugin.Executor)

	// Look in /etc/dkron/plugins
	// 匹配目录下的文件列表
	processors, err := plugin.Discover(&quot;dkron-processor-*&quot;, filepath.Join(&quot;/etc&quot;, &quot;dkron&quot;, &quot;plugins&quot;))
	if err != nil {
		return err
	}

	// Look in /etc/dkron/plugins
	executors, err := plugin.Discover(&quot;dkron-executor-*&quot;, filepath.Join(&quot;/etc&quot;, &quot;dkron&quot;, &quot;plugins&quot;))
	if err != nil {
		return err
	}

	exePath, err := osext.Executable()
	if err != nil {
		logrus.WithError(err).Error(&quot;Error loading exe directory&quot;)
	} else {
		// 从当前 agent 执行目录查找，同目录的可执行文件
		// 默认执行目录为 /opt/local/dkron/
		// 若我们需要添加自定义插件, 将编译后的二进制文件放入同目录即可
		p, err := plugin.Discover(&quot;dkron-processor-*&quot;, filepath.Dir(exePath))
		if err != nil {
			return err
		}
		processors = append(processors, p...)
		e, err := plugin.Discover(&quot;dkron-executor-*&quot;, filepath.Dir(exePath))
		if err != nil {
			return err
		}
		executors = append(executors, e...)
	}

	// pluginFactory
	// 创建运行时
	for _, file := range processors {
		// 文件名按 &quot;-&quot; 分隔，取最后一个元素
		// 问题点: 插件名不支持带 &quot;-&quot;
		pluginName, ok := getPluginName(file)
		if !ok {
			continue
		}

		raw, err := p.pluginFactory(file, dkplugin.ProcessorPluginName)
		if err != nil {
			return err
		}
		p.Processors[pluginName] = raw.(dkplugin.Processor)
	}

	...

	return nil
}
</code></pre>
<h4 id=243-插件实现>2.4.3. 插件实现</h4>
<p>Dkron 封装的 Plugin 结构, 规定可以有 processor 或 executor 两个插件。</p>
<pre><code class=language-go>// PluginMap should be used by clients for the map of plugins.
// Dkron 封装的 Plugin 结构, 规定可以有 processor 或 executor 两个插件
var PluginMap = map[string]plugin.Plugin{
	&quot;processor&quot;: &amp;ProcessorPlugin{},
	&quot;executor&quot;:  &amp;ExecutorPlugin{},
}
</code></pre>
<h5 id=构建插件>构建插件</h5>
<pre><code class=language-go>// cmd/plugins.go
// 创建 Plugin Client
func (p *Plugins) pluginFactory(path string, pluginType string) (interface{}, error) {
	var config plugin.ClientConfig
	// 可执行文件
	config.Cmd = exec.Command(path)
    
	// handshake 配置是 client 与 server 约定的 TOKEN,
	//	不通过环境变量包含同样的 TOKEN 则无法启动 plugin 程序.
	//	具体查看 go-plugin 项目 https://github.com/mayocream/go-plugin/blob/044aadd925bf9f027cb301b2af9bc6b60775dd22/server.go#L248
	config.HandshakeConfig = dkplugin.Handshake
    
	// go-plugin 包会在 NewClient 的时候储存 Client 的指针,
	//	能够调用 cleanClients 统一 Kill 所有的 Client
	config.Managed = true
    
	// 定义一个二进制所能包含的不同插件
	config.Plugins = dkplugin.PluginMap
	config.SyncStdout = os.Stdout
	config.SyncStderr = os.Stderr

	switch pluginType {
	case dkplugin.ProcessorPluginName:
		// processor 使用 golang net/rpc 进行通信
		config.AllowedProtocols = []plugin.Protocol{plugin.ProtocolNetRPC}
	case dkplugin.ExecutorPluginName:
		// executor 使用 gprc 进行通信
		config.AllowedProtocols = []plugin.Protocol{plugin.ProtocolGRPC}
	}

	// 初始化客户端
	client := plugin.NewClient(&amp;config)

	// go-plugin Client() 会用 exec.Start 启动 plugin server,
	//	创建 rpc client, 这个库将连接复用, 以及 rpc/grpc service 挂载
	//	等细节屏蔽了, 开发者只要创建业务逻辑 Interface 并实现 plugin.Plguin 的
	//	Serve/Client 方法就能够进行 rpc 通信
	rpcClient, err := client.Client()
	if err != nil {
		return nil, err
	}

	// 调用指定的 service
	raw, err := rpcClient.Dispense(pluginType)
	if err != nil {
		return nil, err
	}

	return raw, nil
}
</code></pre>
<h5 id=运行插件>运行插件</h5>
<pre><code class=language-go>// go-plugin 插件 start 函数
func (c *Client) Start() (addr net.Addr, err error) {
   ...
    // 启动的环境变量
	env := []string{
		fmt.Sprintf(&quot;%s=%s&quot;, c.config.MagicCookieKey, c.config.MagicCookieValue),
		fmt.Sprintf(&quot;PLUGIN_MIN_PORT=%d&quot;, c.config.MinPort),
		fmt.Sprintf(&quot;PLUGIN_MAX_PORT=%d&quot;, c.config.MaxPort),
		fmt.Sprintf(&quot;PLUGIN_PROTOCOL_VERSIONS=%s&quot;, strings.Join(versionStrings, &quot;,&quot;)),
	}

	cmd := c.config.Cmd
	cmd.Env = append(cmd.Env, os.Environ()...)
	cmd.Env = append(cmd.Env, env...)
	cmd.Stdin = os.Stdin

    // pipe 读取 std 输出
	cmdStdout, err := cmd.StdoutPipe()
	cmdStderr, err := cmd.StderrPipe()

	// 启动二进制程序
	err = cmd.Start()
	if err != nil {
		return
	}

    ...
    // 读取插件执行的 Output
	linesCh := make(chan string)
	c.clientWaitGroup.Add(1)
	go func() {
		defer c.clientWaitGroup.Done()
		defer close(linesCh)

		scanner := bufio.NewScanner(cmdStdout)
		for scanner.Scan() {
			linesCh &lt;- scanner.Text()
		}
	}()

	...
	return
}
</code></pre>
<h4 id=244-插件通信>2.4.4. 插件通信</h4>
<h5 id=插件通信数据结构>插件通信数据结构</h5>
<pre><code class=language-protobuf>message ExecuteRequest {
  string job_name = 1;
  map&lt;string, string&gt; config = 2;
  uint32 status_server = 3;
}

message ExecuteResponse {
    bytes output = 1;
    string error = 2;
}

service Executor {
    rpc Execute (ExecuteRequest) returns (ExecuteResponse);
}

message StatusUpdateRequest {
  bytes output = 2;
  bool error = 3;
}

message StatusUpdateResponse {
  int64 r = 1;
}

service StatusHelper {
  rpc Update(StatusUpdateRequest) returns (StatusUpdateResponse);
}
</code></pre>
<h5 id=grpc-双向流通信>GRPC 双向流通信</h5>
<pre><code class=language-go>// Here is the gRPC client that GRPCClient talks to.
type ExecutorClient struct {
	// This is the real implementation
	client types.ExecutorClient
	broker *plugin.GRPCBroker
}

// ref: https://github.com/distribworks/dkron/pull/719
// 实现执行时实时传输 output, 双向流
func (m *ExecutorClient) Execute(args *types.ExecuteRequest, cb StatusHelper) (*types.ExecuteResponse, error) {
	// This is where the magic conversion to Proto happens
	statusHelperServer := &amp;GRPCStatusHelperServer{Impl: cb}

	var s *grpc.Server
	serverFunc := func(opts []grpc.ServerOption) *grpc.Server {
		s = grpc.NewServer(opts...)
		types.RegisterStatusHelperServer(s, statusHelperServer)

		return s
	}

	brokerID := m.broker.NextId()
	go m.broker.AcceptAndServe(brokerID, serverFunc)

	args.StatusServer = brokerID
	r, err := m.client.Execute(context.Background(), args)

	s.Stop()
	return r, err
}
</code></pre>
<p>目前线上出现过 <code>rpc: transport is closing</code> 的错误，是 GRPC 通信的错误，推测在插件通信的该部分出现错误。</p>
<h3 id=25-任务调度>2.5. 任务调度</h3>
<h4 id=251-概述>2.5.1. 概述</h4>
<p>Dkron Server 中为 Raft Leader 的服务器成为调度服务器，负责定时任务的分发。</p>
<ol>
<li>通过 GRPC 调用下发任务到 agent 节点，Agent 执行任务并通过 GRPC 流实时返回输出；</li>
<li>执行完 Job 后 Leader 通过 Raft Apply 储存 Job 执行记录到各 Server 节点。</li>
</ol>
<h4 id=252-cron-封装>2.5.2. cron 封装</h4>
<p>Dkron 基于 <a href=https://github.com/robfig/cron>robfig/cron</a> 增加了 <code>@at</code> 时间定义，允许指定只允许一次的定时任务。</p>
<pre><code class=language-go>// extcron/extparser.go
// NewParser creates an ExtParser instance
// 启用 second 字段
func NewParser() cron.ScheduleParser {
	return ExtParser{cron.NewParser(cron.Second | cron.Minute | cron.Hour | cron.Dom | cron.Month | cron.Dow | cron.Descriptor)}
}

// Parse parses a cron schedule specification. It accepts the cron spec with
// mandatory seconds parameter, descriptors and the custom descriptors
// &quot;@at &lt;date&gt;&quot; and &quot;@manually&quot;.
// 添加了自定义解析的部分
func (p ExtParser) Parse(spec string) (cron.Schedule, error) {
	if spec == &quot;@manually&quot; {
		return At(time.Time{}), nil
	}

	const at = &quot;@at &quot;
	if strings.HasPrefix(spec, at) {
		date, err := time.Parse(time.RFC3339, spec[len(at):])
		if err != nil {
			return nil, fmt.Errorf(&quot;failed to parse date %s: %s&quot;, spec, err)
		}
		return At(date), nil
	}

	// It's not a dkron specific spec: Let the regular cron schedule parser have it
	return p.parser.Parse(spec)
}
</code></pre>
<h4 id=253-数据一致性>2.5.3. 数据一致性</h4>
<pre><code class=language-go>// dkron/leader.go
// 处理变成 leader 事件
func (a *Agent) leaderLoop(stopCh chan struct{}) {
RECONCILE:
   ...
	// Apply a raft barrier to ensure our FSM is caught up
	start := time.Now()
	barrier := a.raft.Barrier(barrierWriteTimeout)
	if err := barrier.Error(); err != nil {
		a.logger.WithError(err).Error(&quot;dkron: failed to wait for barrier&quot;)
		goto WAIT
	}
   ...
}
</code></pre>
<p>变成 Leader 后调用 <code>raft.Barrier</code> 确保 FSM 同步到最新状态。</p>
<h4 id=254-任务下发>2.5.4. 任务下发</h4>
<h5 id=启动任务调度>启动任务调度</h5>
<pre><code class=language-go>// dkron/scheduler.go
// 启动调度器
func (s *Scheduler) Start(jobs []*Job, agent *Agent) error {
	s.Cron = cron.New(cron.WithParser(extcron.NewParser()))

	for _, job := range jobs {
		job.Agent = agent
		// 添加所有 Job
		s.AddJob(job)
	}
	// 开始定时执行
	s.Cron.Start()
	s.Started = true
	schedulerStarted.Set(1)

	return nil
}
</code></pre>
<p>添加单个 Job 到 cron 定时触发：</p>
<pre><code class=language-go>// dkron/scheduler.go
// AddJob Adds a job to the cron scheduler
// 调度器添加 Job
func (s *Scheduler) AddJob(job *Job) error {
	// Check if the job is already set and remove it if exists
	if _, ok := s.EntryJobMap.Load(job.Name); ok {
		s.RemoveJob(job)
	}

	if job.Disabled || job.ParentJob != &quot;&quot; {
		return nil
	}
    ...

	// 为 cron 添加一个 job
	// Job 的 Run 是 cron 触发的执行方法
	id, err := s.Cron.AddJob(schedule, job)
	if err != nil {
		return err
	}
	// 储存 cron 的 id
	s.EntryJobMap.Store(job.Name, id)
    ...
	return nil
}
</code></pre>
<h5 id=触发任务调度>触发任务调度</h5>
<pre><code class=language-go>// dkron/job.go
// job 的 run 方法实现 cron.Job 接口
func (j *Job) Run() {
	// Check if it's runnable
	if j.isRunnable(j.logger) {
		...
		cronInspect.Set(j.Name, j)

		// Simple execution wrapper
		ex := NewExecution(j.Name)

		// 触发调度运行 Job
		if _, err := j.Agent.Run(j.Name, ex); err != nil {
			j.logger.WithError(err).Error(&quot;job: Error running job&quot;)
		}
	}
}
</code></pre>
<p><code>isRunnable</code> 检查任务是否被禁止，同时通过 GRPC 查询所有 Agent 当前正在执行的任务，是否有相同的 JobName，如果“不允许并发调度”则停止本次调度。</p>
<h5 id=任务分发到-agent>任务分发到 Agent</h5>
<p>注意：Dkron 调度会将任务调度到所有符合 tags 的、同一 region 的节点（Serf Members）上。</p>
<pre><code class=language-go>// dkron/run.go
// 调度运行 Job -&gt; 分发到 agent 执行任务
func (a *Agent) Run(jobName string, ex *Execution) (*Job, error) {
	job, err := a.Store.GetJob(jobName, nil)
    ...
	// In case the job is not a child job, compute the next execution time
	if job.ParentJob == &quot;&quot; {
		// 获取 cron.Entry
		if e, ok := a.sched.GetEntry(jobName); ok {
			// 获取下一次执行时间
			job.Next = e.Next
			// 同步 job 数据
			if err := a.applySetJob(job.ToProto()); err != nil {
				return nil, fmt.Errorf(&quot;agent: Run error storing job %s before running: %w&quot;, jobName, err)
			}
		} else {
			return nil, fmt.Errorf(&quot;agent: Run error retrieving job: %s from scheduler&quot;, jobName)
		}
	}

	// In the first execution attempt we build and filter the target nodes
	// but we use the existing node target in case of retry.
	var filterMap map[string]string
	if ex.Attempt &lt;= 1 {
		filterMap, _, err = a.processFilteredNodes(job)
		if err != nil {
			return nil, fmt.Errorf(&quot;run error processing filtered nodes: %w&quot;, err)
		}
	} else {
		// In case of retrying, find the rpc address of the node or return with an error
		// 重试使用同样的 Node 执行
		var addr string
		for _, m := range a.serf.Members() {
			if ex.NodeName == m.Name {
				if m.Status == serf.StatusAlive {
					addr = m.Tags[&quot;rpc_addr&quot;]
				} else {
					return nil, fmt.Errorf(&quot;retry node is gone: %s for job %s&quot;, ex.NodeName, ex.JobName)
				}
			}
		}
		filterMap = map[string]string{ex.NodeName: addr}
	}

   ...
	var wg sync.WaitGroup
	for _, v := range filterMap {
		// Call here client GRPC AgentRun
		wg.Add(1)
		go func(node string, wg *sync.WaitGroup) {
			defer wg.Done()
              
            // 这里真正调用 GRPC 到 agent 执行
			err := a.GRPCClient.AgentRun(node, job.ToProto(), ex.ToProto())
			if err != nil {
				...
			}
		}(v, &amp;wg)
	}

	// 等待所有节点执行完
	wg.Wait()
	return job, nil
}

</code></pre>
<p>Server 对执行状态的处理：</p>
<ol>
<li>GRPC 通信开始，Agent 接收到任务；</li>
<li>GRPC 结束，Agent 执行完成（成功或失败）任务；</li>
</ol>
<pre><code class=language-go>// dkron/grpc_client.go
// Dkron server 调用此方法通过 GRPC 下发 Job 到 server/agent 执行
func (grpcc *GRPCClient) AgentRun(addr string, job *proto.Job, execution *proto.Execution) error {
	var conn *grpc.ClientConn

	// (MAYO): remove string type wrap
	conn, err := grpcc.Connect(addr)
	if err != nil {
		return err
	}
	defer conn.Close()

	// Streaming call
	a := proto.NewAgentClient(conn)
	stream, err := a.AgentRun(context.Background(), &amp;proto.AgentRunRequest{
		Job:       job,
		Execution: execution,
	})
	if err != nil {
		return err
	}

	var first bool
	for {
		// 读取 GRPC 流
		ars, err := stream.Recv()

		// Stream ends
		if err == io.EOF {
			// 任务执行结束, 发送 done 命令给 leader 持久化储存
			addr := grpcc.agent.raft.Leader()
			if err := grpcc.ExecutionDone(string(addr), NewExecutionFromProto(execution)); err != nil {
				return err
			}
			return nil
		}

		// Error received from the stream
		if err != nil {
			// At this point the execution status will be unknown, set the FinshedAt time and an explanatory message
			execution.FinishedAt = ptypes.TimestampNow()
			execution.Output = []byte(err.Error())

			grpcc.logger.WithError(err).Error(ErrBrokenStream)

			addr := grpcc.agent.raft.Leader()
			if err := grpcc.ExecutionDone(string(addr), NewExecutionFromProto(execution)); err != nil {
				return err
			}
			return err
		}

		// Registers an active stream
		grpcc.agent.activeExecutions.Store(ars.Execution.Key(), ars.Execution)
		execution = ars.Execution
		defer grpcc.agent.activeExecutions.Delete(execution.Key())

		// Store the received execution in the raft log and store
		if !first {
			// 储存执行状态
			if err := grpcc.SetExecution(ars.Execution); err != nil {
				return err
			}
			first = true
		}
	}
}

</code></pre>
<h5 id=agent-任务执行>Agent 任务执行</h5>
<p>任务执行过程：</p>
<ol>
<li>实时发送执行情况到 Server；</li>
<li>Server 宕机，切换一个 Server 发送最终状态；</li>
</ol>
<pre><code class=language-go>// dkron/grpc_agent.go
// Dkron agent 执行任务, GRPC 客户端推流
func (as *AgentServer) AgentRun(req *types.AgentRunRequest, stream types.Agent_AgentRunServer) error {
	job := req.Job
	execution := req.Execution
    // buffer 创建, 储存执行输出
	output, _ := circbuf.NewBuffer(maxBufSize)

	var success bool

	jex := job.Executor
	exc := job.ExecutorConfig

	execution.StartedAt = ptypes.TimestampNow()
	execution.NodeName = as.agent.config.NodeName

	// 发送执行前状态
	if err := stream.Send(&amp;types.AgentRunStream{
		Execution: execution,
	}); err != nil {
		return err
	}

	...

	// Check if executor exists
	// 找到对应的执行插件
	if executor, ok := as.agent.ExecutorPlugins[jex]; ok {
		as.logger.WithField(&quot;plugin&quot;, jex).Debug(&quot;grpc_agent: calling executor plugin&quot;)
		runningExecutions.Store(execution.GetGroup(), execution)
		// go-plugin grpc 调用执行
		out, err := executor.Execute(&amp;types.ExecuteRequest{
			JobName: job.Name,
			Config:  exc,
			// callback, 将执行输出结果赋值到 output, 通过 stream 发送给服务端
			// ref: https://github.com/distribworks/dkron/pull/719
		}, &amp;statusAgentHelper{
			stream:    stream,
			execution: execution,
		})
        ...

		if out != nil {
			output.Write(out.Output)
		}
	} else {
	   ...
		output.Write([]byte(&quot;grpc_agent: Specified executor is not present&quot;))
	}

	// 执行完成
	execution.FinishedAt = ptypes.TimestampNow()
	execution.Success = success
	execution.Output = output.Bytes()

	runningExecutions.Delete(execution.GetGroup())

	// 发送最终状态
	if err := stream.Send(&amp;types.AgentRunStream{
		Execution: execution,
	}); err != nil {
		// 有可能 server 没能接收到最后执行状态
        ...
		// TCP 连接筛选一个 server
		rpcServer, err := as.agent.checkAndSelectServer()
		if err != nil {
			return err
		}
		// 调用执行完成
		return as.agent.GRPCClient.ExecutionDone(rpcServer, NewExecutionFromProto(execution))
	}

	return nil
}

</code></pre>
<h5 id=任务执行后处理>任务执行后处理</h5>
<p>Job 执行完成后处理：</p>
<ol>
<li>必须为 Leader 进行数据持久化；</li>
<li>通过 Raft Apply 同步 Server 间执行记录；</li>
<li>错误重试，再次进行任务调度；</li>
<li>执行 Job 定义中依赖的 Job。</li>
</ol>
<pre><code class=language-go>// dkron/grpc.go
// 执行完成, 进行后续处理
func (grpcs *GRPCServer) ExecutionDone(ctx context.Context, execDoneReq *proto.ExecutionDoneRequest) (*proto.ExecutionDoneResponse, error) {
    ...
	if !grpcs.agent.IsLeader() {
		addr := grpcs.agent.raft.Leader()
		// 如果我不是 leader , 将请求发给 leader 执行
		grpcs.agent.GRPCClient.ExecutionDone(string(addr), NewExecutionFromProto(execDoneReq.Execution))
		return nil, ErrNotLeader
	}

	// This is the leader at this point, so process the execution, encode the value and apply the log to the cluster.
	// Get the defined output types for the job, and call them
	job, err := grpcs.agent.Store.GetJob(execDoneReq.Execution.JobName, nil)
	if err != nil {
		return nil, err
	}

	// 执行 processor, 不需要双向通信
	// 由此推测 exector 使用 grpc 执行是需要 GRPC 双向流
	pbex := *execDoneReq.Execution
	for k, v := range job.Processors {
		if processor, ok := grpcs.agent.ProcessorPlugins[k]; ok {
			v[&quot;reporting_node&quot;] = grpcs.agent.config.NodeName
			pbex = processor.Process(&amp;plugin.ProcessorArgs{Execution: pbex, Config: v})
		} else {
			...
		}
	}

	// 同步集群状态
	execDoneReq.Execution = &amp;pbex
	cmd, err := Encode(ExecutionDoneType, execDoneReq)
	if err != nil {
		return nil, err
	}
	af := grpcs.agent.raft.Apply(cmd, raftTimeout)
	if err := af.Error(); err != nil {
		return nil, err
	}

	// Retrieve the fresh, updated job from the store to work on stored values
	job, err = grpcs.agent.Store.GetJob(job.Name, nil)
	if err != nil {
		grpcs.logger.WithError(err).WithField(&quot;job&quot;, execDoneReq.Execution.JobName).Error(&quot;grpc: Error retrieving job from store&quot;)
		return nil, err
	}

	// 任务执行重试
	execution := NewExecutionFromProto(&amp;pbex)
	if !execution.Success &amp;&amp; uint(execution.Attempt) &lt; job.Retries+1 {
		execution.Attempt++

		// Keep all execution properties intact except the last output
		execution.Output = &quot;&quot;
        ...

		if _, err := grpcs.agent.Run(job.Name, execution); err != nil {
			return nil, err
		}
		return &amp;proto.ExecutionDoneResponse{
			From:    grpcs.agent.config.NodeName,
			Payload: []byte(&quot;retry&quot;),
		}, nil
	}

	exg, err := grpcs.agent.Store.GetExecutionGroup(execution,
		&amp;ExecutionOptions{
			Timezone: job.GetTimeLocation(),
		},
	)
	if err != nil {
		...
		return nil, err
	}

    ...

	// 执行依赖的任务
	if len(job.DependentJobs) &gt; 0 &amp;&amp; job.Status == StatusSuccess {
		for _, djn := range job.DependentJobs {
			dj, err := grpcs.agent.Store.GetJob(djn, nil)
			dj.Agent = grpcs.agent
			if err != nil {
				return nil, err
			}
		}
	}

	return &amp;proto.ExecutionDoneResponse{
		From:    grpcs.agent.config.NodeName,
		Payload: []byte(&quot;saved&quot;),
	}, nil
}
</code></pre>
<h3 id=26-学习笔记>2.6. 学习笔记</h3>
<h4 id=261-serf-cli>2.6.1. Serf CLI</h4>
<p>
<figure class=image>
<img src=/2021/09/dkron-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/images/627201_0.png alt loading=lazy>
</figure></p>
<p>Serf 官方文档的示例主要是分布式运行脚本，通过启动在多个服务器上启动 serf agent（CLI 命令）。</p>
<p>通过配置 Event handler 参数启动：</p>
<pre><code class=language-bash>$ serf agent -event-handler=query:ssh=/bin/bash
</code></pre>
<p>发送 Query 命令就可以执行程序了：</p>
<pre><code class=language-bash>$ serf query ssh uptime
Query 'ssh' dispatched
Ack from 'node1.pocketstudio.net'
Ack from 'node2.pocketstudio.net'
Response from 'node2.pocketstudio.net':  05:25:34 up 21:31,  1 user,  load average: 0.00, 0.00, 0.00
Total Acks: 2
Total Responses: 1
</code></pre>
<p>另外 Consul 的 <code>go.mod</code> 里也引用了 Serf 包。</p>
</div>
<footer class=post-footer>
<div class=post-meta>
<time datetime=" 2021-09-23" itemprop=datePublished>Sep 23, 2021</time>
</div>
</footer>
</article>
<script src=https://cdn.jsdelivr.net/npm/tocbot/dist/tocbot.min.js></script>
<aside class=typeface-sans lang=zh-hans>
<nav class=toc></nav>
</aside>
<script>tocbot.init({tocSelector:'.toc',contentSelector:'article',headingSelector:'h1, h2, h3, h4, h5',hasInnerContainers:!0,collapseDepth:"3",positionFixedSelector:".toc",headingsOffset:10})</script>
</div><script src=https://cdn.jsdelivr.net/npm/prismjs@1.24.1/prism.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/prismjs@1.24.1/plugins/filter-highlight-all/prism-filter-highlight-all.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/prismjs@1.24.1/plugins/autoloader/prism-autoloader.min.js></script>
<script>Prism.plugins.filterHighlightAll.add(function(a){return a.language!=="mermaid"})</script>
<script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script>
<script>mermaid.init({startOnLoad:!0},"pre code.language-mermaid",function(){const a=document.querySelectorAll('code.language-mermaid');for(const c of a){c.style.backgroundColor='initial';const b=c.parentNode;b.style.border='none',b.style.textAlign='center',window.darkmode&&!window.darkmode.isActivated()&&(b.style.backgroundColor='initial')}})</script>
</div>
</main><footer class=site-footer>
<div class=wrapper>
<div class=social-links>
<a class="social-link social-github" href=https://github.com/mayocream><svg style="width:24px;height:24px" viewBox="0 0 24 24"><path fill="currentcolor" d="M12 2A10 10 0 002 12c0 4.42 2.87 8.17 6.84 9.5C9.34 21.58 9.5 21.27 9.5 21 9.5 20.77 9.5 20.14 9.5 19.31 6.73 19.91 6.14 17.97 6.14 17.97c-.46-1.16-1.11-1.47-1.11-1.47C4.12 15.88 5.1 15.9 5.1 15.9 6.1 15.97 6.63 16.93 6.63 16.93 7.5 18.45 8.97 18 9.54 17.76 9.63 17.11 9.89 16.67 10.17 16.42c-2.22-.25-4.55-1.11-4.55-4.92.0-1.11.38-2 1.03-2.71C6.55 8.54 6.2 7.5 6.75 6.15c0 0 .84-.27 2.75 1.02C10.29 6.95 11.15 6.84 12 6.84s1.71.11 2.5.33c1.91-1.29 2.75-1.02 2.75-1.02C17.8 7.5 17.45 8.54 17.35 8.79 18 9.5 18.38 10.39 18.38 11.5c0 3.82-2.34 4.66-4.57 4.91C14.17 16.72 14.5 17.33 14.5 18.26c0 1.34.0 2.42.0 2.74.0.27.16.59.67.5C19.14 20.16 22 16.42 22 12A10 10 0 0012 2z"/></svg>
</a>
<a class="social-link social-twitter" href=https://twitter.com/Freeze_Mayo><svg style="width:24px;height:24px" viewBox="0 0 24 24"><path fill="currentcolor" d="M22.46 6c-.77.35-1.6.58-2.46.69C20.88 6.16 21.56 5.32 21.88 4.31 21.05 4.81 20.13 5.16 19.16 5.36 18.37 4.5 17.26 4 16 4c-2.35.0-4.27 1.92-4.27 4.29C11.73 8.63 11.77 8.96 11.84 9.27 8.28 9.09 5.11 7.38 3 4.79c-.37.63-.58 1.37-.58 2.15.0 1.49.75 2.81 1.91 3.56C3.62 10.5 2.96 10.3 2.38 10V10.03c0 2.08 1.48 3.82 3.44 4.21C5.46 14.34 5.08 14.39 4.69 14.39 4.42 14.39 4.15 14.36 3.89 14.31c.54 1.69 2.11 2.95 4 2.98-1.46 1.16-3.31 1.84-5.33 1.84C2.22 19.13 1.88 19.11 1.54 19.07 3.44 20.29 5.7 21 8.12 21 16 21 20.33 14.46 20.33 8.79 20.33 8.6 20.33 8.42 20.32 8.23 21.16 7.63 21.88 6.87 22.46 6z"/></svg>
</a>
<a class="social-link social-rss" href=/index.xml target=_blank><svg style="width:24px;height:24px" viewBox="0 0 24 24"><path fill="currentcolor" d="M6.18 15.64a2.18 2.18.0 012.18 2.18C8.36 19 7.38 20 6.18 20 5 20 4 19 4 17.82a2.18 2.18.0 012.18-2.18M4 4.44A15.56 15.56.0 0119.56 20H16.73A12.73 12.73.0 004 7.27V4.44M4 10.1A9.9 9.9.0 0113.9 20H11.07A7.07 7.07.0 004 12.93V10.1z"/></svg>
</a>
</div>
<div class=credits>
Theme KAGAMI by Kamikat
</div>
</div>
</footer><script>(function(a){for(var a=Array.prototype.slice.call(document.getElementsByClassName('baseline-fix')),b=0,c;b!=a.length;b++)c=a[b],c.innerHTML=c.innerHTML.replace(/[\u2000-\u206e⸀-\u2e7e⺀-\u2efe⼀-\u2fde⿰-\u2ffe\u3000-〾\u3040-ゞ゠-ヾ\u3100-\u312e\u3130-ㆎ㆐-㆞ㆠ-\u31be㇀-\u31eeㇰ-ㇾ㈀-㋾㌀-㏾㐀-\u4dbe一-\u9ffe\ua960-\ua97e가-\ud7ae\ud7b0-\ud7fe豈-\ufafe︰-﹎\uff00-￮]|[\ud840-\ud868\ud86a-\ud86c][\udc00-\udfff]|\ud82c[\udc00-\udcfe]|\ud869[\udc00-\udede\udf00-\udfff]|\ud86d[\udc00-\udf3e\udf40-\udfff]|\ud86e[\udc00-\udc1e]|\ud87e[\udc00-\ude1e]/g,function(a){return'<span class="baseline-fix-block">'+a+'<'+'/span>'}),c.classList.remove('baseline-fix')})(Array.prototype.slice.call(document.getElementsByClassName('baseline-fix')))</script>
<script type=text/javascript>const resize=function(){this.width=.5*(this.naturalWidth||this.width)};Array.prototype.forEach.call(document.querySelectorAll('.half-size, .retina2x'),function(a){a.naturalWidth?resize.call(a):a.onload=resize})</script>
<script src=https://cdn.jsdelivr.net/npm/twemoji@13.1.0/dist/twemoji.min.js></script>
<script>twemoji.parse(document.body,{folder:'svg',ext:'.svg'})</script>
</body>